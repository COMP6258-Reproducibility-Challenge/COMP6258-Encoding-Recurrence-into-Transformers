Running SLURM prolog script on pink54.cluster.local
===============================================================================
Job started on Fri May 17 09:22:56 BST 2024
Job ID          : 6000054
Job name        : run_enwik8_rsa_small4.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa_small4.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink54
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 7
    - n_head : 8
    - d_head : 8
    - d_embed : 16
    - d_model : 16
    - d_inner : 128
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 64
    - eval_tgt_len : 32
    - ext_len : 0
    - mem_len : 64
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 69592
    - n_nonemb_param : 65996
====================================================================================================
#params = 69592
#non emb params = 65996
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.3163652420043945
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.623166561126709
batch: 2
train_loss: 15.918050765991211
batch: 3
train_loss: 21.21493673324585
batch: 4
train_loss: 26.493075847625732
batch: 5
train_loss: 31.7648286819458
batch: 6
train_loss: 37.022603034973145
batch: 7
train_loss: 42.27904558181763
batch: 8
train_loss: 47.52614498138428
batch: 9
train_loss: 52.763028144836426
batch: 10
train_loss: 57.993287086486816
batch: 11
train_loss: 63.21564197540283
batch: 12
train_loss: 68.43167304992676
batch: 13
train_loss: 73.64396953582764
batch: 14
train_loss: 78.85004663467407
batch: 15
train_loss: 84.0439829826355
batch: 16
train_loss: 89.23683738708496
batch: 17
train_loss: 94.42851638793945
batch: 18
train_loss: 99.60554075241089
batch: 19
train_loss: 104.78391551971436
batch: 20
train_loss: 109.9575891494751
batch: 21
train_loss: 115.11986875534058
batch: 22
train_loss: 120.283616065979
batch: 23
train_loss: 125.4425573348999
batch: 24
train_loss: 130.59075355529785
batch: 25
train_loss: 135.73952388763428
batch: 26
train_loss: 140.87197065353394
batch: 27
train_loss: 146.00158882141113
batch: 28
train_loss: 151.12139225006104
batch: 29
train_loss: 156.24786806106567
batch: 30
train_loss: 161.35816287994385
batch: 31
train_loss: 166.4747929573059
batch: 32
train_loss: 171.58493900299072
batch: 33
train_loss: 176.69663333892822
batch: 34
train_loss: 181.79727983474731
batch: 35
train_loss: 186.8835964202881
batch: 36
train_loss: 191.96756267547607
batch: 37
train_loss: 197.04921436309814
batch: 38
train_loss: 202.12548732757568
batch: 39
train_loss: 207.1974139213562
batch: 40
train_loss: 212.27501726150513
batch: 41
train_loss: 217.33480024337769
batch: 42
train_loss: 222.39136934280396
batch: 43
train_loss: 227.44076204299927
batch: 44
train_loss: 232.46657276153564
batch: 45
train_loss: 237.4919934272766
batch: 46
train_loss: 242.51222324371338
batch: 47
train_loss: 247.53351736068726
batch: 48
train_loss: 252.5481915473938
batch: 49
train_loss: 257.5590810775757
batch: 50
train_loss: 262.5637001991272
batch: 51
train_loss: 267.56870555877686
batch: 52
slurmstepd-pink54: error: *** JOB 6000054 ON pink54 CANCELLED AT 2024-05-17T09:23:33 ***
==============================================================================
Running epilogue script on pink54.

Submit time  : 2024-05-17T09:22:55
Start time   : 2024-05-17T09:22:56
End time     : 2024-05-17T09:23:33
Elapsed time : 00:00:37 (Timelimit=08:00:00)

Job ID: 6000054
Cluster: i5
User/Group: rc3g20/fp
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 8
CPU Utilized: 00:00:01
CPU Efficiency: 0.34% of 00:04:56 core-walltime
Job Wall-clock time: 00:00:37
Memory Utilized: 1.91 GB
Memory Efficiency: 0.00% of 16.00 B

