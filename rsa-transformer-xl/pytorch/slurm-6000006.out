Running SLURM prolog script on pink57.cluster.local
===============================================================================
Job started on Fri May 17 08:57:25 BST 2024
Job ID          : 6000006
Job name        : run_enwik8_rsa_small3.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa_small3.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink57
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 7
    - n_head : 8
    - d_head : 8
    - d_embed : 16
    - d_model : 16
    - d_inner : 256
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 128
    - eval_tgt_len : 32
    - ext_len : 0
    - mem_len : 128
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 99160
    - n_nonemb_param : 95564
====================================================================================================
#params = 99160
#non emb params = 95564
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.326201915740967
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.64685583114624
batch: 2
train_loss: 15.957573413848877
batch: 3
train_loss: 21.26208734512329
batch: 4
train_loss: 26.558716297149658
batch: 5
train_loss: 31.84332513809204
batch: 6
train_loss: 37.118104457855225
batch: 7
train_loss: 42.38746452331543
batch: 8
train_loss: 47.65214729309082
batch: 9
train_loss: 52.91025352478027
batch: 10
train_loss: 58.16516637802124
batch: 11
train_loss: 63.411197662353516
batch: 12
train_loss: 68.65305757522583
batch: 13
train_loss: 73.88487482070923
batch: 14
train_loss: 79.11328411102295
batch: 15
train_loss: 84.33889722824097
batch: 16
train_loss: 89.56311893463135
batch: 17
train_loss: 94.786386013031
batch: 18
train_loss: 99.99682426452637
batch: 19
train_loss: 105.20643472671509
batch: 20
train_loss: 110.40808057785034
batch: 21
train_loss: 115.60506820678711
batch: 22
train_loss: 120.78952693939209
batch: 23
train_loss: 125.97143030166626
batch: 24
train_loss: 131.14855289459229
batch: 25
train_loss: 136.32186651229858
batch: 26
train_loss: 141.49250888824463
batch: 27
train_loss: 146.66103649139404
batch: 28
train_loss: 151.823823928833
batch: 29
train_loss: 156.98052072525024
batch: 30
train_loss: 162.1322135925293
batch: 31
train_loss: 167.27844667434692
batch: 32
train_loss: 172.41564464569092
batch: 33
train_loss: 177.54592370986938
batch: 34
train_loss: 182.67719173431396
batch: 35
train_loss: 187.79161977767944
batch: 36
train_loss: 192.903666973114
batch: 37
train_loss: 198.00036144256592
batch: 38
train_loss: 203.10678958892822
batch: 39
train_loss: 208.18838119506836
batch: 40
train_loss: 213.26884937286377
batch: 41
train_loss: 218.33378410339355
batch: 42
train_loss: 223.40707921981812
batch: 43
train_loss: 228.46683597564697
batch: 44
train_loss: 233.5244002342224
batch: 45
train_loss: 238.57107830047607
batch: 46
train_loss: 243.61549997329712
batch: 47
train_loss: 248.65265226364136
batch: 48
train_loss: 253.67970657348633
batch: 49
train_loss: 258.70761156082153
batch: 50
train_loss: 263.7329397201538
batch: 51
train_loss: 268.7508339881897
batch: 52
train_loss: 273.7633490562439
batch: 53
train_loss: 278.76597118377686
batch: 54
train_loss: 283.76113414764404
batch: 55
train_loss: 288.74233388900757
batch: 56
train_loss: 293.72665214538574
batch: 57
train_loss: 298.7003536224365
batch: 58
train_loss: 303.66369247436523
batch: 59
train_loss: 308.6268515586853
batch: 60
train_loss: 313.5786862373352
batch: 61
train_loss: 318.53874683380127
batch: 62
train_loss: 323.48343229293823
batch: 63
train_loss: 328.4334707260132
batch: 64
train_loss: 333.3669424057007
batch: 65
train_loss: 338.29832458496094
batch: 66
train_loss: 343.2168264389038
batch: 67
train_loss: 348.13987922668457
batch: 68
train_loss: 353.0509476661682
batch: 69
train_loss: 357.95546293258667
batch: 70
train_loss: 362.843309879303
batch: 71
train_loss: 367.73659801483154
batch: 72
train_loss: 372.6169538497925
batch: 73
train_loss: 377.50666904449463
batch: 74
train_loss: 382.387158870697
batch: 75
train_loss: 387.2616858482361
batch: 76
train_loss: 392.1336874961853
batch: 77
train_loss: 396.9814238548279
batch: 78
train_loss: 401.81638288497925
batch: 79
train_loss: 406.6628403663635
batch: 80
train_loss: 411.4911479949951
batch: 81
train_loss: 416.3308210372925
batch: 82
train_loss: 421.14650535583496
batch: 83
train_loss: 425.9481110572815
batch: 84
train_loss: 430.73495531082153
batch: 85
train_loss: 435.52374029159546
batch: 86
train_loss: 440.3213610649109
batch: 87
train_loss: 445.116313457489
batch: 88
train_loss: 449.907675743103
batch: 89
train_loss: 454.6774010658264
batch: 90
train_loss: 459.4484004974365
batch: 91
train_loss: 464.2571907043457
batch: 92
train_loss: 469.0342149734497
batch: 93
train_loss: 473.8133764266968
batch: 94
train_loss: 478.5659689903259
batch: 95
train_loss: 483.3331437110901
batch: 96
train_loss: 488.0708546638489
batch: 97
train_loss: 492.82759380340576
batch: 98
train_loss: 497.55359506607056
batch: 99
train_loss: 502.2791066169739
batch: 100
train_loss: 507.0095839500427
batch: 101
train_loss: 511.7417526245117
batch: 102
train_loss: 516.4673299789429
batch: 103
train_loss: 521.1634411811829
batch: 104
train_loss: 525.8517551422119
batch: 105
train_loss: 530.5542998313904
batch: 106
train_loss: 535.2277498245239
batch: 107
train_loss: 539.8942227363586
batch: 108
train_loss: 544.57182264328
batch: 109
train_loss: 549.2464742660522
batch: 110
train_loss: 553.9128303527832
batch: 111
train_loss: 558.5726432800293
batch: 112
train_loss: 563.2246861457825
batch: 113
train_loss: 567.8626427650452
batch: 114
train_loss: 572.5285987854004
batch: 115
train_loss: 577.1714253425598
batch: 116
train_loss: 581.8325681686401
batch: 117
train_loss: 586.4974775314331
batch: 118
train_loss: 591.1474938392639
batch: 119
train_loss: 595.7747507095337
batch: 120
train_loss: 600.3981199264526
batch: 121
train_loss: 605.0140218734741
batch: 122
train_loss: 609.6236720085144
batch: 123
train_loss: 614.2276382446289
batch: 124
train_loss: 618.8017430305481
batch: 125
train_loss: 623.3701810836792
batch: 126
train_loss: 627.9312043190002
batch: 127
train_loss: 632.4727835655212
batch: 128
train_loss: 637.0224561691284
batch: 129
train_loss: 641.567843914032
batch: 130
train_loss: 646.1148805618286
batch: 131
train_loss: 650.6377263069153
batch: 132
train_loss: 655.1841683387756
batch: 133
train_loss: 659.7042279243469
batch: 134
train_loss: 664.2130351066589
batch: 135
train_loss: 668.7313070297241
batch: 136
train_loss: 673.2129397392273
batch: 137
train_loss: 677.7026772499084
batch: 138
train_loss: 682.1874618530273
batch: 139
train_loss: 686.6599955558777
batch: 140
train_loss: 691.1272559165955
batch: 141
train_loss: 695.6084446907043
batch: 142
train_loss: 700.0968933105469
batch: 143
train_loss: 704.5544233322144
batch: 144
train_loss: 709.0237321853638
batch: 145
train_loss: 713.4814910888672
batch: 146
train_loss: 717.9238805770874
batch: 147
train_loss: 722.3695712089539
batch: 148
train_loss: 726.8308281898499
batch: 149
train_loss: 731.2889804840088
batch: 150
train_loss: 735.7290191650391
batch: 151
train_loss: 740.1661505699158
batch: 152
train_loss: 744.5733699798584
batch: 153
train_loss: 748.9725761413574
batch: 154
train_loss: 753.402681350708
batch: 155
train_loss: 757.837592124939
batch: 156
train_loss: 762.2294054031372
batch: 157
train_loss: 766.6077737808228
batch: 158
train_loss: 770.9810190200806
batch: 159
train_loss: 775.3572216033936
batch: 160
train_loss: 779.7319574356079
batch: 161
train_loss: 784.097903251648
batch: 162
train_loss: 788.4689745903015
batch: 163
train_loss: 792.8513488769531
batch: 164
train_loss: 797.214325428009
batch: 165
train_loss: 801.5979347229004
batch: 166
train_loss: 805.9312629699707
batch: 167
train_loss: 810.2602252960205
batch: 168
train_loss: 814.5684914588928
batch: 169
train_loss: 818.8663730621338
batch: 170
train_loss: 823.1691026687622
batch: 171
train_loss: 827.4851603507996
batch: 172
train_loss: 831.7841711044312
batch: 173
train_loss: 836.0791630744934
batch: 174
train_loss: 840.363507270813
batch: 175
train_loss: 844.6304807662964
batch: 176
train_loss: 848.899950504303
batch: 177
train_loss: 853.1560139656067
batch: 178
train_loss: 857.4125008583069
batch: 179
train_loss: 861.6913290023804
batch: 180
train_loss: 865.9694285392761
batch: 181
train_loss: 870.254487991333
batch: 182
train_loss: 874.5326137542725
batch: 183
train_loss: 878.7827243804932
batch: 184
train_loss: 883.0570197105408
batch: 185
train_loss: 887.3128695487976
batch: 186
train_loss: 891.5595107078552
batch: 187
train_loss: 895.8539018630981
batch: 188
train_loss: 900.1280851364136
batch: 189
train_loss: 904.4348406791687
batch: 190
train_loss: 908.7070393562317
batch: 191
train_loss: 912.9756169319153
batch: 192
train_loss: 917.221197605133
batch: 193
train_loss: 921.4339866638184
batch: 194
train_loss: 925.6915574073792
batch: 195
train_loss: 929.9559063911438
batch: 196
train_loss: 934.1960868835449
batch: 197
train_loss: 938.442777633667
batch: 198
train_loss: 942.6559700965881
batch: 199
train_loss: 946.866651058197
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 1428.11 | loss  4.73 | bpc   6.83020
batch: 200
train_loss: 4.2265625
batch: 201
train_loss: 8.441798210144043
batch: 202
train_loss: 12.692704677581787
batch: 203
train_loss: 16.907952308654785
batch: 204
train_loss: 21.13122272491455
batch: 205
train_loss: 25.34049654006958
batch: 206
train_loss: 29.53258514404297
batch: 207
train_loss: 33.69571495056152
batch: 208
train_loss: 37.87340307235718
batch: 209
train_loss: 42.01572799682617
batch: 210
train_loss: 46.179150104522705
batch: 211
train_loss: 50.31186819076538
batch: 212
train_loss: 54.50088596343994
batch: 213
train_loss: 58.68513011932373
batch: 214
train_loss: 62.851791858673096
batch: 215
train_loss: 67.02160978317261
batch: 216
train_loss: 71.18519496917725
batch: 217
train_loss: 75.35823678970337
batch: 218
train_loss: 79.5423526763916
batch: 219
train_loss: 83.6663327217102
batch: 220
train_loss: 87.81652164459229
batch: 221
train_loss: 91.98779726028442
batch: 222
train_loss: 96.14434623718262
batch: 223
train_loss: 100.26461458206177
batch: 224
train_loss: 104.40382862091064
batch: 225
train_loss: 108.48015594482422
batch: 226
train_loss: 112.5709490776062
batch: 227
train_loss: 116.63773488998413
batch: 228
train_loss: 120.69504499435425
batch: 229
train_loss: 124.79729461669922
batch: 230
train_loss: 128.85621309280396
batch: 231
train_loss: 132.91176462173462
batch: 232
train_loss: 136.97131633758545
batch: 233
train_loss: 141.0736541748047
batch: 234
train_loss: 145.09560823440552
batch: 235
train_loss: 149.1421890258789
batch: 236
train_loss: 153.2236213684082
batch: 237
train_loss: 157.27827405929565
batch: 238
train_loss: 161.35033321380615
batch: 239
train_loss: 165.3596749305725
batch: 240
train_loss: 169.40905284881592
batch: 241
train_loss: 173.366468667984
batch: 242
train_loss: 177.3568799495697
batch: 243
train_loss: 181.3228313922882
batch: 244
train_loss: 185.300621509552
batch: 245
train_loss: 189.31734228134155
batch: 246
train_loss: 193.35233116149902
batch: 247
train_loss: 197.33695816993713
batch: 248
train_loss: 201.32503819465637
batch: 249
train_loss: 205.36666750907898
batch: 250
train_loss: 209.3826539516449
batch: 251
train_loss: 213.43703055381775
batch: 252
train_loss: 217.42680025100708
batch: 253
train_loss: 221.41068601608276
batch: 254
train_loss: 225.45283269882202
batch: 255
train_loss: 229.47371816635132
batch: 256
train_loss: 233.4759783744812
batch: 257
train_loss: 237.4352159500122
batch: 258
train_loss: 241.4549856185913
batch: 259
train_loss: 245.40378260612488
batch: 260
train_loss: 249.3662965297699
batch: 261
train_loss: 253.3342227935791
batch: 262
train_loss: 257.2916886806488
batch: 263
train_loss: 261.27097630500793
batch: 264
train_loss: 265.2280695438385
batch: 265
train_loss: 269.15318417549133
batch: 266
train_loss: 273.12384009361267
batch: 267
train_loss: 277.09412455558777
batch: 268
train_loss: 281.02859711647034
batch: 269
train_loss: 284.9756875038147
batch: 270
train_loss: 288.9143035411835
batch: 271
train_loss: 292.890273809433
batch: 272
train_loss: 296.8657705783844
batch: 273
train_loss: 300.87904381752014
batch: 274
train_loss: 304.7684602737427
batch: 275
train_loss: 308.7306704521179
batch: 276
train_loss: 312.6989483833313
batch: 277
train_loss: 316.6767783164978
batch: 278
train_loss: 320.58925104141235
batch: 279
train_loss: 324.472784280777
batch: 280
train_loss: 328.36887550354004
batch: 281
train_loss: 332.2999577522278
batch: 282
train_loss: 336.21251106262207
batch: 283
train_loss: 340.0791566371918
batch: 284
train_loss: 343.9727563858032
batch: 285
train_loss: 347.84743189811707
batch: 286
train_loss: 351.71250891685486
batch: 287
train_loss: 355.5896472930908
batch: 288
train_loss: 359.41721773147583
batch: 289
train_loss: 363.27801728248596
batch: 290
train_loss: 367.2298605442047
batch: 291
train_loss: 371.15808629989624
batch: 292
train_loss: 375.04557394981384
batch: 293
train_loss: 378.946635723114
batch: 294
train_loss: 382.8054986000061
batch: 295
train_loss: 386.6290888786316
batch: 296
train_loss: 390.4909143447876
batch: 297
train_loss: 394.34724044799805
batch: 298
train_loss: 398.17965173721313
batch: 299
train_loss: 401.99158334732056
batch: 300
train_loss: 405.8736765384674
batch: 301
train_loss: 409.76717710494995
batch: 302
train_loss: 413.6446144580841
batch: 303
train_loss: 417.50480937957764
batch: 304
train_loss: 421.3738248348236
batch: 305
train_loss: 425.1763958930969
batch: 306
train_loss: 429.002161026001
batch: 307
train_loss: 432.8095180988312
batch: 308
train_loss: 436.57294178009033
batch: 309
train_loss: 440.3966042995453
batch: 310
train_loss: 444.27679777145386
batch: 311
train_loss: 448.14145612716675
batch: 312
train_loss: 452.0017466545105
batch: 313
train_loss: 455.8649151325226
batch: 314
train_loss: 459.74624013900757
batch: 315
train_loss: 463.65564489364624
batch: 316
train_loss: 467.56234884262085
batch: 317
train_loss: 471.4579336643219
batch: 318
train_loss: 475.3278298377991
batch: 319
train_loss: 479.2398636341095
batch: 320
train_loss: 483.1096167564392
batch: 321
train_loss: 486.94617915153503
batch: 322
train_loss: 490.75651931762695
batch: 323
train_loss: 494.63315296173096
batch: 324
train_loss: 498.4938259124756
batch: 325
train_loss: 502.40561413764954
batch: 326
train_loss: 506.3441672325134
batch: 327
train_loss: 510.1806106567383
batch: 328
train_loss: 514.0271904468536
batch: 329
train_loss: 517.8506948947906
batch: 330
train_loss: 521.657895565033
batch: 331
train_loss: 525.5181519985199
batch: 332
train_loss: 529.3464775085449
batch: 333
train_loss: 533.2210104465485
batch: 334
train_loss: 537.044585943222
batch: 335
train_loss: 540.8826122283936
batch: 336
train_loss: 544.6433565616608
batch: 337
train_loss: 548.4101099967957
batch: 338
train_loss: 552.1523549556732
batch: 339
train_loss: 555.9187240600586
batch: 340
train_loss: 559.6910905838013
batch: 341
train_loss: 563.497777223587
batch: 342
train_loss: 567.238977432251
batch: 343
train_loss: 571.035249710083
batch: 344
train_loss: 574.7888820171356
batch: 345
train_loss: 578.5959458351135
batch: 346
train_loss: 582.3729691505432
batch: 347
train_loss: 586.2114455699921
batch: 348
train_loss: 589.990553855896
batch: 349
train_loss: 593.797534942627
batch: 350
train_loss: 597.5406370162964
batch: 351
train_loss: 601.2991750240326
batch: 352
train_loss: 605.1117842197418
batch: 353
train_loss: 608.8709828853607
batch: 354
train_loss: 612.6387159824371
batch: 355
train_loss: 616.4079918861389
batch: 356
train_loss: 620.1804888248444
batch: 357
train_loss: 623.9047830104828
batch: 358
train_loss: 627.6596999168396
batch: 359
train_loss: 631.3232827186584
batch: 360
train_loss: 635.0537822246552
batch: 361
train_loss: 638.8046276569366
batch: 362
train_loss: 642.5261507034302
batch: 363
train_loss: 646.2131752967834
batch: 364
train_loss: 649.9147264957428
batch: 365
train_loss: 653.5616073608398
batch: 366
train_loss: 657.185263633728
batch: 367
train_loss: 660.8768126964569
batch: 368
train_loss: 664.6127572059631
batch: 369
train_loss: 668.3327467441559
batch: 370
train_loss: 672.015552520752
batch: 371
train_loss: 675.7350473403931
batch: 372
train_loss: 679.4371044635773
batch: 373
train_loss: 683.1143941879272
batch: 374
train_loss: 686.7785060405731
batch: 375
train_loss: 690.4127414226532
batch: 376
train_loss: 694.1104898452759
batch: 377
train_loss: 697.8023788928986
batch: 378
train_loss: 701.451061964035
batch: 379
train_loss: 705.0618705749512
batch: 380
train_loss: 708.7418668270111
batch: 381
train_loss: 712.4163455963135
batch: 382
train_loss: 716.1039822101593
batch: 383
train_loss: 719.804838180542
batch: 384
train_loss: 723.5698764324188
batch: 385
train_loss: 727.3188328742981
batch: 386
train_loss: 731.0155577659607
batch: 387
train_loss: 734.6699140071869
batch: 388
train_loss: 738.3061354160309
batch: 389
train_loss: 741.9211354255676
batch: 390
train_loss: 745.5390477180481
batch: 391
train_loss: 749.1649661064148
batch: 392
train_loss: 752.8162834644318
batch: 393
train_loss: 756.479546546936
batch: 394
train_loss: 760.1467661857605
batch: 395
train_loss: 763.7501330375671
batch: 396
train_loss: 767.3745408058167
batch: 397
train_loss: 771.0218291282654
batch: 398
train_loss: 774.6339237689972
batch: 399
train_loss: 778.2641611099243
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 1423.32 | loss  3.89 | bpc   5.61399
batch: 400
train_loss: 3.624049186706543
batch: 401
train_loss: 7.301303148269653
batch: 402
train_loss: 10.964378356933594
batch: 403
train_loss: 14.706120252609253
batch: 404
train_loss: 18.396915197372437
batch: 405
train_loss: 22.053884506225586
batch: 406
train_loss: 25.687444925308228
batch: 407
train_loss: 29.40727663040161
batch: 408
train_loss: 33.022541522979736
batch: 409
train_loss: 36.658695936203
batch: 410
train_loss: 40.24694228172302
batch: 411
train_loss: 43.89087629318237
batch: 412
train_loss: 47.5775682926178
batch: 413
train_loss: 51.18850255012512
batch: 414
train_loss: 54.81383919715881
batch: 415
train_loss: 58.421924114227295
batch: 416
train_loss: 61.989497900009155
batch: 417
train_loss: 65.58718609809875
batch: 418
train_loss: 69.21628165245056
batch: 419
train_loss: 72.82716703414917
batch: 420
train_loss: 76.39405035972595
batch: 421
train_loss: 79.99496507644653
batch: 422
train_loss: 83.5933005809784
batch: 423
train_loss: 87.19128489494324
batch: 424
train_loss: 90.82755136489868
batch: 425
train_loss: 94.43741703033447
batch: 426
train_loss: 97.99258351325989
batch: 427
train_loss: 101.58460569381714
batch: 428
train_loss: 105.12350583076477
batch: 429
train_loss: 108.70662808418274
batch: 430
train_loss: 112.28005504608154
batch: 431
train_loss: 115.81867909431458
batch: 432
train_loss: 119.35645699501038
batch: 433
train_loss: 122.8672456741333
batch: 434
train_loss: 126.4392478466034
batch: 435
train_loss: 129.95640659332275
batch: 436
train_loss: 133.47105431556702
batch: 437
train_loss: 136.98745965957642
batch: 438
train_loss: 140.45752882957458
batch: 439
train_loss: 143.90275883674622
batch: 440
train_loss: 147.4077913761139
batch: 441
train_loss: 150.90266275405884
batch: 442
train_loss: 154.35482358932495
batch: 443
train_loss: 157.9002857208252
batch: 444
train_loss: 161.48306465148926
batch: 445
train_loss: 164.99660396575928
batch: 446
train_loss: 168.5056767463684
batch: 447
train_loss: 172.0586974620819
batch: 448
train_loss: 175.622318983078
batch: 449
train_loss: 179.2711639404297
batch: 450
train_loss: 182.86261653900146
batch: 451
train_loss: 186.49762082099915
batch: 452
train_loss: 190.13936948776245
batch: 453
train_loss: 193.69814085960388
batch: 454
train_loss: 197.28502225875854
batch: 455
train_loss: 200.8692798614502
batch: 456
train_loss: 204.45532822608948
batch: 457
train_loss: 208.09786438941956
batch: 458
train_loss: 211.73649644851685
batch: 459
train_loss: 215.29448914527893
batch: 460
train_loss: 218.88334250450134
batch: 461
train_loss: 222.5230314731598
batch: 462
train_loss: 226.1197121143341
batch: 463
train_loss: 229.67343091964722
batch: 464
train_loss: 233.2471628189087
batch: 465
train_loss: 236.7464861869812
batch: 466
train_loss: 240.3392152786255
batch: 467
train_loss: 243.913676738739
batch: 468
train_loss: 247.52169609069824
batch: 469
train_loss: 251.14734983444214
batch: 470
train_loss: 254.6871919631958
batch: 471
train_loss: 258.25192952156067
batch: 472
train_loss: 261.80649638175964
batch: 473
train_loss: 265.41623401641846
batch: 474
train_loss: 268.9633593559265
batch: 475
train_loss: 272.5800576210022
batch: 476
train_loss: 276.14811730384827
batch: 477
train_loss: 279.71115922927856
batch: 478
train_loss: 283.25390434265137
batch: 479
train_loss: 286.8114459514618
batch: 480
train_loss: 290.3563814163208
batch: 481
train_loss: 293.95972084999084
batch: 482
train_loss: 297.6629409790039
batch: 483
train_loss: 301.30993819236755
batch: 484
train_loss: 304.9583296775818
batch: 485
train_loss: 308.5699245929718
batch: 486
train_loss: 312.17314863204956
batch: 487
train_loss: 315.79929661750793
batch: 488
train_loss: 319.37406754493713
batch: 489
train_loss: 322.9559020996094
batch: 490
train_loss: 326.4745111465454
batch: 491
train_loss: 330.0133128166199
batch: 492
train_loss: 333.65289855003357
batch: 493
train_loss: 337.16507720947266
batch: 494
train_loss: 340.74626564979553
batch: 495
train_loss: 344.2413890361786
batch: 496
train_loss: 347.7211534976959
batch: 497
train_loss: 351.20187163352966
batch: 498
train_loss: 354.7612624168396
batch: 499
train_loss: 358.27828335762024
batch: 500
train_loss: 361.7938451766968
batch: 501
train_loss: 365.3998210430145
batch: 502
train_loss: 368.913535118103
batch: 503
train_loss: 372.44714546203613
batch: 504
train_loss: 375.98774695396423
batch: 505
train_loss: 379.51675248146057
batch: 506
train_loss: 383.0908432006836
batch: 507
train_loss: 386.6572411060333
batch: 508
train_loss: 390.12051796913147
batch: 509
train_loss: 393.5652084350586
batch: 510
train_loss: 397.01270294189453
batch: 511
train_loss: 400.5236322879791
batch: 512
train_loss: 403.98668789863586
batch: 513
train_loss: 407.4328718185425
batch: 514
train_loss: 410.9310040473938
batch: 515
train_loss: 414.51219296455383
batch: 516
train_loss: 417.9506514072418
batch: 517
train_loss: 421.39241003990173
batch: 518
train_loss: 424.77413058280945
batch: 519
train_loss: 428.2611629962921
batch: 520
train_loss: 431.66260504722595
batch: 521
train_loss: 435.0785686969757
batch: 522
train_loss: 438.51802110671997
batch: 523
train_loss: 441.92335319519043
batch: 524
train_loss: 445.29649543762207
batch: 525
train_loss: 448.73381090164185
batch: 526
train_loss: 452.13110089302063
batch: 527
train_loss: 455.4851291179657
batch: 528
train_loss: 458.8914692401886
batch: 529
train_loss: 462.3001444339752
batch: 530
train_loss: 465.7587251663208
batch: 531
train_loss: 469.16781425476074
batch: 532
train_loss: 472.6392276287079
batch: 533
train_loss: 476.1140193939209
batch: 534
train_loss: 479.52978706359863
batch: 535
train_loss: 482.97971773147583
batch: 536
train_loss: 486.43717646598816
batch: 537
train_loss: 489.9189374446869
batch: 538
train_loss: 493.3886716365814
batch: 539
train_loss: 496.8331365585327
batch: 540
train_loss: 500.3110020160675
batch: 541
train_loss: 503.7782213687897
batch: 542
train_loss: 507.2585999965668
batch: 543
train_loss: 510.700448513031
batch: 544
train_loss: 514.1250801086426
batch: 545
train_loss: 517.558497428894
batch: 546
train_loss: 520.9854695796967
batch: 547
train_loss: 524.4323039054871
batch: 548
train_loss: 527.8891389369965
batch: 549
train_loss: 531.3222389221191
batch: 550
train_loss: 534.7976438999176
batch: 551
train_loss: 538.1916465759277
batch: 552
train_loss: 541.6561887264252
batch: 553
train_loss: 545.0082137584686
batch: 554
train_loss: 548.4051811695099
batch: 555
train_loss: 551.7866363525391
batch: 556
train_loss: 555.2110123634338
batch: 557
train_loss: 558.6057162284851
batch: 558
train_loss: 561.9621639251709
batch: 559
train_loss: 565.3632848262787
batch: 560
train_loss: 568.7949147224426
batch: 561
train_loss: 572.2307851314545
batch: 562
train_loss: 575.7193768024445
batch: 563
train_loss: 579.1912112236023
batch: 564
train_loss: 582.6709911823273
batch: 565
train_loss: 586.1326885223389
batch: 566
train_loss: 589.5995335578918
batch: 567
train_loss: 593.0428295135498
batch: 568
train_loss: 596.532027721405
batch: 569
train_loss: 599.966034412384
batch: 570
train_loss: 603.3404057025909
batch: 571
train_loss: 606.8068563938141
batch: 572
train_loss: 610.1530048847198
batch: 573
train_loss: 613.543051481247
batch: 574
train_loss: 616.8965833187103
batch: 575
train_loss: 620.2511174678802
batch: 576
train_loss: 623.6451988220215
batch: 577
train_loss: 627.115984916687
batch: 578
train_loss: 630.4949431419373
batch: 579
train_loss: 633.9384589195251
batch: 580
train_loss: 637.336984872818
batch: 581
train_loss: 640.7643673419952
batch: 582
train_loss: 644.1737971305847
batch: 583
train_loss: 647.6202166080475
batch: 584
train_loss: 651.0648844242096
batch: 585
train_loss: 654.5104279518127
batch: 586
train_loss: 657.9673888683319
batch: 587
train_loss: 661.464775800705
batch: 588
train_loss: 664.8974275588989
batch: 589
train_loss: 668.330265045166
batch: 590
train_loss: 671.7280988693237
batch: 591
train_loss: 675.1372892856598
batch: 592
train_loss: 678.4850075244904
batch: 593
train_loss: 681.9639620780945
batch: 594
train_loss: 685.4302906990051
batch: 595
train_loss: 688.8822906017303
batch: 596
train_loss: 692.3460283279419
batch: 597
train_loss: 695.8200805187225
batch: 598
train_loss: 699.333322763443
batch: 599
train_loss: 702.8619689941406
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 1419.87 | loss  3.51 | bpc   5.07008
batch: 600
train_loss: 3.574845790863037
batch: 601
train_loss: 7.164745807647705
batch: 602
train_loss: 10.712778329849243
batch: 603
train_loss: 14.136006116867065
batch: 604
train_loss: 17.608057498931885
batch: 605
train_loss: 21.118885278701782
batch: 606
train_loss: 24.630332469940186
batch: 607
train_loss: 28.173972368240356
batch: 608
train_loss: 31.669333457946777
batch: 609
train_loss: 35.09041714668274
batch: 610
train_loss: 38.42866778373718
batch: 611
train_loss: 41.82080936431885
batch: 612
train_loss: 45.149086713790894
batch: 613
train_loss: 48.532015323638916
batch: 614
train_loss: 51.91021752357483
batch: 615
train_loss: 55.364540338516235
batch: 616
train_loss: 58.82417678833008
batch: 617
train_loss: 62.25232148170471
batch: 618
train_loss: 65.70770645141602
batch: 619
train_loss: 69.13228297233582
batch: 620
train_loss: 72.59934306144714
batch: 621
train_loss: 76.06748962402344
batch: 622
train_loss: 79.45537638664246
batch: 623
train_loss: 82.86362171173096
batch: 624
train_loss: 86.25191497802734
batch: 625
train_loss: 89.57722759246826
batch: 626
train_loss: 92.98183727264404
batch: 627
train_loss: 96.41433238983154
batch: 628
train_loss: 99.84657263755798
batch: 629
train_loss: 103.30137753486633
batch: 630
train_loss: 106.72289299964905
batch: 631
train_loss: 110.13191628456116
batch: 632
train_loss: 113.5817346572876
batch: 633
train_loss: 116.9767837524414
batch: 634
train_loss: 120.42342281341553
batch: 635
train_loss: 123.84355759620667
batch: 636
train_loss: 127.24268555641174
batch: 637
train_loss: 130.6055166721344
batch: 638
train_loss: 133.99375200271606
batch: 639
train_loss: 137.38668775558472
batch: 640
train_loss: 140.79579210281372
batch: 641
train_loss: 144.23248195648193
batch: 642
train_loss: 147.6336154937744
batch: 643
train_loss: 151.0355930328369
batch: 644
train_loss: 154.45884442329407
batch: 645
train_loss: 157.90805077552795
batch: 646
train_loss: 161.31062483787537
batch: 647
train_loss: 164.72104740142822
batch: 648
train_loss: 168.16417026519775
batch: 649
train_loss: 171.5937683582306
batch: 650
train_loss: 175.07318878173828
batch: 651
train_loss: 178.48801016807556
batch: 652
train_loss: 181.9216663837433
batch: 653
train_loss: 185.3735547065735
batch: 654
train_loss: 188.782817363739
batch: 655
train_loss: 192.2394368648529
batch: 656
train_loss: 195.64327597618103
batch: 657
train_loss: 199.00176072120667
batch: 658
train_loss: 202.41085267066956
batch: 659
train_loss: 205.82535099983215
batch: 660
train_loss: 209.21864938735962
batch: 661
train_loss: 212.61019897460938
batch: 662
train_loss: 215.99020314216614
batch: 663
train_loss: 219.45346355438232
batch: 664
train_loss: 222.8197524547577
batch: 665
train_loss: 226.18322896957397
batch: 666
train_loss: 229.58353185653687
batch: 667
train_loss: 232.9222710132599
batch: 668
train_loss: 236.28465032577515
batch: 669
train_loss: 239.63651061058044
batch: 670
train_loss: 242.95221424102783
batch: 671
train_loss: 246.32200026512146
batch: 672
train_loss: 249.72590947151184
batch: 673
train_loss: 253.0248031616211
batch: 674
train_loss: 256.37282156944275
batch: 675
train_loss: 259.68241119384766
batch: 676
train_loss: 262.9996566772461
batch: 677
train_loss: 266.3197479248047
batch: 678
train_loss: 269.60760521888733
batch: 679
train_loss: 272.86958408355713
batch: 680
train_loss: 276.18596267700195
batch: 681
train_loss: 279.5248999595642
batch: 682
train_loss: 282.8430333137512
batch: 683
train_loss: 286.1572263240814
batch: 684
train_loss: 289.50805950164795
batch: 685
train_loss: 292.8625433444977
batch: 686
train_loss: 296.2266149520874
batch: 687
train_loss: 299.55264711380005
batch: 688
train_loss: 302.8690068721771
batch: 689
train_loss: 306.21234488487244
batch: 690
train_loss: 309.462842464447
batch: 691
train_loss: 312.72996759414673
batch: 692
train_loss: 316.0712921619415
batch: 693
train_loss: 319.39233922958374
batch: 694
train_loss: 322.6641700267792
batch: 695
train_loss: 325.8939206600189
batch: 696
train_loss: 329.1585760116577
batch: 697
train_loss: 332.4571363925934
batch: 698
train_loss: 335.8021996021271
batch: 699
train_loss: 339.17773389816284
batch: 700
train_loss: 342.51073694229126
batch: 701
train_loss: 345.7895019054413
batch: 702
train_loss: 349.11358094215393
batch: 703
train_loss: 352.4570653438568
batch: 704
train_loss: 355.77131938934326
batch: 705
train_loss: 359.129337310791
batch: 706
train_loss: 362.4041197299957
batch: 707
train_loss: 365.7281608581543
batch: 708
train_loss: 369.04988503456116
batch: 709
train_loss: 372.3901455402374
batch: 710
train_loss: 375.76037883758545
batch: 711
train_loss: 379.0707404613495
batch: 712
train_loss: 382.35790729522705
batch: 713
train_loss: 385.65463423728943
batch: 714
train_loss: 388.95757246017456
batch: 715
train_loss: 392.26461839675903
batch: 716
train_loss: 395.60390877723694
batch: 717
train_loss: 399.003849029541
batch: 718
train_loss: 402.39685130119324
batch: 719
train_loss: 405.78282403945923
batch: 720
train_loss: 409.0436384677887
batch: 721
train_loss: 412.30605697631836
batch: 722
train_loss: 415.66853046417236
batch: 723
train_loss: 419.03426694869995
batch: 724
train_loss: 422.4085259437561
batch: 725
train_loss: 425.63746643066406
batch: 726
train_loss: 428.94700169563293
batch: 727
train_loss: 432.1505002975464
batch: 728
train_loss: 435.3927550315857
batch: 729
train_loss: 438.61186051368713
batch: 730
train_loss: 441.8049910068512
batch: 731
train_loss: 445.02002477645874
batch: 732
train_loss: 448.22339487075806
batch: 733
train_loss: 451.4226076602936
batch: 734
train_loss: 454.62024784088135
batch: 735
train_loss: 457.8131363391876
batch: 736
train_loss: 461.0335714817047
batch: 737
train_loss: 464.28577876091003
batch: 738
train_loss: 467.56621623039246
batch: 739
train_loss: 470.8110589981079
batch: 740
train_loss: 474.03701186180115
batch: 741
train_loss: 477.28305077552795
batch: 742
train_loss: 480.49745416641235
batch: 743
train_loss: 483.732782125473
batch: 744
train_loss: 486.9543967247009
batch: 745
train_loss: 490.2555286884308
batch: 746
train_loss: 493.4951422214508
batch: 747
train_loss: 496.78233528137207
batch: 748
train_loss: 499.9980547428131
batch: 749
train_loss: 503.2112741470337
batch: 750
train_loss: 506.5047769546509
batch: 751
train_loss: 509.6990649700165
batch: 752
train_loss: 512.9390697479248
batch: 753
train_loss: 516.2426083087921
batch: 754
train_loss: 519.6108996868134
batch: 755
train_loss: 522.9781873226166
batch: 756
train_loss: 526.2608511447906
batch: 757
train_loss: 529.5245056152344
batch: 758
train_loss: 532.8179428577423
batch: 759
train_loss: 536.0974555015564
batch: 760
train_loss: 539.3114705085754
batch: 761
train_loss: 542.5499532222748
batch: 762
train_loss: 545.8707122802734
batch: 763
train_loss: 549.1950483322144
batch: 764
train_loss: 552.5104458332062
batch: 765
train_loss: 555.7552363872528
batch: 766
train_loss: 559.0124216079712
batch: 767
train_loss: 562.2802455425262
batch: 768
train_loss: 565.559504032135
batch: 769
train_loss: 568.8338923454285
batch: 770
train_loss: 572.1575000286102
batch: 771
train_loss: 575.4654355049133
batch: 772
train_loss: 578.7658009529114
batch: 773
train_loss: 582.0510656833649
batch: 774
train_loss: 585.366278886795
batch: 775
train_loss: 588.6789572238922
batch: 776
train_loss: 592.0848188400269
batch: 777
train_loss: 595.5351066589355
batch: 778
train_loss: 598.8467490673065
batch: 779
train_loss: 602.1606237888336
batch: 780
train_loss: 605.4956493377686
batch: 781
train_loss: 608.8078770637512
batch: 782
train_loss: 612.0920956134796
batch: 783
train_loss: 615.3929476737976
batch: 784
train_loss: 618.7604479789734
batch: 785
train_loss: 622.0769090652466
batch: 786
train_loss: 625.3072490692139
batch: 787
train_loss: 628.5548403263092
batch: 788
train_loss: 631.8065690994263
batch: 789
train_loss: 635.0671508312225
batch: 790
train_loss: 638.3534951210022
batch: 791
train_loss: 641.6049370765686
batch: 792
train_loss: 644.8833174705505
batch: 793
train_loss: 648.1565983295441
batch: 794
train_loss: 651.426023721695
batch: 795
train_loss: 654.7574062347412
batch: 796
train_loss: 657.94979429245
batch: 797
train_loss: 661.2298181056976
batch: 798
train_loss: 664.466504573822
batch: 799
train_loss: 667.7052929401398
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 1422.79 | loss  3.34 | bpc   4.81648
batch: 800
train_loss: 3.2446746826171875
batch: 801
train_loss: 6.4869890213012695
batch: 802
train_loss: 9.766287326812744
batch: 803
train_loss: 13.013525485992432
batch: 804
train_loss: 16.268636226654053
batch: 805
train_loss: 19.525550842285156
batch: 806
train_loss: 22.85901713371277
batch: 807
train_loss: 26.15641450881958
batch: 808
train_loss: 29.454901456832886
batch: 809
train_loss: 32.74807834625244
batch: 810
train_loss: 36.07449460029602
batch: 811
train_loss: 39.37477397918701
batch: 812
train_loss: 42.68728065490723
batch: 813
train_loss: 45.98060441017151
batch: 814
train_loss: 49.335493087768555
batch: 815
train_loss: 52.703723430633545
batch: 816
train_loss: 56.079018115997314
batch: 817
train_loss: 59.473299980163574
batch: 818
train_loss: 62.85718059539795
batch: 819
train_loss: 66.24516797065735
batch: 820
train_loss: 69.63994789123535
batch: 821
train_loss: 72.96137738227844
batch: 822
train_loss: 76.30802869796753
batch: 823
train_loss: 79.59865379333496
batch: 824
train_loss: 82.8384485244751
batch: 825
train_loss: 86.11329960823059
batch: 826
train_loss: 89.40770053863525
batch: 827
train_loss: 92.64991235733032
batch: 828
train_loss: 95.94249868392944
batch: 829
train_loss: 99.19824743270874
batch: 830
train_loss: 102.45287322998047
batch: 831
train_loss: 105.6898422241211
batch: 832
train_loss: 108.9109423160553
batch: 833
train_loss: 112.108078956604
batch: 834
train_loss: 115.31334805488586
batch: 835
train_loss: 118.53448057174683
batch: 836
train_loss: 121.75280261039734
batch: 837
train_loss: 124.99078488349915
batch: 838
train_loss: 128.23137855529785
batch: 839
train_loss: 131.46965050697327
batch: 840
train_loss: 134.69739723205566
batch: 841
train_loss: 137.95618557929993
batch: 842
train_loss: 141.18766140937805
batch: 843
train_loss: 144.3647425174713
batch: 844
train_loss: 147.6360309123993
batch: 845
train_loss: 150.91139340400696
batch: 846
train_loss: 154.14912462234497
batch: 847
train_loss: 157.35266828536987
batch: 848
train_loss: 160.61111569404602
batch: 849
train_loss: 163.94357109069824
batch: 850
train_loss: 167.1883261203766
batch: 851
train_loss: 170.46048259735107
batch: 852
train_loss: 173.72823524475098
batch: 853
train_loss: 177.048490524292
batch: 854
train_loss: 180.36585330963135
batch: 855
train_loss: 183.62818717956543
batch: 856
train_loss: 186.86352562904358
batch: 857
train_loss: 190.14249205589294
batch: 858
train_loss: 193.4308624267578
batch: 859
train_loss: 196.72594666481018
batch: 860
train_loss: 199.99638867378235
batch: 861
train_loss: 203.22380352020264
batch: 862
train_loss: 206.4660403728485
batch: 863
train_loss: 209.6921694278717
batch: 864
train_loss: 212.94925546646118
batch: 865
train_loss: 216.21947169303894
batch: 866
train_loss: 219.50796675682068
batch: 867
train_loss: 222.81614327430725
batch: 868
train_loss: 226.08287692070007
batch: 869
train_loss: 229.31838250160217
batch: 870
train_loss: 232.5129144191742
batch: 871
train_loss: 235.74816274642944
batch: 872
train_loss: 238.9349343776703
batch: 873
train_loss: 242.14291524887085
batch: 874
train_loss: 245.34275436401367
batch: 875
train_loss: 248.6322705745697
batch: 876
train_loss: 251.8812062740326
batch: 877
train_loss: 255.1127805709839
batch: 878
train_loss: 258.3699862957001
batch: 879
train_loss: 261.6565155982971
batch: 880
train_loss: 264.9501795768738
batch: 881
train_loss: 268.3273591995239
batch: 882
train_loss: 271.76652693748474
batch: 883
train_loss: 275.07584381103516
batch: 884
train_loss: 278.4011573791504
batch: 885
train_loss: 281.72443747520447
batch: 886
train_loss: 285.0520489215851
batch: 887
train_loss: 288.2897593975067
batch: 888
train_loss: 291.58038115501404
batch: 889
train_loss: 294.88261342048645
batch: 890
train_loss: 298.2107849121094
batch: 891
train_loss: 301.44663858413696
batch: 892
train_loss: 304.6564362049103
batch: 893
train_loss: 307.90481424331665
batch: 894
train_loss: 311.17926478385925
batch: 895
train_loss: 314.44501781463623
batch: 896
train_loss: 317.71728587150574
batch: 897
train_loss: 320.9825613498688
batch: 898
train_loss: 324.3453154563904
batch: 899
train_loss: 327.61032223701477
batch: 900
train_loss: 330.9316511154175
batch: 901
train_loss: 334.32397270202637
batch: 902
train_loss: 337.73562717437744
batch: 903
train_loss: 341.0613315105438
batch: 904
train_loss: 344.3111836910248
batch: 905
train_loss: 347.66877698898315
batch: 906
train_loss: 350.9726197719574
batch: 907
train_loss: 354.17184686660767
batch: 908
train_loss: 357.37960386276245
batch: 909
train_loss: 360.61387729644775
batch: 910
train_loss: 363.85803174972534
batch: 911
train_loss: 367.1305100917816
batch: 912
train_loss: 370.35731840133667
batch: 913
train_loss: 373.5363018512726
batch: 914
train_loss: 376.7000422477722
batch: 915
train_loss: 379.92566108703613
batch: 916
train_loss: 383.1280360221863
batch: 917
train_loss: 386.3195526599884
batch: 918
train_loss: 389.5185389518738
batch: 919
train_loss: 392.7152569293976
batch: 920
train_loss: 395.90924978256226
batch: 921
train_loss: 399.11844635009766
batch: 922
train_loss: 402.33454060554504
batch: 923
train_loss: 405.62425088882446
batch: 924
train_loss: 408.8606345653534
batch: 925
train_loss: 412.07210993766785
batch: 926
train_loss: 415.362744808197
batch: 927
train_loss: 418.703759431839
batch: 928
train_loss: 421.9707441329956
batch: 929
train_loss: 425.2817916870117
batch: 930
train_loss: 428.5351927280426
batch: 931
train_loss: 431.7849221229553
batch: 932
train_loss: 435.0245621204376
batch: 933
train_loss: 438.35950803756714
batch: 934
train_loss: 441.6254258155823
batch: 935
train_loss: 444.9166877269745
batch: 936
train_loss: 448.2786476612091
batch: 937
train_loss: 451.61453914642334
batch: 938
train_loss: 454.8612561225891
batch: 939
train_loss: 458.1409537792206
batch: 940
train_loss: 461.46394062042236
batch: 941
train_loss: 464.7815508842468
batch: 942
train_loss: 468.0959162712097
batch: 943
train_loss: 471.39393639564514
batch: 944
train_loss: 474.5963296890259
batch: 945
train_loss: 477.71117639541626
batch: 946
train_loss: 480.93302154541016
batch: 947
train_loss: 484.19377160072327
batch: 948
train_loss: 487.42980432510376
batch: 949
train_loss: 490.679411649704
batch: 950
train_loss: 493.8546419143677
batch: 951
train_loss: 497.0801913738251
batch: 952
train_loss: 500.25150179862976
batch: 953
train_loss: 503.5025734901428
batch: 954
train_loss: 506.7935194969177
batch: 955
train_loss: 510.0724301338196
batch: 956
train_loss: 513.3015401363373
batch: 957
train_loss: 516.4773151874542
batch: 958
train_loss: 519.7200384140015
batch: 959
train_loss: 522.9239521026611
batch: 960
train_loss: 526.1077234745026
batch: 961
train_loss: 529.2988796234131
batch: 962
train_loss: 532.4674685001373
batch: 963
train_loss: 535.6636989116669
batch: 964
train_loss: 538.8354732990265
batch: 965
train_loss: 542.0186350345612
batch: 966
train_loss: 545.2870745658875
batch: 967
train_loss: 548.4952983856201
batch: 968
train_loss: 551.7122921943665
batch: 969
train_loss: 554.956157207489
batch: 970
train_loss: 558.1905477046967
batch: 971
train_loss: 561.3887267112732
batch: 972
train_loss: 564.5115571022034
batch: 973
train_loss: 567.6609346866608
batch: 974
train_loss: 570.8830943107605
batch: 975
train_loss: 574.0775034427643
batch: 976
train_loss: 577.2430102825165
batch: 977
train_loss: 580.4441061019897
batch: 978
train_loss: 583.6382765769958
batch: 979
train_loss: 586.8460533618927
batch: 980
train_loss: 590.0105662345886
batch: 981
train_loss: 593.1145708560944
batch: 982
train_loss: 596.2885568141937
batch: 983
train_loss: 599.4720592498779
batch: 984
train_loss: 602.6551632881165
batch: 985
train_loss: 605.8419055938721
batch: 986
train_loss: 609.0312809944153
batch: 987
train_loss: 612.1518981456757
batch: 988
train_loss: 615.2604751586914
batch: 989
train_loss: 618.4182393550873
batch: 990
train_loss: 621.5551204681396
batch: 991
train_loss: 624.6637823581696
batch: 992
train_loss: 627.7771465778351
batch: 993
train_loss: 630.9045214653015
batch: 994
train_loss: 634.0101177692413
batch: 995
train_loss: 637.1265423297882
batch: 996
train_loss: 640.3020823001862
batch: 997
train_loss: 643.4674699306488
batch: 998
train_loss: 646.6071231365204
batch: 999
train_loss: 649.7382116317749
| epoch   1 step     1000 |   1000 batches | lr 0.00025 | ms/batch 1293.04 | loss  3.25 | bpc   4.68687
batch: 1000
train_loss: 3.130436658859253
batch: 1001
train_loss: 6.262030839920044
batch: 1002
train_loss: 9.410495281219482
batch: 1003
train_loss: 12.553428173065186
batch: 1004
train_loss: 15.70500922203064
batch: 1005
train_loss: 18.91693377494812
batch: 1006
train_loss: 22.089384078979492
batch: 1007
train_loss: 25.21754264831543
batch: 1008
train_loss: 28.41942024230957
batch: 1009
train_loss: 31.534859657287598
batch: 1010
train_loss: 34.65440392494202
batch: 1011
train_loss: 37.74750065803528
batch: 1012
train_loss: 40.94111657142639
batch: 1013
train_loss: 44.20086932182312
batch: 1014
train_loss: 47.42353177070618
batch: 1015
train_loss: 50.56802201271057
batch: 1016
train_loss: 53.71085596084595
batch: 1017
train_loss: 56.921061277389526
batch: 1018
train_loss: 60.157201528549194
batch: 1019
train_loss: 63.348944425582886
batch: 1020
train_loss: 66.6274266242981
batch: 1021
train_loss: 69.8276150226593
batch: 1022
train_loss: 73.05986070632935
batch: 1023
train_loss: 76.29945111274719
batch: 1024
train_loss: 79.58786129951477
batch: 1025
train_loss: 82.83635210990906
batch: 1026
train_loss: 86.03627514839172
batch: 1027
train_loss: 89.29412984848022
batch: 1028
train_loss: 92.53000044822693
batch: 1029
train_loss: 95.68275237083435
batch: 1030
train_loss: 98.80408334732056
batch: 1031
train_loss: 101.92185926437378
batch: 1032
train_loss: 105.13305711746216
batch: 1033
train_loss: 108.2554862499237
batch: 1034
train_loss: 111.33766651153564
batch: 1035
train_loss: 114.4239649772644
batch: 1036
train_loss: 117.54728674888611
batch: 1037
train_loss: 120.67985129356384
batch: 1038
train_loss: 123.81669116020203
batch: 1039
train_loss: 126.94466042518616
batch: 1040
train_loss: 130.04215478897095
batch: 1041
train_loss: 133.14268136024475
batch: 1042
train_loss: 136.22986769676208
batch: 1043
train_loss: 139.310941696167
batch: 1044
train_loss: 142.43680667877197
batch: 1045
train_loss: 145.5648319721222
batch: 1046
train_loss: 148.66505002975464
batch: 1047
train_loss: 151.76240253448486
batch: 1048
train_loss: 154.85426449775696
batch: 1049
train_loss: 157.9528841972351
batch: 1050
train_loss: 161.0406632423401
batch: 1051
train_loss: 164.11071848869324
batch: 1052
train_loss: 167.23712158203125
batch: 1053
train_loss: 170.28734278678894
batch: 1054
train_loss: 173.39513564109802
batch: 1055
train_loss: 176.4935052394867
batch: 1056
train_loss: 179.61168599128723
batch: 1057
train_loss: 182.76404571533203
batch: 1058
train_loss: 185.89355993270874
batch: 1059
train_loss: 189.05057501792908
batch: 1060
train_loss: 192.2139117717743
batch: 1061
train_loss: 195.33798027038574
batch: 1062
train_loss: 198.48555088043213
batch: 1063
train_loss: 201.65157794952393
batch: 1064
train_loss: 204.78712582588196
batch: 1065
train_loss: 207.94492864608765
batch: 1066
train_loss: 211.09521460533142
batch: 1067
train_loss: 214.1985363960266
batch: 1068
train_loss: 217.33690690994263
batch: 1069
train_loss: 220.4967954158783
batch: 1070
train_loss: 223.66870069503784
batch: 1071
train_loss: 226.7634973526001
batch: 1072
train_loss: 229.89173221588135
batch: 1073
train_loss: 232.986234664917
batch: 1074
train_loss: 236.0931851863861
batch: 1075
train_loss: 239.25208640098572
batch: 1076
train_loss: 242.4122610092163
batch: 1077
train_loss: 245.56767344474792
batch: 1078
train_loss: 248.76124548912048
batch: 1079
train_loss: 251.94427704811096
batch: 1080
train_loss: 255.15365982055664
batch: 1081
train_loss: 258.272292137146
batch: 1082
train_loss: 261.5884475708008
batch: 1083
train_loss: 264.80833292007446
batch: 1084
train_loss: 268.0266947746277
batch: 1085
train_loss: 271.1736867427826
batch: 1086
train_loss: 274.3561396598816
batch: 1087
train_loss: 277.5046467781067
batch: 1088
train_loss: 280.67663836479187
batch: 1089
train_loss: 283.8645899295807
batch: 1090
train_loss: 287.09430861473083
batch: 1091
train_loss: 290.3337495326996
batch: 1092
train_loss: 293.50331449508667
batch: 1093
train_loss: 296.6709086894989
batch: 1094
train_loss: 299.81525897979736
batch: 1095
train_loss: 302.8927776813507
batch: 1096
train_loss: 306.06831407546997
batch: 1097
train_loss: 309.1916582584381
batch: 1098
train_loss: 312.41411209106445
batch: 1099
train_loss: 315.56102228164673
batch: 1100
train_loss: 318.68389892578125
batch: 1101
train_loss: 321.8052933216095
batch: 1102
train_loss: 324.9108040332794
batch: 1103
train_loss: 328.10684609413147
batch: 1104
train_loss: 331.33149671554565
batch: 1105
train_loss: 334.48740124702454
batch: 1106
train_loss: 337.6603229045868
batch: 1107
train_loss: 340.84501814842224
batch: 1108
train_loss: 343.9829270839691
batch: 1109
train_loss: 347.10400915145874
batch: 1110
train_loss: 350.1914391517639
batch: 1111
train_loss: 353.3321120738983
batch: 1112
train_loss: 356.51201343536377
batch: 1113
train_loss: 359.6675765514374
batch: 1114
train_loss: 362.80115604400635
batch: 1115
train_loss: 365.91995334625244
batch: 1116
train_loss: 369.13776779174805
batch: 1117
train_loss: 372.3236610889435
batch: 1118
train_loss: 375.4809923171997
batch: 1119
train_loss: 378.65711212158203
batch: 1120
train_loss: 381.8636894226074
batch: 1121
train_loss: 385.00130248069763
batch: 1122
train_loss: 388.17259788513184
batch: 1123
train_loss: 391.3570966720581
batch: 1124
train_loss: 394.47358679771423
batch: 1125
train_loss: 397.7405173778534
batch: 1126
train_loss: 400.9008505344391
batch: 1127
train_loss: 404.06940054893494
batch: 1128
train_loss: 407.2565932273865
batch: 1129
train_loss: 410.38553524017334
batch: 1130
train_loss: 413.5504138469696
batch: 1131
train_loss: 416.6856646537781
batch: 1132
train_loss: 419.8279230594635
batch: 1133
train_loss: 422.99481201171875
batch: 1134
train_loss: 426.2057876586914
batch: 1135
train_loss: 429.3920180797577
batch: 1136
train_loss: 432.5776174068451
batch: 1137
train_loss: 435.79089999198914
batch: 1138
train_loss: 438.9388470649719
batch: 1139
train_loss: 442.13524055480957
batch: 1140
train_loss: 445.34630489349365
batch: 1141
train_loss: 448.4815490245819
batch: 1142
train_loss: 451.60360956192017
batch: 1143
train_loss: 454.7371737957001
batch: 1144
train_loss: 457.91583919525146
batch: 1145
train_loss: 461.06808400154114
batch: 1146
train_loss: 464.1927025318146
batch: 1147
train_loss: 467.32545924186707
batch: 1148
train_loss: 470.4617760181427
batch: 1149
train_loss: 473.62752866744995
batch: 1150
train_loss: 476.73768973350525
batch: 1151
train_loss: 479.90504264831543
batch: 1152
train_loss: 483.06174421310425
batch: 1153
train_loss: 486.18261528015137
batch: 1154
train_loss: 489.3537931442261
batch: 1155
train_loss: 492.4891574382782
batch: 1156
train_loss: 495.6549189090729
batch: 1157
train_loss: 498.86459374427795
batch: 1158
train_loss: 501.9856667518616
batch: 1159
train_loss: 505.128573179245
batch: 1160
train_loss: 508.2467179298401
batch: 1161
train_loss: 511.3585946559906
batch: 1162
train_loss: 514.4700372219086
batch: 1163
train_loss: 517.6485335826874
batch: 1164
train_loss: 520.7414908409119
batch: 1165
train_loss: 523.7989640235901
batch: 1166
train_loss: 526.9017195701599
batch: 1167
train_loss: 530.0084090232849
batch: 1168
train_loss: 533.177020072937
batch: 1169
train_loss: 536.3487257957458
batch: 1170
train_loss: 539.4768788814545
batch: 1171
train_loss: 542.6245527267456
batch: 1172
train_loss: 545.7361814975739
batch: 1173
train_loss: 548.8249907493591
batch: 1174
train_loss: 551.9299108982086
batch: 1175
train_loss: 555.0091733932495
batch: 1176
train_loss: 558.112532377243
batch: 1177
train_loss: 561.2249240875244
batch: 1178
train_loss: 564.2413778305054
batch: 1179
train_loss: 567.2619802951813
batch: 1180
train_loss: 570.3226392269135
batch: 1181
train_loss: 573.4353764057159
batch: 1182
train_loss: 576.5507581233978
batch: 1183
train_loss: 579.6660976409912
batch: 1184
train_loss: 582.7822864055634
batch: 1185
train_loss: 585.9241473674774
batch: 1186
train_loss: 589.168288230896
batch: 1187
train_loss: 592.3290555477142
batch: 1188
train_loss: 595.4850764274597
batch: 1189
train_loss: 598.6949853897095
batch: 1190
train_loss: 601.8895652294159
batch: 1191
train_loss: 605.0872571468353
batch: 1192
train_loss: 608.2994403839111
batch: 1193
train_loss: 611.4927303791046
batch: 1194
train_loss: 614.6079111099243
batch: 1195
train_loss: 617.7262585163116
batch: 1196
train_loss: 620.8106381893158
batch: 1197
train_loss: 623.8646495342255
batch: 1198
train_loss: 626.9858109951019
batch: 1199
train_loss: 630.0550601482391
| epoch   1 step     1200 |   1200 batches | lr 0.000249 | ms/batch 1167.97 | loss  3.15 | bpc   4.54489
batch: 1200
train_loss: 3.0766959190368652
batch: 1201
train_loss: 6.1479270458221436
batch: 1202
train_loss: 9.248818159103394
batch: 1203
train_loss: 12.31419062614441
batch: 1204
train_loss: 15.402498006820679
batch: 1205
train_loss: 18.493403911590576
batch: 1206
train_loss: 21.53207564353943
batch: 1207
train_loss: 24.598012447357178
batch: 1208
train_loss: 27.684436559677124
batch: 1209
train_loss: 30.773454666137695
batch: 1210
train_loss: 33.863558769226074
batch: 1211
train_loss: 36.93964743614197
batch: 1212
train_loss: 40.00871920585632
batch: 1213
train_loss: 43.09334468841553
batch: 1214
train_loss: 46.13894033432007
batch: 1215
train_loss: 49.215893268585205
batch: 1216
train_loss: 52.3185498714447
batch: 1217
train_loss: 55.40642046928406
batch: 1218
train_loss: 58.49275231361389
batch: 1219
train_loss: 61.5403790473938
batch: 1220
train_loss: 64.63019847869873
batch: 1221
train_loss: 67.67298698425293
batch: 1222
train_loss: 70.72746229171753
batch: 1223
train_loss: 73.80926299095154
batch: 1224
train_loss: 76.81315851211548
batch: 1225
train_loss: 79.85715174674988
batch: 1226
train_loss: 82.96897864341736
batch: 1227
train_loss: 86.03306555747986
batch: 1228
train_loss: 89.0850133895874
batch: 1229
train_loss: 92.14963603019714
batch: 1230
train_loss: 95.19985747337341
batch: 1231
train_loss: 98.2615237236023
batch: 1232
train_loss: 101.33306670188904
batch: 1233
train_loss: 104.47494196891785
batch: 1234
train_loss: 107.59874129295349
batch: 1235
train_loss: 110.66596150398254
batch: 1236
train_loss: 113.77134323120117
batch: 1237
train_loss: 116.80328273773193
batch: 1238
train_loss: 119.93078589439392
batch: 1239
train_loss: 123.04625606536865
batch: 1240
train_loss: 126.10281729698181
batch: 1241
train_loss: 129.21752905845642
batch: 1242
train_loss: 132.34140825271606
batch: 1243
train_loss: 135.3890585899353
batch: 1244
train_loss: 138.4192817211151
batch: 1245
train_loss: 141.49864554405212
batch: 1246
train_loss: 144.58687710762024
batch: 1247
train_loss: 147.60803699493408
batch: 1248
train_loss: 150.65106749534607
batch: 1249
train_loss: 153.69862174987793
batch: 1250
train_loss: 156.7102301120758
batch: 1251
train_loss: 159.70516443252563
batch: 1252
train_loss: 162.77192449569702
batch: 1253
train_loss: 165.80558562278748
batch: 1254
train_loss: 168.89854669570923
batch: 1255
train_loss: 171.98719954490662
batch: 1256
train_loss: 175.0977394580841
batch: 1257
train_loss: 178.2400152683258
batch: 1258
train_loss: 181.31406927108765
batch: 1259
train_loss: 184.4273636341095
batch: 1260
train_loss: 187.62094044685364
batch: 1261
train_loss: 190.77774262428284
batch: 1262
train_loss: 193.78784084320068
batch: 1263
train_loss: 196.86357188224792
batch: 1264
train_loss: 199.89524459838867
batch: 1265
train_loss: 203.02926993370056
batch: 1266
train_loss: 206.1173825263977
batch: 1267
train_loss: 209.2561297416687
batch: 1268
train_loss: 212.28093457221985
batch: 1269
train_loss: 215.36936926841736
batch: 1270
train_loss: 218.50416254997253
batch: 1271
train_loss: 221.61758875846863
batch: 1272
train_loss: 224.75432443618774
batch: 1273
train_loss: 227.88919019699097
batch: 1274
train_loss: 230.92538189888
batch: 1275
train_loss: 234.0051326751709
batch: 1276
train_loss: 237.1131341457367
batch: 1277
train_loss: 240.21942472457886
batch: 1278
train_loss: 243.3333432674408
batch: 1279
train_loss: 246.42841386795044
batch: 1280
train_loss: 249.4626386165619
batch: 1281
train_loss: 252.54711961746216
batch: 1282
train_loss: 255.66933417320251
batch: 1283
train_loss: 258.750079870224
batch: 1284
train_loss: 261.82664704322815
batch: 1285
train_loss: 264.8906490802765
batch: 1286
train_loss: 267.9405872821808
batch: 1287
train_loss: 271.02397060394287
batch: 1288
train_loss: 274.1141846179962
batch: 1289
train_loss: 277.2311501502991
batch: 1290
train_loss: 280.28596901893616
batch: 1291
train_loss: 283.35890913009644
batch: 1292
train_loss: 286.4652621746063
batch: 1293
train_loss: 289.5218300819397
batch: 1294
train_loss: 292.5657105445862
batch: 1295
train_loss: 295.5967593193054
batch: 1296
train_loss: 298.66909527778625
batch: 1297
train_loss: 301.696115732193
batch: 1298
train_loss: 304.6562490463257
batch: 1299
train_loss: 307.6322898864746
batch: 1300
train_loss: 310.67588353157043
batch: 1301
train_loss: 313.70356702804565
batch: 1302
train_loss: 316.7582516670227
batch: 1303
train_loss: 319.7841558456421
batch: 1304
train_loss: 322.85011196136475
batch: 1305
train_loss: 325.9614462852478
batch: 1306
train_loss: 328.994567155838
batch: 1307
train_loss: 332.0637605190277
batch: 1308
train_loss: 335.0628023147583
batch: 1309
train_loss: 338.0907099246979
batch: 1310
train_loss: 341.14418601989746
batch: 1311
train_loss: 344.1960566043854
batch: 1312
train_loss: 347.2990982532501
batch: 1313
train_loss: 350.34281396865845
batch: 1314
train_loss: 353.3893599510193
batch: 1315
train_loss: 356.4482636451721
batch: 1316
train_loss: 359.46605610847473
batch: 1317
train_loss: 362.4523000717163
batch: 1318
train_loss: 365.4202992916107
batch: 1319
train_loss: 368.4563250541687
batch: 1320
train_loss: 371.47675681114197
batch: 1321
train_loss: 374.56464195251465
batch: 1322
train_loss: 377.6640431880951
batch: 1323
train_loss: 380.7157642841339
batch: 1324
train_loss: 383.7114267349243
batch: 1325
train_loss: 386.7649292945862
batch: 1326
train_loss: 389.82759165763855
batch: 1327
train_loss: 392.840213060379
batch: 1328
train_loss: 395.91254210472107
batch: 1329
train_loss: 398.96180987358093
batch: 1330
train_loss: 402.013943195343
batch: 1331
train_loss: 405.0921039581299
batch: 1332
train_loss: 408.1569154262543
batch: 1333
train_loss: 411.21802616119385
batch: 1334
train_loss: 414.3447983264923
batch: 1335
train_loss: 417.3616943359375
batch: 1336
train_loss: 420.4297080039978
batch: 1337
train_loss: 423.4772312641144
batch: 1338
train_loss: 426.4581952095032
batch: 1339
train_loss: 429.5212709903717
batch: 1340
train_loss: 432.5534625053406
batch: 1341
train_loss: 435.56270933151245
batch: 1342
train_loss: 438.61883902549744
batch: 1343
train_loss: 441.5647945404053
batch: 1344
train_loss: 444.60456943511963
batch: 1345
train_loss: 447.5242221355438
batch: 1346
train_loss: 450.4713542461395
batch: 1347
train_loss: 453.4465928077698
batch: 1348
train_loss: 456.42491698265076
batch: 1349
train_loss: 459.40202045440674
batch: 1350
train_loss: 462.3150634765625
batch: 1351
train_loss: 465.2643051147461
batch: 1352
train_loss: 468.21315455436707
batch: 1353
train_loss: 471.224187374115
batch: 1354
train_loss: 474.2684590816498
batch: 1355
train_loss: 477.30081701278687
batch: 1356
train_loss: 480.307119846344
batch: 1357
train_loss: 483.3474669456482
batch: 1358
train_loss: 486.34018445014954
batch: 1359
train_loss: 489.39137172698975
batch: 1360
train_loss: 492.38836216926575
batch: 1361
train_loss: 495.3799555301666
batch: 1362
train_loss: 498.3391914367676
batch: 1363
train_loss: 501.3199851512909
batch: 1364
train_loss: 504.30878043174744
batch: 1365
train_loss: 507.2435531616211
batch: 1366
train_loss: 510.19586873054504
batch: 1367
train_loss: 513.2002449035645
batch: 1368
train_loss: 516.1455991268158
batch: 1369
train_loss: 519.0898373126984
batch: 1370
train_loss: 522.0359888076782
batch: 1371
train_loss: 525.0325706005096
batch: 1372
train_loss: 527.9084012508392
batch: 1373
train_loss: 530.8178114891052
batch: 1374
train_loss: 533.7439208030701
batch: 1375
train_loss: 536.6410326957703
batch: 1376
train_loss: 539.5955421924591
batch: 1377
train_loss: 542.6141104698181
batch: 1378
train_loss: 545.6152448654175
batch: 1379
train_loss: 548.6389529705048
batch: 1380
train_loss: 551.67400598526
batch: 1381
train_loss: 554.7211592197418
batch: 1382
train_loss: 557.7700140476227
batch: 1383
train_loss: 560.7530288696289
batch: 1384
train_loss: 563.8477165699005
batch: 1385
train_loss: 566.9451086521149
batch: 1386
train_loss: 570.0381760597229
batch: 1387
train_loss: 573.1712806224823
batch: 1388
train_loss: 576.2828526496887
batch: 1389
train_loss: 579.255172252655
batch: 1390
train_loss: 582.3180832862854
batch: 1391
train_loss: 585.3388574123383
batch: 1392
train_loss: 588.408670425415
batch: 1393
train_loss: 591.4497866630554
batch: 1394
train_loss: 594.4892978668213
batch: 1395
train_loss: 597.5661764144897
batch: 1396
train_loss: 600.6604704856873
batch: 1397
train_loss: 603.7154614925385
batch: 1398
train_loss: 606.8176634311676
batch: 1399
train_loss: 610.0018966197968
| epoch   1 step     1400 |   1400 batches | lr 0.000249 | ms/batch 1171.84 | loss  3.05 | bpc   4.40023
batch: 1400
train_loss: 3.004699468612671
batch: 1401
train_loss: 5.993670463562012
batch: 1402
train_loss: 9.006977558135986
batch: 1403
train_loss: 12.020495414733887
batch: 1404
train_loss: 15.051113605499268
batch: 1405
train_loss: 18.116962909698486
batch: 1406
train_loss: 21.113831281661987
batch: 1407
train_loss: 24.133403778076172
batch: 1408
train_loss: 27.22121572494507
batch: 1409
train_loss: 30.260152339935303
batch: 1410
train_loss: 33.298454999923706
batch: 1411
train_loss: 36.24239897727966
batch: 1412
train_loss: 39.28001403808594
batch: 1413
train_loss: 42.30664849281311
batch: 1414
train_loss: 45.318663358688354
batch: 1415
train_loss: 48.3896381855011
batch: 1416
train_loss: 51.464329957962036
batch: 1417
train_loss: 54.535778522491455
batch: 1418
train_loss: 57.548563957214355
batch: 1419
train_loss: 60.5698184967041
batch: 1420
train_loss: 63.598389625549316
batch: 1421
train_loss: 66.62097930908203
batch: 1422
train_loss: 69.63586187362671
batch: 1423
train_loss: 72.57403182983398
batch: 1424
train_loss: 75.55554056167603
batch: 1425
train_loss: 78.54005265235901
batch: 1426
train_loss: 81.56012201309204
batch: 1427
train_loss: 84.5880753993988
batch: 1428
train_loss: 87.65543746948242
batch: 1429
train_loss: 90.61520481109619
batch: 1430
train_loss: 93.5610818862915
batch: 1431
train_loss: 96.57452273368835
batch: 1432
train_loss: 99.58370018005371
batch: 1433
train_loss: 102.65549492835999
batch: 1434
train_loss: 105.6264762878418
batch: 1435
train_loss: 108.66497492790222
batch: 1436
train_loss: 111.70062565803528
batch: 1437
train_loss: 114.6353554725647
batch: 1438
train_loss: 117.6244683265686
batch: 1439
train_loss: 120.67743515968323
batch: 1440
train_loss: 123.73075866699219
batch: 1441
train_loss: 126.77851796150208
batch: 1442
train_loss: 129.88647866249084
batch: 1443
train_loss: 132.95084238052368
batch: 1444
train_loss: 135.89390206336975
batch: 1445
train_loss: 138.82384872436523
batch: 1446
train_loss: 141.73503422737122
batch: 1447
train_loss: 144.656423330307
batch: 1448
train_loss: 147.57862401008606
batch: 1449
train_loss: 150.41415691375732
batch: 1450
train_loss: 153.26582503318787
batch: 1451
train_loss: 156.18487238883972
batch: 1452
train_loss: 159.0300087928772
batch: 1453
train_loss: 161.9253795146942
batch: 1454
train_loss: 164.8314037322998
batch: 1455
train_loss: 167.7044563293457
batch: 1456
train_loss: 170.62469792366028
batch: 1457
train_loss: 173.56923151016235
batch: 1458
train_loss: 176.52400064468384
batch: 1459
train_loss: 179.43704462051392
batch: 1460
train_loss: 182.33816862106323
batch: 1461
train_loss: 185.25826001167297
batch: 1462
train_loss: 188.1927809715271
batch: 1463
train_loss: 191.1095426082611
batch: 1464
train_loss: 194.08926725387573
batch: 1465
train_loss: 197.0831172466278
batch: 1466
train_loss: 200.01546907424927
batch: 1467
train_loss: 202.93136882781982
batch: 1468
train_loss: 205.92360496520996
batch: 1469
train_loss: 208.923921585083
batch: 1470
train_loss: 211.96562314033508
batch: 1471
train_loss: 214.91507863998413
batch: 1472
train_loss: 217.82838487625122
batch: 1473
train_loss: 220.78932857513428
batch: 1474
train_loss: 223.72962188720703
batch: 1475
train_loss: 226.6203305721283
batch: 1476
train_loss: 229.51463079452515
batch: 1477
train_loss: 232.46200108528137
batch: 1478
train_loss: 235.3712751865387
batch: 1479
train_loss: 238.32370734214783
batch: 1480
train_loss: 241.20400094985962
batch: 1481
train_loss: 244.09724497795105
batch: 1482
train_loss: 247.05748343467712
batch: 1483
train_loss: 250.081556558609
batch: 1484
train_loss: 253.0029582977295
batch: 1485
train_loss: 255.93916726112366
batch: 1486
train_loss: 258.83674454689026
batch: 1487
train_loss: 261.8318238258362
batch: 1488
train_loss: 264.8105194568634
batch: 1489
train_loss: 267.70976090431213
batch: 1490
train_loss: 270.697092294693
batch: 1491
train_loss: 273.6515805721283
batch: 1492
train_loss: 276.6213948726654
batch: 1493
train_loss: 279.6526472568512
batch: 1494
train_loss: 282.6062569618225
batch: 1495
train_loss: 285.62296295166016
batch: 1496
train_loss: 288.63801431655884
batch: 1497
train_loss: 291.7421724796295
batch: 1498
train_loss: 294.78688740730286
batch: 1499
train_loss: 297.9064927101135
batch: 1500
train_loss: 300.9071776866913
batch: 1501
train_loss: 303.83713245391846
batch: 1502
train_loss: 306.8450129032135
batch: 1503
train_loss: 309.84359407424927
batch: 1504
train_loss: 312.84190034866333
batch: 1505
train_loss: 315.87220644950867
batch: 1506
train_loss: 318.9155068397522
batch: 1507
train_loss: 321.9871871471405
batch: 1508
train_loss: 324.8999960422516
batch: 1509
train_loss: 327.9602665901184
batch: 1510
train_loss: 330.92048621177673
batch: 1511
train_loss: 333.9112637042999
batch: 1512
train_loss: 336.95060563087463
batch: 1513
train_loss: 339.93454813957214
batch: 1514
train_loss: 342.8745620250702
batch: 1515
train_loss: 345.8615758419037
batch: 1516
train_loss: 348.8521990776062
batch: 1517
train_loss: 351.8779239654541
batch: 1518
train_loss: 354.8590877056122
batch: 1519
train_loss: 357.8171911239624
batch: 1520
train_loss: 360.81150102615356
batch: 1521
train_loss: 363.7828884124756
batch: 1522
train_loss: 366.74503898620605
batch: 1523
train_loss: 369.743812084198
batch: 1524
train_loss: 372.6791605949402
batch: 1525
train_loss: 375.62119603157043
batch: 1526
train_loss: 378.5548746585846
batch: 1527
train_loss: 381.497230052948
batch: 1528
train_loss: 384.3894112110138
batch: 1529
train_loss: 387.25650358200073
batch: 1530
train_loss: 390.1609559059143
batch: 1531
train_loss: 393.07807326316833
batch: 1532
train_loss: 395.9580421447754
batch: 1533
train_loss: 398.922162771225
batch: 1534
train_loss: 401.7985019683838
batch: 1535
train_loss: 404.76681995391846
batch: 1536
train_loss: 407.69811725616455
batch: 1537
train_loss: 410.63219928741455
batch: 1538
train_loss: 413.48958015441895
batch: 1539
train_loss: 416.5022814273834
batch: 1540
train_loss: 419.4206073284149
batch: 1541
train_loss: 422.4041805267334
batch: 1542
train_loss: 425.41956996917725
batch: 1543
train_loss: 428.4072332382202
batch: 1544
train_loss: 431.3928151130676
batch: 1545
train_loss: 434.2972493171692
batch: 1546
train_loss: 437.2795310020447
batch: 1547
train_loss: 440.20845651626587
batch: 1548
train_loss: 443.24108600616455
batch: 1549
train_loss: 446.2400691509247
batch: 1550
train_loss: 449.21430015563965
batch: 1551
train_loss: 452.1581914424896
batch: 1552
train_loss: 455.0904891490936
batch: 1553
train_loss: 458.09713792800903
batch: 1554
train_loss: 461.0290586948395
batch: 1555
train_loss: 464.0548641681671
batch: 1556
train_loss: 467.0407919883728
batch: 1557
train_loss: 470.0366921424866
batch: 1558
train_loss: 473.0329806804657
batch: 1559
train_loss: 476.02050590515137
batch: 1560
train_loss: 478.98844861984253
batch: 1561
train_loss: 482.0742619037628
batch: 1562
train_loss: 485.080774307251
batch: 1563
train_loss: 488.0661447048187
batch: 1564
train_loss: 490.924081325531
batch: 1565
train_loss: 493.7838854789734
batch: 1566
train_loss: 496.62328004837036
batch: 1567
train_loss: 499.56973028182983
batch: 1568
train_loss: 502.450555562973
batch: 1569
train_loss: 505.36673045158386
batch: 1570
train_loss: 508.31804943084717
batch: 1571
train_loss: 511.2033588886261
batch: 1572
train_loss: 514.1337542533875
batch: 1573
train_loss: 517.0514109134674
batch: 1574
train_loss: 519.9583039283752
batch: 1575
train_loss: 522.8867743015289
batch: 1576
train_loss: 525.7750074863434
batch: 1577
train_loss: 528.683534860611
batch: 1578
train_loss: 531.6442806720734
batch: 1579
train_loss: 534.5420355796814
batch: 1580
train_loss: 537.4553639888763
batch: 1581
train_loss: 540.4009945392609
batch: 1582
train_loss: 543.388799905777
batch: 1583
train_loss: 546.359263420105
batch: 1584
train_loss: 549.4069716930389
batch: 1585
train_loss: 552.4522829055786
batch: 1586
train_loss: 555.4187552928925
batch: 1587
train_loss: 558.3632123470306
batch: 1588
train_loss: 561.3007953166962
batch: 1589
train_loss: 564.1920194625854
batch: 1590
train_loss: 567.0852019786835
batch: 1591
train_loss: 570.0456960201263
batch: 1592
train_loss: 572.9541013240814
batch: 1593
train_loss: 575.874344587326
batch: 1594
train_loss: 578.7462735176086
batch: 1595
train_loss: 581.6587264537811
batch: 1596
train_loss: 584.4734189510345
batch: 1597
train_loss: 587.3514823913574
batch: 1598
train_loss: 590.1854345798492
batch: 1599
train_loss: 593.0190789699554
| epoch   1 step     1600 |   1600 batches | lr 0.000249 | ms/batch 1164.90 | loss  2.97 | bpc   4.27773
batch: 1600
train_loss: 2.7841317653656006
batch: 1601
train_loss: 5.624590635299683
batch: 1602
train_loss: 8.461591482162476
batch: 1603
train_loss: 11.26577377319336
batch: 1604
train_loss: 14.105203866958618
batch: 1605
train_loss: 16.91823434829712
batch: 1606
train_loss: 19.8490571975708
batch: 1607
train_loss: 22.81044864654541
batch: 1608
train_loss: 25.844545364379883
batch: 1609
train_loss: 28.698705911636353
batch: 1610
train_loss: 31.676780700683594
batch: 1611
train_loss: 34.60384821891785
batch: 1612
train_loss: 37.51708912849426
batch: 1613
train_loss: 40.45976185798645
batch: 1614
train_loss: 43.36992335319519
batch: 1615
train_loss: 46.23183059692383
batch: 1616
train_loss: 49.10821032524109
batch: 1617
train_loss: 52.08235836029053
batch: 1618
train_loss: 55.1184024810791
batch: 1619
train_loss: 58.08171629905701
batch: 1620
train_loss: 60.982645750045776
batch: 1621
train_loss: 63.864737033843994
batch: 1622
train_loss: 66.7926721572876
batch: 1623
train_loss: 69.69245100021362
batch: 1624
train_loss: 72.63813877105713
batch: 1625
train_loss: 75.51325631141663
batch: 1626
train_loss: 78.475026845932
batch: 1627
train_loss: 81.41220426559448
batch: 1628
train_loss: 84.36948823928833
batch: 1629
train_loss: 87.40764570236206
batch: 1630
train_loss: 90.49239301681519
batch: 1631
train_loss: 93.47728681564331
batch: 1632
train_loss: 96.43709349632263
batch: 1633
train_loss: 99.42386102676392
batch: 1634
train_loss: 102.32502388954163
batch: 1635
train_loss: 105.33270955085754
batch: 1636
train_loss: 108.28699493408203
batch: 1637
train_loss: 111.27006816864014
batch: 1638
train_loss: 114.17260408401489
batch: 1639
train_loss: 117.05697464942932
batch: 1640
train_loss: 119.96493911743164
batch: 1641
train_loss: 122.82630681991577
batch: 1642
train_loss: 125.74841690063477
batch: 1643
train_loss: 128.75975894927979
batch: 1644
train_loss: 131.72735929489136
batch: 1645
train_loss: 134.66214084625244
batch: 1646
train_loss: 137.63281965255737
batch: 1647
train_loss: 140.54707264900208
batch: 1648
train_loss: 143.43836379051208
batch: 1649
train_loss: 146.37677311897278
batch: 1650
train_loss: 149.31292295455933
batch: 1651
train_loss: 152.21618056297302
batch: 1652
train_loss: 155.25974655151367
batch: 1653
train_loss: 158.19043636322021
batch: 1654
train_loss: 161.0572054386139
batch: 1655
train_loss: 164.01046013832092
batch: 1656
train_loss: 166.90342330932617
batch: 1657
train_loss: 169.80304193496704
batch: 1658
train_loss: 172.67617392539978
batch: 1659
train_loss: 175.5577449798584
batch: 1660
train_loss: 178.50911498069763
batch: 1661
train_loss: 181.4053041934967
batch: 1662
train_loss: 184.36966276168823
batch: 1663
train_loss: 187.28040504455566
batch: 1664
train_loss: 190.264386177063
batch: 1665
train_loss: 193.26323437690735
batch: 1666
train_loss: 196.26457166671753
batch: 1667
train_loss: 199.23094367980957
batch: 1668
train_loss: 202.14435029029846
batch: 1669
train_loss: 205.11175918579102
batch: 1670
train_loss: 208.0235390663147
batch: 1671
train_loss: 210.95833373069763
batch: 1672
train_loss: 213.95826029777527
batch: 1673
train_loss: 216.9776632785797
batch: 1674
train_loss: 219.99962067604065
batch: 1675
train_loss: 223.03609442710876
batch: 1676
train_loss: 225.9676160812378
batch: 1677
train_loss: 228.88801455497742
batch: 1678
train_loss: 231.84008264541626
batch: 1679
train_loss: 234.8202006816864
batch: 1680
train_loss: 237.7880823612213
batch: 1681
train_loss: 240.7711524963379
batch: 1682
train_loss: 243.62309646606445
batch: 1683
train_loss: 246.53960037231445
batch: 1684
train_loss: 249.46992588043213
batch: 1685
train_loss: 252.4869668483734
batch: 1686
train_loss: 255.4092059135437
batch: 1687
train_loss: 258.3136956691742
batch: 1688
train_loss: 261.1745231151581
batch: 1689
train_loss: 264.1080963611603
batch: 1690
train_loss: 267.03089451789856
batch: 1691
train_loss: 270.03743600845337
batch: 1692
train_loss: 273.02052998542786
batch: 1693
train_loss: 276.0106568336487
batch: 1694
train_loss: 278.92327976226807
batch: 1695
train_loss: 281.7871072292328
batch: 1696
train_loss: 284.6605975627899
batch: 1697
train_loss: 287.4940686225891
batch: 1698
train_loss: 290.40285873413086
batch: 1699
train_loss: 293.3186774253845
batch: 1700
train_loss: 296.2763030529022
batch: 1701
train_loss: 299.24968957901
batch: 1702
train_loss: 302.1715757846832
batch: 1703
train_loss: 305.12194871902466
batch: 1704
train_loss: 308.0232539176941
batch: 1705
train_loss: 310.96583366394043
batch: 1706
train_loss: 313.88982129096985
batch: 1707
train_loss: 316.86345958709717
batch: 1708
train_loss: 319.7947986125946
batch: 1709
train_loss: 322.73978447914124
batch: 1710
train_loss: 325.6488630771637
batch: 1711
train_loss: 328.5661118030548
batch: 1712
train_loss: 331.4370951652527
batch: 1713
train_loss: 334.29147028923035
batch: 1714
train_loss: 337.1764805316925
batch: 1715
train_loss: 340.12018489837646
batch: 1716
train_loss: 343.0134971141815
batch: 1717
train_loss: 345.9169125556946
batch: 1718
train_loss: 348.85987877845764
batch: 1719
train_loss: 351.73350524902344
batch: 1720
train_loss: 354.5706236362457
batch: 1721
train_loss: 357.41198563575745
batch: 1722
train_loss: 360.2070896625519
batch: 1723
train_loss: 363.0894284248352
batch: 1724
train_loss: 365.89582443237305
batch: 1725
train_loss: 368.7651319503784
batch: 1726
train_loss: 371.61181712150574
batch: 1727
train_loss: 374.3265070915222
batch: 1728
train_loss: 377.0688896179199
batch: 1729
train_loss: 379.83242321014404
batch: 1730
train_loss: 382.6663887500763
batch: 1731
train_loss: 385.44910860061646
batch: 1732
train_loss: 388.28530383110046
batch: 1733
train_loss: 391.1415545940399
batch: 1734
train_loss: 394.0521981716156
batch: 1735
train_loss: 396.9120578765869
batch: 1736
train_loss: 399.6997883319855
batch: 1737
train_loss: 402.5117666721344
batch: 1738
train_loss: 405.4164867401123
batch: 1739
train_loss: 408.2202386856079
batch: 1740
train_loss: 411.03633880615234
batch: 1741
train_loss: 413.7938241958618
batch: 1742
train_loss: 416.601331949234
batch: 1743
train_loss: 419.37678599357605
batch: 1744
train_loss: 422.14250326156616
batch: 1745
train_loss: 424.90283489227295
batch: 1746
train_loss: 427.7082760334015
batch: 1747
train_loss: 430.58705401420593
batch: 1748
train_loss: 433.36925745010376
batch: 1749
train_loss: 436.1570725440979
batch: 1750
train_loss: 438.91859197616577
batch: 1751
train_loss: 441.7271566390991
batch: 1752
train_loss: 444.5235962867737
batch: 1753
train_loss: 447.2895128726959
batch: 1754
train_loss: 450.0876910686493
batch: 1755
train_loss: 452.98246932029724
batch: 1756
train_loss: 455.8709716796875
batch: 1757
train_loss: 458.69946670532227
batch: 1758
train_loss: 461.5447657108307
batch: 1759
train_loss: 464.51828265190125
batch: 1760
train_loss: 467.3919756412506
batch: 1761
train_loss: 470.24647307395935
batch: 1762
train_loss: 473.05327701568604
batch: 1763
train_loss: 475.9064726829529
batch: 1764
train_loss: 478.6436822414398
batch: 1765
train_loss: 481.52529430389404
batch: 1766
train_loss: 484.34884691238403
batch: 1767
train_loss: 487.1570987701416
batch: 1768
train_loss: 489.95992255210876
batch: 1769
train_loss: 492.72070813179016
batch: 1770
train_loss: 495.5645580291748
batch: 1771
train_loss: 498.4634835720062
batch: 1772
train_loss: 501.2605814933777
batch: 1773
train_loss: 504.1079752445221
batch: 1774
train_loss: 506.9385509490967
batch: 1775
train_loss: 509.79823994636536
batch: 1776
train_loss: 512.6763427257538
batch: 1777
train_loss: 515.6045036315918
batch: 1778
train_loss: 518.554429769516
batch: 1779
train_loss: 521.4064249992371
batch: 1780
train_loss: 524.290002822876
batch: 1781
train_loss: 527.1775853633881
batch: 1782
train_loss: 530.0141768455505
batch: 1783
train_loss: 532.8760673999786
batch: 1784
train_loss: 535.7480597496033
batch: 1785
train_loss: 538.6667573451996
batch: 1786
train_loss: 541.5166110992432
batch: 1787
train_loss: 544.4377779960632
batch: 1788
train_loss: 547.3200056552887
batch: 1789
train_loss: 550.2455503940582
batch: 1790
train_loss: 553.0132298469543
batch: 1791
train_loss: 555.8542203903198
batch: 1792
train_loss: 558.7537009716034
batch: 1793
train_loss: 561.5814561843872
batch: 1794
train_loss: 564.3593006134033
batch: 1795
train_loss: 567.18177485466
batch: 1796
train_loss: 570.0607614517212
batch: 1797
train_loss: 572.8805496692657
batch: 1798
train_loss: 575.7381362915039
batch: 1799
train_loss: 578.5710608959198
| epoch   1 step     1800 |   1800 batches | lr 0.000249 | ms/batch 1166.09 | loss  2.89 | bpc   4.17351
batch: 1800
train_loss: 2.9105467796325684
batch: 1801
train_loss: 5.798896789550781
batch: 1802
train_loss: 8.74929666519165
batch: 1803
train_loss: 11.651527643203735
batch: 1804
train_loss: 14.575291872024536
batch: 1805
train_loss: 17.425500869750977
batch: 1806
train_loss: 20.4537410736084
batch: 1807
train_loss: 23.4700345993042
batch: 1808
train_loss: 26.32671618461609
batch: 1809
train_loss: 29.223498106002808
batch: 1810
train_loss: 32.12016558647156
batch: 1811
train_loss: 35.03450059890747
batch: 1812
train_loss: 37.911051750183105
batch: 1813
train_loss: 40.72985577583313
batch: 1814
train_loss: 43.639418601989746
batch: 1815
train_loss: 46.484649896621704
batch: 1816
train_loss: 49.28523516654968
batch: 1817
train_loss: 52.12952995300293
batch: 1818
train_loss: 54.994571924209595
batch: 1819
train_loss: 57.89173650741577
batch: 1820
train_loss: 60.81667709350586
batch: 1821
train_loss: 63.680195808410645
batch: 1822
train_loss: 66.61485004425049
batch: 1823
train_loss: 69.57448697090149
batch: 1824
train_loss: 72.47346901893616
batch: 1825
train_loss: 75.45308232307434
batch: 1826
train_loss: 78.33952856063843
batch: 1827
train_loss: 81.19739317893982
batch: 1828
train_loss: 84.17428922653198
batch: 1829
train_loss: 87.10864806175232
batch: 1830
train_loss: 90.11252975463867
batch: 1831
train_loss: 93.08308339118958
batch: 1832
train_loss: 96.07313919067383
batch: 1833
train_loss: 99.03590679168701
batch: 1834
train_loss: 101.95601963996887
batch: 1835
train_loss: 104.89929413795471
batch: 1836
train_loss: 107.8138837814331
batch: 1837
train_loss: 110.74474143981934
batch: 1838
train_loss: 113.67956352233887
batch: 1839
train_loss: 116.55941104888916
batch: 1840
train_loss: 119.35357475280762
batch: 1841
train_loss: 122.28315758705139
batch: 1842
train_loss: 125.19912600517273
batch: 1843
train_loss: 128.06711387634277
batch: 1844
train_loss: 130.92190861701965
batch: 1845
train_loss: 133.8060109615326
batch: 1846
train_loss: 136.7044665813446
batch: 1847
train_loss: 139.5439670085907
batch: 1848
train_loss: 142.33232927322388
batch: 1849
train_loss: 145.2297706604004
batch: 1850
train_loss: 148.0749852657318
batch: 1851
train_loss: 150.94280672073364
batch: 1852
train_loss: 153.92483186721802
batch: 1853
train_loss: 156.9688093662262
batch: 1854
train_loss: 159.87962579727173
batch: 1855
train_loss: 162.8296091556549
batch: 1856
train_loss: 165.8315372467041
batch: 1857
train_loss: 168.67171454429626
batch: 1858
train_loss: 171.62106275558472
batch: 1859
train_loss: 174.55102705955505
batch: 1860
train_loss: 177.41463017463684
batch: 1861
train_loss: 180.2954182624817
batch: 1862
train_loss: 183.17454504966736
batch: 1863
train_loss: 186.09881973266602
batch: 1864
train_loss: 188.89333319664001
batch: 1865
train_loss: 191.77796697616577
batch: 1866
train_loss: 194.61546516418457
batch: 1867
train_loss: 197.53305530548096
batch: 1868
train_loss: 200.29642343521118
batch: 1869
train_loss: 203.226797580719
batch: 1870
train_loss: 206.11517477035522
batch: 1871
train_loss: 208.955313205719
batch: 1872
train_loss: 211.81164598464966
batch: 1873
train_loss: 214.68984842300415
batch: 1874
train_loss: 217.49450087547302
batch: 1875
train_loss: 220.3370988368988
batch: 1876
train_loss: 223.2253119945526
batch: 1877
train_loss: 226.04272198677063
batch: 1878
train_loss: 228.88781452178955
batch: 1879
train_loss: 231.85556292533875
batch: 1880
train_loss: 234.73492169380188
batch: 1881
train_loss: 237.6274425983429
batch: 1882
train_loss: 240.56463813781738
batch: 1883
train_loss: 243.478435754776
batch: 1884
train_loss: 246.47606587409973
batch: 1885
train_loss: 249.37743878364563
batch: 1886
train_loss: 252.3142466545105
batch: 1887
train_loss: 255.15545129776
batch: 1888
train_loss: 257.9748332500458
batch: 1889
train_loss: 260.82381534576416
batch: 1890
train_loss: 263.7317786216736
batch: 1891
train_loss: 266.66599345207214
batch: 1892
train_loss: 269.52962732315063
batch: 1893
train_loss: 272.43164467811584
batch: 1894
train_loss: 275.3994781970978
batch: 1895
train_loss: 278.3616783618927
batch: 1896
train_loss: 281.34032464027405
batch: 1897
train_loss: 284.2742066383362
batch: 1898
train_loss: 287.18395829200745
batch: 1899
train_loss: 290.12655377388
batch: 1900
train_loss: 293.1710958480835
batch: 1901
train_loss: 296.1475896835327
batch: 1902
train_loss: 299.13174510002136
batch: 1903
train_loss: 302.0614364147186
batch: 1904
train_loss: 304.9437618255615
batch: 1905
train_loss: 307.8412311077118
batch: 1906
train_loss: 310.8334171772003
batch: 1907
train_loss: 313.7371897697449
batch: 1908
train_loss: 316.6593255996704
batch: 1909
train_loss: 319.6071047782898
batch: 1910
train_loss: 322.4959661960602
batch: 1911
train_loss: 325.30470752716064
batch: 1912
train_loss: 328.12856125831604
batch: 1913
train_loss: 331.00592708587646
batch: 1914
train_loss: 333.933354139328
batch: 1915
train_loss: 336.78892993927
batch: 1916
train_loss: 339.66354846954346
batch: 1917
train_loss: 342.5178496837616
batch: 1918
train_loss: 345.33728075027466
batch: 1919
train_loss: 348.1672990322113
batch: 1920
train_loss: 350.9675877094269
batch: 1921
train_loss: 353.75794649124146
batch: 1922
train_loss: 356.639023065567
batch: 1923
train_loss: 359.4302890300751
batch: 1924
train_loss: 362.22146940231323
batch: 1925
train_loss: 364.99921011924744
batch: 1926
train_loss: 367.83751702308655
batch: 1927
train_loss: 370.63138246536255
batch: 1928
train_loss: 373.48314142227173
batch: 1929
train_loss: 376.34010577201843
batch: 1930
train_loss: 379.1597967147827
batch: 1931
train_loss: 382.03060007095337
batch: 1932
train_loss: 384.9485182762146
batch: 1933
train_loss: 387.7615694999695
batch: 1934
train_loss: 390.6427719593048
batch: 1935
train_loss: 393.54029178619385
batch: 1936
train_loss: 396.3940670490265
batch: 1937
train_loss: 399.26832342147827
batch: 1938
train_loss: 402.10092186927795
batch: 1939
train_loss: 404.92673921585083
batch: 1940
train_loss: 407.7661678791046
batch: 1941
train_loss: 410.6357855796814
batch: 1942
train_loss: 413.48496103286743
batch: 1943
train_loss: 416.3007872104645
batch: 1944
train_loss: 419.23007130622864
batch: 1945
train_loss: 422.0559070110321
batch: 1946
train_loss: 424.9508054256439
batch: 1947
train_loss: 427.8252878189087
batch: 1948
train_loss: 430.721417427063
batch: 1949
train_loss: 433.6603150367737
batch: 1950
train_loss: 436.5982737541199
batch: 1951
train_loss: 439.3934235572815
batch: 1952
train_loss: 442.23342633247375
batch: 1953
train_loss: 445.0958528518677
batch: 1954
train_loss: 447.9108245372772
batch: 1955
train_loss: 450.65419125556946
batch: 1956
train_loss: 453.3755669593811
batch: 1957
train_loss: 456.2125418186188
batch: 1958
train_loss: 459.05089116096497
batch: 1959
train_loss: 461.8427276611328
batch: 1960
train_loss: 464.6218903064728
batch: 1961
train_loss: 467.43958616256714
batch: 1962
train_loss: 470.21321725845337
batch: 1963
train_loss: 472.9849519729614
batch: 1964
train_loss: 475.74279594421387
batch: 1965
train_loss: 478.5111210346222
batch: 1966
train_loss: 481.3662853240967
batch: 1967
train_loss: 484.17073106765747
batch: 1968
train_loss: 487.00761556625366
batch: 1969
train_loss: 489.8626263141632
batch: 1970
train_loss: 492.700115442276
batch: 1971
train_loss: 495.4653582572937
batch: 1972
train_loss: 498.3291349411011
batch: 1973
train_loss: 501.10308504104614
batch: 1974
train_loss: 503.9723057746887
batch: 1975
train_loss: 506.77781558036804
batch: 1976
train_loss: 509.5507757663727
batch: 1977
train_loss: 512.3161504268646
batch: 1978
train_loss: 515.1765582561493
batch: 1979
train_loss: 517.969568490982
batch: 1980
train_loss: 520.93124127388
batch: 1981
train_loss: 523.7041432857513
batch: 1982
train_loss: 526.5308651924133
batch: 1983
train_loss: 529.4712300300598
batch: 1984
train_loss: 532.3557276725769
batch: 1985
train_loss: 535.187198638916
batch: 1986
train_loss: 537.9425637722015
batch: 1987
train_loss: 540.7670252323151
batch: 1988
train_loss: 543.6030259132385
batch: 1989
train_loss: 546.3968732357025
batch: 1990
train_loss: 549.1360812187195
batch: 1991
train_loss: 551.9006810188293
batch: 1992
train_loss: 554.7322382926941
batch: 1993
train_loss: 557.490656375885
batch: 1994
train_loss: 560.3648581504822
batch: 1995
train_loss: 563.2020313739777
batch: 1996
train_loss: 566.031726360321
batch: 1997
train_loss: 568.8575208187103
batch: 1998
train_loss: 571.7231593132019
batch: 1999
train_loss: 574.6020791530609
| epoch   1 step     2000 |   2000 batches | lr 0.000248 | ms/batch 1167.93 | loss  2.87 | bpc   4.14488
batch: 2000
train_loss: 2.831075429916382
batch: 2001
train_loss: 5.699024200439453
batch: 2002
train_loss: 8.660249471664429
batch: 2003
train_loss: 11.60442590713501
batch: 2004
train_loss: 14.588919162750244
batch: 2005
train_loss: 17.45434594154358
batch: 2006
train_loss: 20.345247983932495
batch: 2007
train_loss: 23.21818447113037
batch: 2008
train_loss: 26.078969478607178
batch: 2009
train_loss: 28.913092851638794
batch: 2010
train_loss: 31.81075882911682
batch: 2011
train_loss: 34.703102827072144
batch: 2012
train_loss: 37.59543752670288
batch: 2013
train_loss: 40.63618850708008
batch: 2014
train_loss: 43.67543339729309
batch: 2015
train_loss: 46.6523163318634
batch: 2016
train_loss: 49.576234579086304
batch: 2017
train_loss: 52.50800347328186
batch: 2018
train_loss: 55.40564942359924
batch: 2019
train_loss: 58.36568522453308
batch: 2020
train_loss: 61.28480362892151
batch: 2021
train_loss: 64.19598436355591
batch: 2022
train_loss: 67.05639457702637
batch: 2023
train_loss: 69.94395732879639
batch: 2024
train_loss: 72.89418339729309
batch: 2025
train_loss: 75.78037452697754
batch: 2026
train_loss: 78.71272253990173
batch: 2027
train_loss: 81.65656089782715
batch: 2028
train_loss: 84.5979368686676
batch: 2029
train_loss: 87.49822807312012
batch: 2030
train_loss: 90.33340191841125
batch: 2031
train_loss: 93.32777762413025
batch: 2032
train_loss: 96.19952726364136
batch: 2033
train_loss: 99.10858678817749
batch: 2034
train_loss: 102.02395153045654
batch: 2035
train_loss: 105.00609135627747
batch: 2036
train_loss: 107.85428524017334
batch: 2037
train_loss: 110.8145980834961
batch: 2038
train_loss: 113.73738193511963
batch: 2039
train_loss: 116.60359287261963
batch: 2040
train_loss: 119.43787550926208
batch: 2041
train_loss: 122.24410963058472
batch: 2042
train_loss: 125.07095098495483
batch: 2043
train_loss: 127.82117867469788
batch: 2044
train_loss: 130.6274311542511
batch: 2045
train_loss: 133.47941899299622
batch: 2046
train_loss: 136.35726499557495
batch: 2047
train_loss: 139.19120335578918
batch: 2048
train_loss: 141.96821689605713
batch: 2049
train_loss: 144.73758816719055
batch: 2050
train_loss: 147.52164483070374
batch: 2051
train_loss: 150.29843091964722
batch: 2052
train_loss: 153.1091446876526
batch: 2053
train_loss: 155.9368236064911
batch: 2054
train_loss: 158.80362033843994
batch: 2055
train_loss: 161.66325569152832
batch: 2056
train_loss: 164.48044061660767
batch: 2057
train_loss: 167.2459065914154
batch: 2058
train_loss: 170.1523938179016
batch: 2059
train_loss: 173.04670763015747
batch: 2060
train_loss: 175.82486295700073
batch: 2061
train_loss: 178.5962519645691
batch: 2062
train_loss: 181.30604338645935
batch: 2063
train_loss: 184.06157040596008
batch: 2064
train_loss: 186.83335638046265
batch: 2065
train_loss: 189.622065782547
batch: 2066
train_loss: 192.4099259376526
batch: 2067
train_loss: 195.2204840183258
batch: 2068
train_loss: 198.0468988418579
batch: 2069
train_loss: 200.89124631881714
batch: 2070
train_loss: 203.70484447479248
batch: 2071
train_loss: 206.56697702407837
batch: 2072
train_loss: 209.37814593315125
batch: 2073
train_loss: 212.2488408088684
batch: 2074
train_loss: 215.21238350868225
batch: 2075
train_loss: 218.14703750610352
batch: 2076
train_loss: 221.14311528205872
batch: 2077
train_loss: 224.01687908172607
batch: 2078
train_loss: 226.89995574951172
batch: 2079
train_loss: 229.79093861579895
batch: 2080
train_loss: 232.67800092697144
batch: 2081
train_loss: 235.55371403694153
batch: 2082
train_loss: 238.42285680770874
batch: 2083
train_loss: 241.29726219177246
batch: 2084
train_loss: 244.14368557929993
batch: 2085
train_loss: 246.98458003997803
batch: 2086
train_loss: 249.81626868247986
batch: 2087
train_loss: 252.74486231803894
batch: 2088
train_loss: 255.58314275741577
batch: 2089
train_loss: 258.4797291755676
batch: 2090
train_loss: 261.3375015258789
batch: 2091
train_loss: 264.2595856189728
batch: 2092
train_loss: 267.13366985321045
batch: 2093
train_loss: 270.09942150115967
batch: 2094
train_loss: 273.1192259788513
batch: 2095
train_loss: 276.0208113193512
batch: 2096
train_loss: 278.91708111763
batch: 2097
train_loss: 281.8267934322357
batch: 2098
train_loss: 284.7541365623474
batch: 2099
train_loss: 287.65342569351196
batch: 2100
train_loss: 290.61320519447327
batch: 2101
train_loss: 293.4720826148987
batch: 2102
train_loss: 296.3884246349335
batch: 2103
train_loss: 299.3010549545288
batch: 2104
train_loss: 302.1867849826813
batch: 2105
train_loss: 304.98257970809937
batch: 2106
train_loss: 307.76474022865295
batch: 2107
train_loss: 310.55008602142334
batch: 2108
train_loss: 313.3844985961914
batch: 2109
train_loss: 316.13889360427856
batch: 2110
train_loss: 318.9943301677704
batch: 2111
train_loss: 321.8555977344513
batch: 2112
train_loss: 324.6772520542145
batch: 2113
train_loss: 327.5261287689209
batch: 2114
train_loss: 330.3047342300415
batch: 2115
train_loss: 333.24885749816895
batch: 2116
train_loss: 336.1497766971588
batch: 2117
train_loss: 339.0835053920746
batch: 2118
train_loss: 341.9522500038147
batch: 2119
train_loss: 344.8141031265259
batch: 2120
train_loss: 347.66374826431274
batch: 2121
train_loss: 350.4758903980255
batch: 2122
train_loss: 353.28695583343506
batch: 2123
train_loss: 356.19039940834045
batch: 2124
train_loss: 358.98907446861267
batch: 2125
train_loss: 361.760865688324
batch: 2126
train_loss: 364.50998187065125
batch: 2127
train_loss: 367.3284306526184
batch: 2128
train_loss: 370.14169335365295
batch: 2129
train_loss: 372.85361790657043
batch: 2130
train_loss: 375.5872402191162
batch: 2131
train_loss: 378.37687277793884
batch: 2132
train_loss: 381.2049894332886
batch: 2133
train_loss: 383.965211391449
batch: 2134
train_loss: 386.7218852043152
batch: 2135
train_loss: 389.44667530059814
batch: 2136
train_loss: 392.1750886440277
batch: 2137
train_loss: 394.9429051876068
batch: 2138
train_loss: 397.6823363304138
batch: 2139
train_loss: 400.4591324329376
batch: 2140
train_loss: 403.2464015483856
batch: 2141
train_loss: 405.9953899383545
batch: 2142
train_loss: 408.7747461795807
batch: 2143
train_loss: 411.59999799728394
batch: 2144
train_loss: 414.4375784397125
batch: 2145
train_loss: 417.1980850696564
batch: 2146
train_loss: 420.02581548690796
batch: 2147
train_loss: 422.87747287750244
batch: 2148
train_loss: 425.81605553627014
batch: 2149
train_loss: 428.62788915634155
batch: 2150
train_loss: 431.4381320476532
batch: 2151
train_loss: 434.32327032089233
batch: 2152
train_loss: 437.11355900764465
batch: 2153
train_loss: 439.8811330795288
batch: 2154
train_loss: 442.7015767097473
batch: 2155
train_loss: 445.6158215999603
batch: 2156
train_loss: 448.44824504852295
batch: 2157
train_loss: 451.2350513935089
batch: 2158
train_loss: 453.97202587127686
batch: 2159
train_loss: 456.8675146102905
batch: 2160
train_loss: 459.8094313144684
batch: 2161
train_loss: 462.7055695056915
batch: 2162
train_loss: 465.5266127586365
batch: 2163
train_loss: 468.36394810676575
batch: 2164
train_loss: 471.20283818244934
batch: 2165
train_loss: 473.9793555736542
batch: 2166
train_loss: 476.7796812057495
batch: 2167
train_loss: 479.53526759147644
batch: 2168
train_loss: 482.3422634601593
batch: 2169
train_loss: 485.1817526817322
batch: 2170
train_loss: 488.02711391448975
batch: 2171
train_loss: 490.8100028038025
batch: 2172
train_loss: 493.68163108825684
batch: 2173
train_loss: 496.51483058929443
batch: 2174
train_loss: 499.30218625068665
batch: 2175
train_loss: 502.24405813217163
batch: 2176
train_loss: 505.06638646125793
batch: 2177
train_loss: 508.00701451301575
batch: 2178
train_loss: 510.825181722641
batch: 2179
train_loss: 513.7134525775909
batch: 2180
train_loss: 516.611617565155
batch: 2181
train_loss: 519.4915001392365
batch: 2182
train_loss: 522.3562271595001
batch: 2183
train_loss: 525.2202458381653
batch: 2184
train_loss: 527.9913339614868
batch: 2185
train_loss: 530.8873097896576
batch: 2186
train_loss: 533.80220246315
batch: 2187
train_loss: 536.6762969493866
batch: 2188
train_loss: 539.5601689815521
batch: 2189
train_loss: 542.4575488567352
batch: 2190
train_loss: 545.3454236984253
batch: 2191
train_loss: 548.2589244842529
batch: 2192
train_loss: 551.1790401935577
batch: 2193
train_loss: 554.0177099704742
batch: 2194
train_loss: 556.9613125324249
batch: 2195
train_loss: 559.754674911499
batch: 2196
train_loss: 562.6271512508392
batch: 2197
train_loss: 565.5047895908356
batch: 2198
train_loss: 568.4052243232727
batch: 2199
train_loss: 571.306556224823
| epoch   1 step     2200 |   2200 batches | lr 0.000248 | ms/batch 1169.26 | loss  2.86 | bpc   4.12111
batch: 2200
train_loss: 2.8846497535705566
batch: 2201
train_loss: 5.676944017410278
batch: 2202
train_loss: 8.617243766784668
batch: 2203
train_loss: 11.488608121871948
batch: 2204
train_loss: 14.409407138824463
batch: 2205
train_loss: 17.305509090423584
batch: 2206
train_loss: 20.21349263191223
batch: 2207
train_loss: 23.160441875457764
batch: 2208
train_loss: 26.14203190803528
batch: 2209
train_loss: 29.113945960998535
batch: 2210
train_loss: 32.06197166442871
batch: 2211
train_loss: 35.018672466278076
batch: 2212
train_loss: 38.01278042793274
batch: 2213
train_loss: 40.9105658531189
batch: 2214
train_loss: 43.824525117874146
batch: 2215
train_loss: 46.719942808151245
batch: 2216
train_loss: 49.6811785697937
batch: 2217
train_loss: 52.63430571556091
batch: 2218
train_loss: 55.51404881477356
batch: 2219
train_loss: 58.43153381347656
batch: 2220
train_loss: 61.3655481338501
batch: 2221
train_loss: 64.35065460205078
batch: 2222
train_loss: 67.248286485672
batch: 2223
train_loss: 70.08353519439697
batch: 2224
train_loss: 72.84631276130676
batch: 2225
train_loss: 75.72135639190674
batch: 2226
train_loss: 78.54822301864624
batch: 2227
train_loss: 81.33373665809631
batch: 2228
train_loss: 84.18669986724854
batch: 2229
train_loss: 87.00465202331543
batch: 2230
train_loss: 89.84278583526611
batch: 2231
train_loss: 92.73390364646912
batch: 2232
train_loss: 95.55710363388062
batch: 2233
train_loss: 98.47764039039612
batch: 2234
train_loss: 101.32156300544739
batch: 2235
train_loss: 104.1007432937622
batch: 2236
train_loss: 106.8975830078125
batch: 2237
train_loss: 109.74233865737915
batch: 2238
train_loss: 112.62567377090454
batch: 2239
train_loss: 115.53027868270874
batch: 2240
train_loss: 118.41161847114563
batch: 2241
train_loss: 121.37157034873962
batch: 2242
train_loss: 124.33393168449402
batch: 2243
train_loss: 127.30698013305664
batch: 2244
train_loss: 130.23094081878662
batch: 2245
train_loss: 133.23594212532043
batch: 2246
train_loss: 136.1061372756958
batch: 2247
train_loss: 138.94095253944397
batch: 2248
train_loss: 141.84092473983765
batch: 2249
train_loss: 144.74149250984192
batch: 2250
train_loss: 147.66430521011353
batch: 2251
train_loss: 150.4783434867859
batch: 2252
train_loss: 153.36551547050476
batch: 2253
train_loss: 156.29911923408508
batch: 2254
train_loss: 159.13468623161316
batch: 2255
train_loss: 162.0139617919922
batch: 2256
train_loss: 164.85237336158752
batch: 2257
train_loss: 167.70131921768188
batch: 2258
train_loss: 170.51061463356018
batch: 2259
train_loss: 173.34359288215637
batch: 2260
train_loss: 176.16152954101562
batch: 2261
train_loss: 179.02649927139282
batch: 2262
train_loss: 181.82815623283386
batch: 2263
train_loss: 184.69613528251648
batch: 2264
train_loss: 187.48009848594666
batch: 2265
train_loss: 190.34122920036316
batch: 2266
train_loss: 193.18164134025574
batch: 2267
train_loss: 196.084463596344
batch: 2268
train_loss: 198.96301531791687
batch: 2269
train_loss: 201.83094358444214
batch: 2270
train_loss: 204.59696626663208
batch: 2271
train_loss: 207.47330331802368
batch: 2272
train_loss: 210.39104390144348
batch: 2273
train_loss: 213.25327014923096
batch: 2274
train_loss: 216.13946652412415
batch: 2275
train_loss: 219.05040645599365
batch: 2276
train_loss: 221.90264320373535
batch: 2277
train_loss: 224.75608658790588
batch: 2278
train_loss: 227.6665496826172
batch: 2279
train_loss: 230.54432225227356
batch: 2280
train_loss: 233.4483151435852
batch: 2281
train_loss: 236.22381830215454
batch: 2282
train_loss: 239.11386585235596
batch: 2283
train_loss: 242.00463676452637
batch: 2284
train_loss: 244.79987406730652
batch: 2285
train_loss: 247.5816957950592
batch: 2286
train_loss: 250.45475316047668
batch: 2287
train_loss: 253.3211030960083
batch: 2288
train_loss: 256.1477406024933
batch: 2289
train_loss: 259.0034453868866
batch: 2290
train_loss: 261.88431453704834
batch: 2291
train_loss: 264.6597638130188
batch: 2292
train_loss: 267.41893124580383
batch: 2293
train_loss: 270.1804165840149
batch: 2294
train_loss: 272.93643450737
batch: 2295
train_loss: 275.68396401405334
batch: 2296
train_loss: 278.3859429359436
batch: 2297
train_loss: 281.18270564079285
batch: 2298
train_loss: 283.94564986228943
batch: 2299
train_loss: 286.7034845352173
batch: 2300
train_loss: 289.56933760643005
batch: 2301
train_loss: 292.41256523132324
batch: 2302
train_loss: 295.24509716033936
batch: 2303
train_loss: 298.06332540512085
batch: 2304
train_loss: 301.02571535110474
batch: 2305
train_loss: 303.9189794063568
batch: 2306
train_loss: 306.7465806007385
batch: 2307
train_loss: 309.5574760437012
batch: 2308
train_loss: 312.37973952293396
batch: 2309
train_loss: 315.2299690246582
batch: 2310
train_loss: 318.0283913612366
batch: 2311
train_loss: 320.80973863601685
batch: 2312
train_loss: 323.6460871696472
batch: 2313
train_loss: 326.4759724140167
batch: 2314
train_loss: 329.38756465911865
batch: 2315
train_loss: 332.27956986427307
batch: 2316
train_loss: 335.1188027858734
batch: 2317
train_loss: 338.0143463611603
batch: 2318
train_loss: 340.94381952285767
batch: 2319
train_loss: 343.7538969516754
batch: 2320
train_loss: 346.59318113327026
batch: 2321
train_loss: 349.4320764541626
batch: 2322
train_loss: 352.2472975254059
batch: 2323
train_loss: 355.1039922237396
batch: 2324
train_loss: 357.9675831794739
batch: 2325
train_loss: 360.7736768722534
batch: 2326
train_loss: 363.6525993347168
batch: 2327
train_loss: 366.50783824920654
batch: 2328
train_loss: 369.2882869243622
batch: 2329
train_loss: 372.11459970474243
batch: 2330
train_loss: 374.9579393863678
batch: 2331
train_loss: 377.75393867492676
batch: 2332
train_loss: 380.6056156158447
batch: 2333
train_loss: 383.4365746974945
batch: 2334
train_loss: 386.24433183670044
batch: 2335
train_loss: 389.04865884780884
batch: 2336
train_loss: 391.79430627822876
batch: 2337
train_loss: 394.58156847953796
batch: 2338
train_loss: 397.36039447784424
batch: 2339
train_loss: 400.21404004096985
batch: 2340
train_loss: 403.00398659706116
batch: 2341
train_loss: 405.8496563434601
batch: 2342
train_loss: 408.6045153141022
batch: 2343
train_loss: 411.4562957286835
batch: 2344
train_loss: 414.24800515174866
batch: 2345
train_loss: 417.03130197525024
batch: 2346
train_loss: 419.80291271209717
batch: 2347
train_loss: 422.5400664806366
batch: 2348
train_loss: 425.3128385543823
batch: 2349
train_loss: 428.162966966629
batch: 2350
train_loss: 430.9535276889801
batch: 2351
train_loss: 433.7376687526703
batch: 2352
train_loss: 436.4450297355652
batch: 2353
train_loss: 439.17673110961914
batch: 2354
train_loss: 441.9047918319702
batch: 2355
train_loss: 444.7474720478058
batch: 2356
train_loss: 447.5395562648773
batch: 2357
train_loss: 450.3294725418091
batch: 2358
train_loss: 453.15613198280334
batch: 2359
train_loss: 455.8876883983612
batch: 2360
train_loss: 458.6946165561676
batch: 2361
train_loss: 461.54596638679504
batch: 2362
train_loss: 464.3303711414337
batch: 2363
train_loss: 467.15602469444275
batch: 2364
train_loss: 469.97263956069946
batch: 2365
train_loss: 472.76585268974304
batch: 2366
train_loss: 475.55335307121277
batch: 2367
train_loss: 478.2939534187317
batch: 2368
train_loss: 481.0230829715729
batch: 2369
train_loss: 483.8374707698822
batch: 2370
train_loss: 486.584685087204
batch: 2371
train_loss: 489.361599445343
batch: 2372
train_loss: 492.0740637779236
batch: 2373
train_loss: 494.8608663082123
batch: 2374
train_loss: 497.7295913696289
batch: 2375
train_loss: 500.4975860118866
batch: 2376
train_loss: 503.21853733062744
batch: 2377
train_loss: 506.0190989971161
batch: 2378
train_loss: 508.80062770843506
batch: 2379
train_loss: 511.54521560668945
batch: 2380
train_loss: 514.2923049926758
batch: 2381
train_loss: 517.0667297840118
batch: 2382
train_loss: 519.8574705123901
batch: 2383
train_loss: 522.5902523994446
batch: 2384
train_loss: 525.3980598449707
batch: 2385
train_loss: 528.209465265274
batch: 2386
train_loss: 531.0223662853241
batch: 2387
train_loss: 533.8498501777649
batch: 2388
train_loss: 536.6816325187683
batch: 2389
train_loss: 539.4520580768585
batch: 2390
train_loss: 542.2093043327332
batch: 2391
train_loss: 545.1043846607208
batch: 2392
train_loss: 547.915679693222
batch: 2393
train_loss: 550.7235007286072
batch: 2394
train_loss: 553.6238481998444
batch: 2395
train_loss: 556.3892350196838
batch: 2396
train_loss: 559.2246387004852
batch: 2397
train_loss: 562.0231876373291
batch: 2398
train_loss: 564.7427077293396
batch: 2399
train_loss: 567.4973664283752
| epoch   1 step     2400 |   2400 batches | lr 0.000248 | ms/batch 1170.24 | loss  2.84 | bpc   4.09363
batch: 2400
train_loss: 2.750530958175659
batch: 2401
train_loss: 5.565180540084839
batch: 2402
train_loss: 8.264378070831299
batch: 2403
train_loss: 11.14270806312561
batch: 2404
train_loss: 13.914551258087158
batch: 2405
train_loss: 16.713230848312378
batch: 2406
train_loss: 19.50722312927246
batch: 2407
train_loss: 22.247986316680908
batch: 2408
train_loss: 24.997317790985107
batch: 2409
train_loss: 27.69808340072632
batch: 2410
train_loss: 30.416991472244263
batch: 2411
train_loss: 33.173617362976074
batch: 2412
train_loss: 35.90257430076599
batch: 2413
train_loss: 38.70373868942261
batch: 2414
train_loss: 41.43469500541687
batch: 2415
train_loss: 44.196646213531494
batch: 2416
train_loss: 47.03306841850281
batch: 2417
train_loss: 49.864396810531616
batch: 2418
train_loss: 52.59208822250366
batch: 2419
train_loss: 55.34809613227844
batch: 2420
train_loss: 58.07881569862366
batch: 2421
train_loss: 60.85081911087036
batch: 2422
train_loss: 63.62484812736511
batch: 2423
train_loss: 66.42174744606018
batch: 2424
train_loss: 69.20213317871094
batch: 2425
train_loss: 71.99116921424866
batch: 2426
train_loss: 74.8333306312561
batch: 2427
train_loss: 77.65658020973206
batch: 2428
train_loss: 80.500727891922
batch: 2429
train_loss: 83.34565901756287
batch: 2430
train_loss: 86.14303135871887
batch: 2431
train_loss: 88.97364711761475
batch: 2432
train_loss: 91.79037928581238
batch: 2433
train_loss: 94.64731359481812
batch: 2434
train_loss: 97.54008197784424
batch: 2435
train_loss: 100.50188374519348
batch: 2436
train_loss: 103.41517877578735
batch: 2437
train_loss: 106.31079411506653
batch: 2438
train_loss: 109.09721326828003
batch: 2439
train_loss: 112.02475619316101
batch: 2440
train_loss: 115.00153756141663
batch: 2441
train_loss: 117.89781308174133
batch: 2442
train_loss: 120.739492893219
batch: 2443
train_loss: 123.61306691169739
batch: 2444
train_loss: 126.43975162506104
batch: 2445
train_loss: 129.29037642478943
batch: 2446
train_loss: 132.14781379699707
batch: 2447
train_loss: 134.9249496459961
batch: 2448
train_loss: 137.79520177841187
batch: 2449
train_loss: 140.59990048408508
batch: 2450
train_loss: 143.42688035964966
batch: 2451
train_loss: 146.24310946464539
batch: 2452
train_loss: 149.00461745262146
batch: 2453
train_loss: 151.78092694282532
batch: 2454
train_loss: 154.6081190109253
batch: 2455
train_loss: 157.42382264137268
batch: 2456
train_loss: 160.2198987007141
batch: 2457
train_loss: 163.0288770198822
batch: 2458
train_loss: 165.80641889572144
batch: 2459
train_loss: 168.65643405914307
batch: 2460
train_loss: 171.41959595680237
batch: 2461
train_loss: 174.30451488494873
batch: 2462
train_loss: 177.10132145881653
batch: 2463
train_loss: 179.94785833358765
batch: 2464
train_loss: 182.67304348945618
batch: 2465
train_loss: 185.42723155021667
batch: 2466
train_loss: 188.0822491645813
batch: 2467
train_loss: 190.87922620773315
batch: 2468
train_loss: 193.61923146247864
batch: 2469
train_loss: 196.29229640960693
batch: 2470
train_loss: 199.10256958007812
batch: 2471
train_loss: 201.93122243881226
batch: 2472
train_loss: 204.73587489128113
batch: 2473
train_loss: 207.53404331207275
batch: 2474
train_loss: 210.38340640068054
batch: 2475
train_loss: 213.17445731163025
batch: 2476
train_loss: 216.0077142715454
batch: 2477
train_loss: 218.8310580253601
batch: 2478
train_loss: 221.63898706436157
batch: 2479
train_loss: 224.4605803489685
batch: 2480
train_loss: 227.2181634902954
batch: 2481
train_loss: 230.00290989875793
batch: 2482
train_loss: 232.87941145896912
batch: 2483
train_loss: 235.72991180419922
batch: 2484
train_loss: 238.5929310321808
batch: 2485
train_loss: 241.4552915096283
batch: 2486
train_loss: 244.17929410934448
batch: 2487
train_loss: 246.95773816108704
batch: 2488
train_loss: 249.77017831802368
batch: 2489
train_loss: 252.55872082710266
batch: 2490
train_loss: 255.35250568389893
batch: 2491
train_loss: 258.12019419670105
batch: 2492
train_loss: 260.911856174469
batch: 2493
train_loss: 263.69071769714355
batch: 2494
train_loss: 266.4051687717438
batch: 2495
train_loss: 269.13474011421204
batch: 2496
train_loss: 271.9178054332733
batch: 2497
train_loss: 274.6421995162964
batch: 2498
train_loss: 277.3857502937317
batch: 2499
train_loss: 280.06241059303284
batch: 2500
train_loss: 282.7413663864136
batch: 2501
train_loss: 285.4558107852936
batch: 2502
train_loss: 288.24734377861023
batch: 2503
train_loss: 290.9874634742737
batch: 2504
train_loss: 293.6541874408722
batch: 2505
train_loss: 296.3126516342163
batch: 2506
train_loss: 299.0429675579071
batch: 2507
train_loss: 301.7659306526184
batch: 2508
train_loss: 304.49021649360657
batch: 2509
train_loss: 307.20505809783936
batch: 2510
train_loss: 309.94674944877625
batch: 2511
train_loss: 312.770920753479
batch: 2512
train_loss: 315.48821330070496
batch: 2513
train_loss: 318.2542243003845
batch: 2514
train_loss: 320.9612686634064
batch: 2515
train_loss: 323.76694202423096
batch: 2516
train_loss: 326.4468674659729
batch: 2517
train_loss: 329.1885874271393
batch: 2518
train_loss: 331.9420521259308
batch: 2519
train_loss: 334.7130403518677
batch: 2520
train_loss: 337.4409968852997
batch: 2521
train_loss: 340.1665127277374
batch: 2522
train_loss: 342.8772990703583
batch: 2523
train_loss: 345.64423966407776
batch: 2524
train_loss: 348.353830575943
batch: 2525
train_loss: 351.105833530426
batch: 2526
train_loss: 353.8781554698944
batch: 2527
train_loss: 356.69034695625305
batch: 2528
train_loss: 359.4360861778259
batch: 2529
train_loss: 362.22076392173767
batch: 2530
train_loss: 365.0703444480896
batch: 2531
train_loss: 367.87380194664
batch: 2532
train_loss: 370.60528445243835
batch: 2533
train_loss: 373.48166275024414
batch: 2534
train_loss: 376.2308006286621
batch: 2535
train_loss: 378.95648407936096
batch: 2536
train_loss: 381.72222208976746
batch: 2537
train_loss: 384.4508099555969
batch: 2538
train_loss: 387.179664850235
batch: 2539
train_loss: 389.9422426223755
batch: 2540
train_loss: 392.67274618148804
batch: 2541
train_loss: 395.4771337509155
batch: 2542
train_loss: 398.3380546569824
batch: 2543
train_loss: 401.2477378845215
batch: 2544
train_loss: 404.01741766929626
batch: 2545
train_loss: 406.80340003967285
batch: 2546
train_loss: 409.56752252578735
batch: 2547
train_loss: 412.32271242141724
batch: 2548
train_loss: 415.1310279369354
batch: 2549
train_loss: 418.0369212627411
batch: 2550
train_loss: 420.85127234458923
batch: 2551
train_loss: 423.68567633628845
batch: 2552
train_loss: 426.5449411869049
batch: 2553
train_loss: 429.498263835907
batch: 2554
train_loss: 432.3962171077728
batch: 2555
train_loss: 435.2856948375702
batch: 2556
train_loss: 438.16644835472107
batch: 2557
train_loss: 441.1023254394531
batch: 2558
train_loss: 443.9666633605957
batch: 2559
train_loss: 446.7763931751251
batch: 2560
train_loss: 449.6073958873749
batch: 2561
train_loss: 452.49005484580994
batch: 2562
train_loss: 455.3659987449646
batch: 2563
train_loss: 458.1245427131653
batch: 2564
train_loss: 460.9379050731659
batch: 2565
train_loss: 463.8268983364105
batch: 2566
train_loss: 466.6391990184784
batch: 2567
train_loss: 469.3654637336731
batch: 2568
train_loss: 472.22332525253296
batch: 2569
train_loss: 474.9757344722748
batch: 2570
train_loss: 477.827743768692
batch: 2571
train_loss: 480.70193552970886
batch: 2572
train_loss: 483.47232484817505
batch: 2573
train_loss: 486.4053671360016
batch: 2574
train_loss: 489.2081604003906
batch: 2575
train_loss: 491.974809885025
batch: 2576
train_loss: 494.8263440132141
batch: 2577
train_loss: 497.6850769519806
batch: 2578
train_loss: 500.5080711841583
batch: 2579
train_loss: 503.3690598011017
batch: 2580
train_loss: 506.19181394577026
batch: 2581
train_loss: 508.9177167415619
batch: 2582
train_loss: 511.84926438331604
batch: 2583
train_loss: 514.6307382583618
batch: 2584
train_loss: 517.4141330718994
batch: 2585
train_loss: 520.2363505363464
batch: 2586
train_loss: 522.96355676651
batch: 2587
train_loss: 525.7379324436188
batch: 2588
train_loss: 528.4974811077118
batch: 2589
train_loss: 531.3152599334717
batch: 2590
train_loss: 534.0921907424927
batch: 2591
train_loss: 536.8251624107361
batch: 2592
train_loss: 539.5529053211212
batch: 2593
train_loss: 542.3204655647278
batch: 2594
train_loss: 545.046468257904
batch: 2595
train_loss: 547.7545182704926
batch: 2596
train_loss: 550.4049088954926
batch: 2597
train_loss: 553.1001124382019
batch: 2598
train_loss: 555.8663647174835
batch: 2599
train_loss: 558.6142587661743
| epoch   1 step     2600 |   2600 batches | lr 0.000247 | ms/batch 1172.14 | loss  2.79 | bpc   4.02955
batch: 2600
train_loss: 2.754108190536499
batch: 2601
train_loss: 5.445913076400757
batch: 2602
train_loss: 8.18547797203064
batch: 2603
train_loss: 10.97449254989624
batch: 2604
train_loss: 13.70174765586853
batch: 2605
train_loss: 16.40461230278015
batch: 2606
train_loss: 19.230448961257935
batch: 2607
train_loss: 22.051806688308716
batch: 2608
train_loss: 24.78191828727722
batch: 2609
train_loss: 27.513964891433716
batch: 2610
train_loss: 30.215307235717773
batch: 2611
train_loss: 32.97017168998718
batch: 2612
train_loss: 35.80729579925537
batch: 2613
train_loss: 38.54706430435181
batch: 2614
train_loss: 41.359448194503784
batch: 2615
train_loss: 44.042235374450684
batch: 2616
train_loss: 46.918621301651
batch: 2617
train_loss: 49.68966579437256
batch: 2618
train_loss: 52.4946346282959
batch: 2619
train_loss: 55.230167865753174
batch: 2620
train_loss: 57.96296834945679
batch: 2621
train_loss: 60.69454622268677
batch: 2622
train_loss: 63.428828954696655
batch: 2623
train_loss: 66.1554183959961
batch: 2624
train_loss: 68.9228904247284
batch: 2625
train_loss: 71.74207019805908
batch: 2626
train_loss: 74.49991798400879
batch: 2627
train_loss: 77.23216700553894
batch: 2628
train_loss: 80.01210618019104
batch: 2629
train_loss: 82.73582339286804
batch: 2630
train_loss: 85.54443311691284
batch: 2631
train_loss: 88.34663033485413
batch: 2632
train_loss: 91.13939499855042
batch: 2633
train_loss: 93.83057403564453
batch: 2634
train_loss: 96.57524967193604
batch: 2635
train_loss: 99.34667205810547
batch: 2636
train_loss: 102.09792017936707
batch: 2637
train_loss: 104.89141893386841
batch: 2638
train_loss: 107.68075370788574
batch: 2639
train_loss: 110.42290163040161
batch: 2640
train_loss: 113.1836268901825
batch: 2641
train_loss: 115.95467233657837
batch: 2642
train_loss: 118.78483080863953
batch: 2643
train_loss: 121.5413830280304
batch: 2644
train_loss: 124.26045656204224
batch: 2645
train_loss: 126.98959517478943
batch: 2646
train_loss: 129.71248054504395
batch: 2647
train_loss: 132.52916169166565
batch: 2648
train_loss: 135.34207606315613
batch: 2649
train_loss: 138.1519227027893
batch: 2650
train_loss: 141.0229616165161
batch: 2651
train_loss: 143.81873178482056
batch: 2652
train_loss: 146.55103206634521
batch: 2653
train_loss: 149.35674142837524
batch: 2654
train_loss: 152.1605007648468
batch: 2655
train_loss: 155.0174744129181
batch: 2656
train_loss: 157.85166835784912
batch: 2657
train_loss: 160.69589257240295
batch: 2658
train_loss: 163.5661940574646
batch: 2659
train_loss: 166.51647782325745
batch: 2660
train_loss: 169.38369297981262
batch: 2661
train_loss: 172.28280782699585
batch: 2662
train_loss: 175.10510301589966
batch: 2663
train_loss: 177.91166639328003
batch: 2664
train_loss: 180.7561764717102
batch: 2665
train_loss: 183.52886414527893
batch: 2666
train_loss: 186.36753630638123
batch: 2667
train_loss: 189.1338427066803
batch: 2668
train_loss: 191.89671421051025
batch: 2669
train_loss: 194.62663054466248
batch: 2670
train_loss: 197.32036209106445
batch: 2671
train_loss: 200.1096429824829
batch: 2672
train_loss: 202.84848165512085
batch: 2673
train_loss: 205.55739212036133
batch: 2674
train_loss: 208.28936100006104
batch: 2675
train_loss: 210.97441220283508
batch: 2676
train_loss: 213.79283738136292
batch: 2677
train_loss: 216.51736187934875
batch: 2678
train_loss: 219.32769536972046
batch: 2679
train_loss: 222.17699670791626
batch: 2680
train_loss: 224.90754580497742
batch: 2681
train_loss: 227.60277581214905
batch: 2682
train_loss: 230.38996744155884
batch: 2683
train_loss: 233.1063849925995
batch: 2684
train_loss: 235.82378816604614
batch: 2685
train_loss: 238.65992784500122
batch: 2686
train_loss: 241.5057122707367
batch: 2687
train_loss: 244.292138338089
batch: 2688
train_loss: 247.09923839569092
batch: 2689
train_loss: 249.818745136261
batch: 2690
train_loss: 252.5552318096161
batch: 2691
train_loss: 255.3094289302826
batch: 2692
train_loss: 258.10051798820496
batch: 2693
train_loss: 260.86535263061523
batch: 2694
train_loss: 263.6265640258789
batch: 2695
train_loss: 266.3095471858978
batch: 2696
train_loss: 268.94309639930725
batch: 2697
train_loss: 271.68237495422363
batch: 2698
train_loss: 274.3750660419464
batch: 2699
train_loss: 277.0165264606476
batch: 2700
train_loss: 279.6821734905243
batch: 2701
train_loss: 282.415447473526
batch: 2702
train_loss: 285.2025055885315
batch: 2703
train_loss: 287.8286943435669
batch: 2704
train_loss: 290.5547950267792
batch: 2705
train_loss: 293.275541305542
batch: 2706
train_loss: 296.0055944919586
batch: 2707
train_loss: 298.893435716629
batch: 2708
train_loss: 301.5927560329437
batch: 2709
train_loss: 304.23652839660645
batch: 2710
train_loss: 306.92053413391113
batch: 2711
train_loss: 309.6265275478363
batch: 2712
train_loss: 312.30382895469666
batch: 2713
train_loss: 314.97432112693787
batch: 2714
train_loss: 317.6947658061981
batch: 2715
train_loss: 320.3997313976288
batch: 2716
train_loss: 323.09139037132263
batch: 2717
train_loss: 325.8483257293701
batch: 2718
train_loss: 328.55160117149353
batch: 2719
train_loss: 331.2850947380066
batch: 2720
train_loss: 334.0114450454712
batch: 2721
train_loss: 336.7126877307892
batch: 2722
train_loss: 339.4820990562439
batch: 2723
train_loss: 342.2348098754883
batch: 2724
train_loss: 345.0200386047363
batch: 2725
train_loss: 347.85845613479614
batch: 2726
train_loss: 350.6212348937988
batch: 2727
train_loss: 353.3913652896881
batch: 2728
train_loss: 356.06635761260986
batch: 2729
train_loss: 358.7934913635254
batch: 2730
train_loss: 361.5052435398102
batch: 2731
train_loss: 364.2937309741974
batch: 2732
train_loss: 367.01261734962463
batch: 2733
train_loss: 369.7966401576996
batch: 2734
train_loss: 372.51678490638733
batch: 2735
train_loss: 375.2669024467468
batch: 2736
train_loss: 378.06227803230286
batch: 2737
train_loss: 380.8254568576813
batch: 2738
train_loss: 383.53805708885193
batch: 2739
train_loss: 386.2649714946747
batch: 2740
train_loss: 388.9680178165436
batch: 2741
train_loss: 391.6753075122833
batch: 2742
train_loss: 394.34834003448486
batch: 2743
train_loss: 397.13521003723145
batch: 2744
train_loss: 399.8521728515625
batch: 2745
train_loss: 402.5161101818085
batch: 2746
train_loss: 405.2067403793335
batch: 2747
train_loss: 408.0158052444458
batch: 2748
train_loss: 410.7076165676117
batch: 2749
train_loss: 413.4196536540985
batch: 2750
train_loss: 416.16010665893555
batch: 2751
train_loss: 418.86278319358826
batch: 2752
train_loss: 421.6087565422058
batch: 2753
train_loss: 424.38478112220764
batch: 2754
train_loss: 427.1115894317627
batch: 2755
train_loss: 429.8258249759674
batch: 2756
train_loss: 432.5687851905823
batch: 2757
train_loss: 435.2781112194061
batch: 2758
train_loss: 438.0414845943451
batch: 2759
train_loss: 440.8525984287262
batch: 2760
train_loss: 443.51790595054626
batch: 2761
train_loss: 446.22371459007263
batch: 2762
train_loss: 448.9779143333435
batch: 2763
train_loss: 451.7139616012573
batch: 2764
train_loss: 454.46535301208496
batch: 2765
train_loss: 457.2343065738678
batch: 2766
train_loss: 460.00464129447937
batch: 2767
train_loss: 462.8097937107086
batch: 2768
train_loss: 465.5256905555725
batch: 2769
train_loss: 468.2303047180176
batch: 2770
train_loss: 470.9989376068115
batch: 2771
train_loss: 473.6426832675934
batch: 2772
train_loss: 476.32322573661804
batch: 2773
train_loss: 478.97202467918396
batch: 2774
train_loss: 481.74167013168335
batch: 2775
train_loss: 484.4154019355774
batch: 2776
train_loss: 487.02897024154663
batch: 2777
train_loss: 489.8376622200012
batch: 2778
train_loss: 492.53958535194397
batch: 2779
train_loss: 495.2324252128601
batch: 2780
train_loss: 497.95633363723755
batch: 2781
train_loss: 500.6678807735443
batch: 2782
train_loss: 503.4468574523926
batch: 2783
train_loss: 506.2471752166748
batch: 2784
train_loss: 508.9862148761749
batch: 2785
train_loss: 511.6595802307129
batch: 2786
train_loss: 514.3881578445435
batch: 2787
train_loss: 517.1135683059692
batch: 2788
train_loss: 519.8346531391144
batch: 2789
train_loss: 522.6528913974762
batch: 2790
train_loss: 525.530886888504
batch: 2791
train_loss: 528.3471465110779
batch: 2792
train_loss: 531.1417653560638
batch: 2793
train_loss: 534.0357627868652
batch: 2794
train_loss: 536.8657789230347
batch: 2795
train_loss: 539.7212555408478
batch: 2796
train_loss: 542.5782709121704
batch: 2797
train_loss: 545.5213820934296
batch: 2798
train_loss: 548.2739205360413
batch: 2799
train_loss: 551.0140044689178
| epoch   1 step     2800 |   2800 batches | lr 0.000247 | ms/batch 1164.62 | loss  2.76 | bpc   3.97473
batch: 2800
train_loss: 2.782742500305176
batch: 2801
train_loss: 5.559852123260498
batch: 2802
train_loss: 8.350403785705566
batch: 2803
train_loss: 11.077791690826416
batch: 2804
train_loss: 13.869337558746338
batch: 2805
train_loss: 16.711833000183105
batch: 2806
train_loss: 19.44145655632019
batch: 2807
train_loss: 22.084919929504395
batch: 2808
train_loss: 24.797454595565796
batch: 2809
train_loss: 27.54782462120056
batch: 2810
train_loss: 30.372300148010254
batch: 2811
train_loss: 33.155664920806885
batch: 2812
train_loss: 35.929972410202026
batch: 2813
train_loss: 38.68789625167847
batch: 2814
train_loss: 41.439701557159424
batch: 2815
train_loss: 44.213441371917725
batch: 2816
train_loss: 46.908453702926636
batch: 2817
train_loss: 49.66180443763733
batch: 2818
train_loss: 52.49131393432617
batch: 2819
train_loss: 55.279502153396606
batch: 2820
train_loss: 58.18378520011902
batch: 2821
train_loss: 60.93383312225342
batch: 2822
train_loss: 63.6805944442749
batch: 2823
train_loss: 66.46844363212585
batch: 2824
train_loss: 69.28087306022644
batch: 2825
train_loss: 71.98044180870056
batch: 2826
train_loss: 74.73726558685303
batch: 2827
train_loss: 77.53728747367859
batch: 2828
train_loss: 80.30561113357544
batch: 2829
train_loss: 83.05785274505615
batch: 2830
train_loss: 85.81855845451355
batch: 2831
train_loss: 88.46723437309265
batch: 2832
train_loss: 91.14424109458923
batch: 2833
train_loss: 93.81526494026184
batch: 2834
train_loss: 96.44734263420105
batch: 2835
train_loss: 99.12736821174622
batch: 2836
train_loss: 101.82110643386841
batch: 2837
train_loss: 104.58198022842407
batch: 2838
train_loss: 107.27623319625854
batch: 2839
train_loss: 109.89874196052551
batch: 2840
train_loss: 112.52861952781677
batch: 2841
train_loss: 115.19720530509949
batch: 2842
train_loss: 117.93258213996887
batch: 2843
train_loss: 120.65863037109375
batch: 2844
train_loss: 123.43044114112854
batch: 2845
train_loss: 126.39039921760559
batch: 2846
train_loss: 129.28916311264038
batch: 2847
train_loss: 132.0631558895111
batch: 2848
train_loss: 134.81682634353638
batch: 2849
train_loss: 137.58508586883545
batch: 2850
train_loss: 140.45250630378723
batch: 2851
train_loss: 143.2632303237915
batch: 2852
train_loss: 146.02013087272644
batch: 2853
train_loss: 148.83217668533325
batch: 2854
train_loss: 151.61342597007751
batch: 2855
train_loss: 154.39698553085327
batch: 2856
train_loss: 157.12198281288147
batch: 2857
train_loss: 159.97913813591003
batch: 2858
train_loss: 162.73152875900269
batch: 2859
train_loss: 165.58718609809875
batch: 2860
train_loss: 168.41267013549805
batch: 2861
train_loss: 171.1302547454834
batch: 2862
train_loss: 173.88449358940125
batch: 2863
train_loss: 176.6170938014984
batch: 2864
train_loss: 179.40817189216614
batch: 2865
train_loss: 182.1093168258667
batch: 2866
train_loss: 184.88258290290833
batch: 2867
train_loss: 187.58752584457397
batch: 2868
train_loss: 190.33046698570251
batch: 2869
train_loss: 193.1008825302124
batch: 2870
train_loss: 195.86917996406555
batch: 2871
train_loss: 198.6545763015747
batch: 2872
train_loss: 201.40402722358704
batch: 2873
train_loss: 204.1364028453827
batch: 2874
train_loss: 206.90512132644653
batch: 2875
train_loss: 209.60063982009888
batch: 2876
train_loss: 212.35179615020752
batch: 2877
train_loss: 215.19171142578125
batch: 2878
train_loss: 217.89994406700134
batch: 2879
train_loss: 220.59121084213257
batch: 2880
train_loss: 223.38170647621155
batch: 2881
train_loss: 226.19370794296265
batch: 2882
train_loss: 229.05301332473755
batch: 2883
train_loss: 231.89642977714539
batch: 2884
train_loss: 234.71070456504822
batch: 2885
train_loss: 237.5395905971527
batch: 2886
train_loss: 240.3486852645874
batch: 2887
train_loss: 243.13433504104614
batch: 2888
train_loss: 245.96308159828186
batch: 2889
train_loss: 248.7668468952179
batch: 2890
train_loss: 251.48647713661194
batch: 2891
train_loss: 254.2276747226715
batch: 2892
train_loss: 256.9771399497986
batch: 2893
train_loss: 259.72417187690735
batch: 2894
train_loss: 262.46139550209045
batch: 2895
train_loss: 265.1220352649689
batch: 2896
train_loss: 267.8052587509155
batch: 2897
train_loss: 270.53032302856445
batch: 2898
train_loss: 273.3309600353241
batch: 2899
train_loss: 276.1561813354492
batch: 2900
train_loss: 278.9045958518982
batch: 2901
train_loss: 281.6965911388397
batch: 2902
train_loss: 284.44342398643494
batch: 2903
train_loss: 287.24136543273926
batch: 2904
train_loss: 290.08721804618835
batch: 2905
train_loss: 292.8811492919922
batch: 2906
train_loss: 295.6304533481598
batch: 2907
train_loss: 298.4304299354553
batch: 2908
train_loss: 301.1284945011139
batch: 2909
train_loss: 303.931227684021
batch: 2910
train_loss: 306.68805289268494
batch: 2911
train_loss: 309.41026067733765
batch: 2912
train_loss: 312.1941065788269
batch: 2913
train_loss: 314.9573223590851
batch: 2914
train_loss: 317.7655394077301
batch: 2915
train_loss: 320.54864048957825
batch: 2916
train_loss: 323.29236459732056
batch: 2917
train_loss: 326.02088379859924
batch: 2918
train_loss: 328.82130098342896
batch: 2919
train_loss: 331.51756262779236
batch: 2920
train_loss: 334.2811920642853
batch: 2921
train_loss: 337.04833030700684
batch: 2922
train_loss: 339.74337124824524
batch: 2923
train_loss: 342.4884469509125
batch: 2924
train_loss: 345.22192549705505
batch: 2925
train_loss: 347.93190836906433
batch: 2926
train_loss: 350.66132497787476
batch: 2927
train_loss: 353.4222912788391
batch: 2928
train_loss: 356.1455817222595
batch: 2929
train_loss: 358.96348118782043
batch: 2930
train_loss: 361.7571437358856
batch: 2931
train_loss: 364.41895389556885
batch: 2932
train_loss: 367.0559802055359
batch: 2933
train_loss: 369.6713547706604
batch: 2934
train_loss: 372.3915960788727
batch: 2935
train_loss: 375.1482365131378
batch: 2936
train_loss: 377.7731122970581
batch: 2937
train_loss: 380.50266337394714
batch: 2938
train_loss: 383.31898856163025
batch: 2939
train_loss: 386.0267961025238
batch: 2940
train_loss: 388.7632474899292
batch: 2941
train_loss: 391.3869094848633
batch: 2942
train_loss: 394.0747010707855
batch: 2943
train_loss: 396.82201957702637
batch: 2944
train_loss: 399.52061891555786
batch: 2945
train_loss: 402.2752113342285
batch: 2946
train_loss: 404.96436071395874
batch: 2947
train_loss: 407.6621949672699
batch: 2948
train_loss: 410.3253242969513
batch: 2949
train_loss: 413.08376932144165
batch: 2950
train_loss: 415.8405876159668
batch: 2951
train_loss: 418.5713140964508
batch: 2952
train_loss: 421.2825083732605
batch: 2953
train_loss: 423.984414100647
batch: 2954
train_loss: 426.7240309715271
batch: 2955
train_loss: 429.39761900901794
batch: 2956
train_loss: 432.08483386039734
batch: 2957
train_loss: 434.8656425476074
batch: 2958
train_loss: 437.5738868713379
batch: 2959
train_loss: 440.2365927696228
batch: 2960
train_loss: 442.991818189621
batch: 2961
train_loss: 445.7269723415375
batch: 2962
train_loss: 448.47236466407776
batch: 2963
train_loss: 451.09871792793274
batch: 2964
train_loss: 453.7510995864868
batch: 2965
train_loss: 456.461464881897
batch: 2966
train_loss: 459.1647307872772
batch: 2967
train_loss: 461.77120542526245
batch: 2968
train_loss: 464.48458456993103
batch: 2969
train_loss: 467.17712688446045
batch: 2970
train_loss: 469.8243923187256
batch: 2971
train_loss: 472.5397183895111
batch: 2972
train_loss: 475.2446026802063
batch: 2973
train_loss: 477.93155884742737
batch: 2974
train_loss: 480.6625862121582
batch: 2975
train_loss: 483.32497119903564
batch: 2976
train_loss: 485.9622337818146
batch: 2977
train_loss: 488.61349511146545
batch: 2978
train_loss: 491.3048806190491
batch: 2979
train_loss: 493.9616370201111
batch: 2980
train_loss: 496.61274576187134
batch: 2981
train_loss: 499.3128910064697
batch: 2982
train_loss: 501.97089409828186
batch: 2983
train_loss: 504.6576340198517
batch: 2984
train_loss: 507.3294372558594
batch: 2985
train_loss: 509.9323709011078
batch: 2986
train_loss: 512.5577130317688
batch: 2987
train_loss: 515.2510340213776
batch: 2988
train_loss: 517.9479792118073
batch: 2989
train_loss: 520.595134973526
batch: 2990
train_loss: 523.241461277008
batch: 2991
train_loss: 525.9876866340637
batch: 2992
train_loss: 528.680948972702
batch: 2993
train_loss: 531.357159614563
batch: 2994
train_loss: 534.0533187389374
batch: 2995
train_loss: 536.7096924781799
batch: 2996
train_loss: 539.3703961372375
batch: 2997
train_loss: 542.0167384147644
batch: 2998
train_loss: 544.8445794582367
batch: 2999
train_loss: 547.482259273529
| epoch   1 step     3000 |   3000 batches | lr 0.000247 | ms/batch 1167.27 | loss  2.74 | bpc   3.94925
batch: 3000
train_loss: 2.716371774673462
batch: 3001
train_loss: 5.34185004234314
batch: 3002
train_loss: 8.03908658027649
batch: 3003
train_loss: 10.765384674072266
batch: 3004
train_loss: 13.47812008857727
batch: 3005
train_loss: 16.13629126548767
batch: 3006
train_loss: 18.819303035736084
batch: 3007
train_loss: 21.524901390075684
batch: 3008
train_loss: 24.17722487449646
batch: 3009
train_loss: 26.849020957946777
batch: 3010
train_loss: 29.564934730529785
batch: 3011
train_loss: 32.22652506828308
batch: 3012
train_loss: 34.983120918273926
batch: 3013
train_loss: 37.709574460983276
batch: 3014
train_loss: 40.31538939476013
batch: 3015
train_loss: 42.9517765045166
batch: 3016
train_loss: 45.598024129867554
batch: 3017
train_loss: 48.249886989593506
batch: 3018
train_loss: 50.92529249191284
batch: 3019
train_loss: 53.67129683494568
batch: 3020
train_loss: 56.396652936935425
batch: 3021
train_loss: 59.11801195144653
batch: 3022
train_loss: 61.849895000457764
batch: 3023
train_loss: 64.49625873565674
batch: 3024
train_loss: 67.12304425239563
batch: 3025
train_loss: 69.78278994560242
batch: 3026
train_loss: 72.55481052398682
batch: 3027
train_loss: 75.22022128105164
batch: 3028
train_loss: 77.92277121543884
batch: 3029
train_loss: 80.69594383239746
batch: 3030
train_loss: 83.33503675460815
batch: 3031
train_loss: 86.01786494255066
batch: 3032
train_loss: 88.74420261383057
batch: 3033
train_loss: 91.45008540153503
batch: 3034
train_loss: 94.13942837715149
batch: 3035
train_loss: 96.8158962726593
batch: 3036
train_loss: 99.61370086669922
batch: 3037
train_loss: 102.38079023361206
batch: 3038
train_loss: 105.02937126159668
batch: 3039
train_loss: 107.6546266078949
batch: 3040
train_loss: 110.28665781021118
batch: 3041
train_loss: 112.88424777984619
batch: 3042
train_loss: 115.53539443016052
batch: 3043
train_loss: 118.26192259788513
batch: 3044
train_loss: 120.8689832687378
batch: 3045
train_loss: 123.49558687210083
batch: 3046
train_loss: 126.16519689559937
batch: 3047
train_loss: 128.90766954421997
batch: 3048
train_loss: 131.5686411857605
batch: 3049
train_loss: 134.25164604187012
batch: 3050
train_loss: 136.93185806274414
batch: 3051
train_loss: 139.66313552856445
batch: 3052
train_loss: 142.34408593177795
batch: 3053
train_loss: 145.06919407844543
batch: 3054
train_loss: 147.82485485076904
batch: 3055
train_loss: 150.61054587364197
batch: 3056
train_loss: 153.3659529685974
batch: 3057
train_loss: 156.04824709892273
batch: 3058
train_loss: 158.68685007095337
batch: 3059
train_loss: 161.3790521621704
batch: 3060
train_loss: 164.1255886554718
batch: 3061
train_loss: 166.8698375225067
batch: 3062
train_loss: 169.66661500930786
batch: 3063
train_loss: 172.40548539161682
batch: 3064
train_loss: 175.1811227798462
batch: 3065
train_loss: 177.95086026191711
batch: 3066
train_loss: 180.66134691238403
batch: 3067
train_loss: 183.33305025100708
batch: 3068
train_loss: 186.11267638206482
batch: 3069
train_loss: 188.8489146232605
batch: 3070
train_loss: 191.60802245140076
batch: 3071
train_loss: 194.45485973358154
batch: 3072
train_loss: 197.2191436290741
batch: 3073
train_loss: 200.06581020355225
batch: 3074
train_loss: 202.86868929862976
batch: 3075
train_loss: 205.65601420402527
batch: 3076
train_loss: 208.4283413887024
batch: 3077
train_loss: 211.101642370224
batch: 3078
train_loss: 213.90654945373535
batch: 3079
train_loss: 216.63490295410156
batch: 3080
train_loss: 219.4121916294098
batch: 3081
train_loss: 222.06795525550842
batch: 3082
train_loss: 224.66759967803955
batch: 3083
train_loss: 227.34443283081055
batch: 3084
train_loss: 229.95389032363892
batch: 3085
train_loss: 232.55766320228577
batch: 3086
train_loss: 235.24917674064636
batch: 3087
train_loss: 237.86303567886353
batch: 3088
train_loss: 240.48182225227356
batch: 3089
train_loss: 243.16024112701416
batch: 3090
train_loss: 245.86065316200256
batch: 3091
train_loss: 248.5279335975647
batch: 3092
train_loss: 251.18023133277893
batch: 3093
train_loss: 253.92457675933838
batch: 3094
train_loss: 256.5174298286438
batch: 3095
train_loss: 259.1258969306946
batch: 3096
train_loss: 261.7807674407959
batch: 3097
train_loss: 264.41940116882324
batch: 3098
train_loss: 267.0750207901001
batch: 3099
train_loss: 269.6507053375244
batch: 3100
train_loss: 272.42658710479736
batch: 3101
train_loss: 275.23162841796875
batch: 3102
train_loss: 277.93523120880127
batch: 3103
train_loss: 280.6177966594696
batch: 3104
train_loss: 283.2732744216919
batch: 3105
train_loss: 285.93071961402893
batch: 3106
train_loss: 288.5863907337189
batch: 3107
train_loss: 291.2182812690735
batch: 3108
train_loss: 293.91192054748535
batch: 3109
train_loss: 296.6567780971527
batch: 3110
train_loss: 299.3637595176697
batch: 3111
train_loss: 302.07010555267334
batch: 3112
train_loss: 304.7223229408264
batch: 3113
train_loss: 307.35977244377136
batch: 3114
train_loss: 310.1107816696167
batch: 3115
train_loss: 312.77537417411804
batch: 3116
train_loss: 315.52353382110596
batch: 3117
train_loss: 318.20207500457764
batch: 3118
train_loss: 320.8950672149658
batch: 3119
train_loss: 323.6294467449188
batch: 3120
train_loss: 326.3373680114746
batch: 3121
train_loss: 329.0906448364258
batch: 3122
train_loss: 331.85325384140015
batch: 3123
train_loss: 334.54627299308777
batch: 3124
train_loss: 337.2496237754822
batch: 3125
train_loss: 339.966637134552
batch: 3126
train_loss: 342.6983575820923
batch: 3127
train_loss: 345.4471879005432
batch: 3128
train_loss: 348.2381763458252
batch: 3129
train_loss: 350.9994080066681
batch: 3130
train_loss: 353.8100109100342
batch: 3131
train_loss: 356.592413187027
batch: 3132
train_loss: 359.3147842884064
batch: 3133
train_loss: 362.1803448200226
batch: 3134
train_loss: 364.91811203956604
batch: 3135
train_loss: 367.6442120075226
batch: 3136
train_loss: 370.3256998062134
batch: 3137
train_loss: 373.06493377685547
batch: 3138
train_loss: 375.8360974788666
batch: 3139
train_loss: 378.52034187316895
batch: 3140
train_loss: 381.1167585849762
batch: 3141
train_loss: 383.8736321926117
batch: 3142
train_loss: 386.5136294364929
batch: 3143
train_loss: 389.27338433265686
batch: 3144
train_loss: 391.9799373149872
batch: 3145
train_loss: 394.7076368331909
batch: 3146
train_loss: 397.46111941337585
batch: 3147
train_loss: 400.1082100868225
batch: 3148
train_loss: 402.7943949699402
batch: 3149
train_loss: 405.4186894893646
batch: 3150
train_loss: 408.0716369152069
batch: 3151
train_loss: 410.77046036720276
batch: 3152
train_loss: 413.39572978019714
batch: 3153
train_loss: 416.0401420593262
batch: 3154
train_loss: 418.7646942138672
batch: 3155
train_loss: 421.4187903404236
batch: 3156
train_loss: 424.04711270332336
batch: 3157
train_loss: 426.7666850090027
batch: 3158
train_loss: 429.3847997188568
batch: 3159
train_loss: 432.07414412498474
batch: 3160
train_loss: 434.77897930145264
batch: 3161
train_loss: 437.45206928253174
batch: 3162
train_loss: 440.05063700675964
batch: 3163
