Running SLURM prolog script on pink54.cluster.local
===============================================================================
Job started on Fri May 17 09:39:50 BST 2024
Job ID          : 6000081
Job name        : run_enwik8_rsa_small4.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa_small4.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink54
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 7
    - n_head : 8
    - d_head : 8
    - d_embed : 16
    - d_model : 16
    - d_inner : 128
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 64
    - eval_tgt_len : 32
    - ext_len : 0
    - mem_len : 64
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 69592
    - n_nonemb_param : 65996
====================================================================================================
#params = 69592
#non emb params = 65996
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.3163652420043945
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.623166561126709
batch: 2
train_loss: 15.918050765991211
batch: 3
train_loss: 21.21493673324585
batch: 4
train_loss: 26.493075847625732
batch: 5
train_loss: 31.7648286819458
batch: 6
train_loss: 37.022603034973145
batch: 7
train_loss: 42.27904558181763
batch: 8
train_loss: 47.52614498138428
batch: 9
train_loss: 52.763028144836426
batch: 10
train_loss: 57.993287086486816
batch: 11
train_loss: 63.21564197540283
batch: 12
train_loss: 68.43167304992676
batch: 13
train_loss: 73.64396953582764
batch: 14
train_loss: 78.85004663467407
batch: 15
train_loss: 84.0439829826355
batch: 16
train_loss: 89.23683738708496
batch: 17
train_loss: 94.42851638793945
batch: 18
train_loss: 99.60554075241089
batch: 19
train_loss: 104.78391551971436
batch: 20
train_loss: 109.9575891494751
batch: 21
train_loss: 115.11986875534058
batch: 22
train_loss: 120.283616065979
batch: 23
train_loss: 125.4425573348999
batch: 24
train_loss: 130.59075355529785
batch: 25
train_loss: 135.73952388763428
batch: 26
train_loss: 140.87197065353394
batch: 27
train_loss: 146.00158882141113
batch: 28
train_loss: 151.12139225006104
batch: 29
train_loss: 156.24786806106567
batch: 30
train_loss: 161.35816287994385
batch: 31
train_loss: 166.4747929573059
batch: 32
train_loss: 171.58493900299072
batch: 33
train_loss: 176.69663333892822
batch: 34
train_loss: 181.79727983474731
batch: 35
train_loss: 186.8835964202881
batch: 36
train_loss: 191.96756267547607
batch: 37
train_loss: 197.04921436309814
batch: 38
train_loss: 202.12548732757568
batch: 39
train_loss: 207.1974139213562
batch: 40
train_loss: 212.27501726150513
batch: 41
train_loss: 217.33480024337769
batch: 42
train_loss: 222.39136934280396
batch: 43
train_loss: 227.44076204299927
batch: 44
train_loss: 232.46657276153564
batch: 45
train_loss: 237.4919934272766
batch: 46
train_loss: 242.51222324371338
batch: 47
train_loss: 247.53351736068726
batch: 48
train_loss: 252.5481915473938
batch: 49
train_loss: 257.5590810775757
batch: 50
train_loss: 262.5637001991272
batch: 51
train_loss: 267.56870555877686
batch: 52
train_loss: 272.56347465515137
batch: 53
train_loss: 277.5467367172241
batch: 54
train_loss: 282.54653310775757
batch: 55
train_loss: 287.53144788742065
batch: 56
train_loss: 292.5092272758484
batch: 57
train_loss: 297.48518085479736
batch: 58
train_loss: 302.4540014266968
batch: 59
train_loss: 307.4112071990967
batch: 60
train_loss: 312.36476135253906
batch: 61
train_loss: 317.3126473426819
batch: 62
train_loss: 322.27065324783325
batch: 63
train_loss: 327.2012062072754
batch: 64
train_loss: 332.11879301071167
batch: 65
train_loss: 337.03923654556274
batch: 66
train_loss: 341.95530366897583
batch: 67
train_loss: 346.86234283447266
batch: 68
train_loss: 351.7829303741455
batch: 69
train_loss: 356.67062520980835
batch: 70
train_loss: 361.5420207977295
batch: 71
train_loss: 366.42063570022583
batch: 72
train_loss: 371.28543996810913
batch: 73
train_loss: 376.16193103790283
batch: 74
train_loss: 380.9993314743042
batch: 75
train_loss: 385.82858419418335
batch: 76
train_loss: 390.69976568222046
batch: 77
train_loss: 395.5529475212097
batch: 78
train_loss: 400.3788275718689
batch: 79
train_loss: 405.17665004730225
batch: 80
train_loss: 409.98246335983276
batch: 81
train_loss: 414.7851810455322
batch: 82
train_loss: 419.57286071777344
batch: 83
train_loss: 424.3351969718933
batch: 84
train_loss: 429.12278032302856
batch: 85
train_loss: 433.90608167648315
batch: 86
train_loss: 438.6703658103943
batch: 87
train_loss: 443.4264278411865
batch: 88
train_loss: 448.19243574142456
batch: 89
train_loss: 452.93100786209106
batch: 90
train_loss: 457.6657028198242
batch: 91
train_loss: 462.3990726470947
batch: 92
train_loss: 467.1278820037842
batch: 93
train_loss: 471.85425662994385
batch: 94
train_loss: 476.57517528533936
batch: 95
train_loss: 481.2993755340576
batch: 96
train_loss: 486.0024924278259
batch: 97
train_loss: 490.69721698760986
batch: 98
train_loss: 495.3958878517151
batch: 99
train_loss: 500.09545135498047
batch: 100
train_loss: 504.7933382987976
batch: 101
train_loss: 509.4902558326721
batch: 102
train_loss: 514.1800541877747
batch: 103
train_loss: 518.8588809967041
batch: 104
train_loss: 523.5291237831116
batch: 105
train_loss: 528.1931943893433
batch: 106
train_loss: 532.8642649650574
batch: 107
train_loss: 537.5193243026733
batch: 108
train_loss: 542.169707775116
batch: 109
train_loss: 546.824782371521
batch: 110
train_loss: 551.4505767822266
batch: 111
train_loss: 556.0805172920227
batch: 112
train_loss: 560.7068586349487
batch: 113
train_loss: 565.3320488929749
batch: 114
train_loss: 569.9385447502136
batch: 115
train_loss: 574.5336318016052
batch: 116
train_loss: 579.1273245811462
batch: 117
train_loss: 583.7208743095398
batch: 118
train_loss: 588.3090605735779
batch: 119
train_loss: 592.8941164016724
batch: 120
train_loss: 597.4675970077515
batch: 121
train_loss: 602.0431437492371
batch: 122
train_loss: 606.6358985900879
batch: 123
train_loss: 611.2155871391296
batch: 124
train_loss: 615.7656579017639
batch: 125
train_loss: 620.3170127868652
batch: 126
train_loss: 624.8726391792297
batch: 127
train_loss: 629.4540700912476
batch: 128
train_loss: 633.9913339614868
batch: 129
train_loss: 638.5344200134277
batch: 130
train_loss: 643.0538477897644
batch: 131
train_loss: 647.5911674499512
batch: 132
train_loss: 652.0850853919983
batch: 133
train_loss: 656.6045899391174
batch: 134
train_loss: 661.1080408096313
batch: 135
train_loss: 665.6312370300293
batch: 136
train_loss: 670.1291298866272
batch: 137
train_loss: 674.6240639686584
batch: 138
train_loss: 679.08251953125
batch: 139
train_loss: 683.5802202224731
batch: 140
train_loss: 688.0334496498108
batch: 141
train_loss: 692.4870929718018
batch: 142
train_loss: 696.9527163505554
batch: 143
train_loss: 701.4093399047852
batch: 144
train_loss: 705.8639330863953
batch: 145
train_loss: 710.288592338562
batch: 146
train_loss: 714.7301669120789
batch: 147
train_loss: 719.1994009017944
batch: 148
train_loss: 723.6680936813354
batch: 149
train_loss: 728.0962815284729
batch: 150
train_loss: 732.548698425293
batch: 151
train_loss: 736.9672932624817
batch: 152
train_loss: 741.4175410270691
batch: 153
train_loss: 745.8459053039551
batch: 154
train_loss: 750.2587342262268
batch: 155
train_loss: 754.6320476531982
batch: 156
train_loss: 758.9959950447083
batch: 157
train_loss: 763.3564195632935
batch: 158
train_loss: 767.7621474266052
batch: 159
train_loss: 772.1411027908325
batch: 160
train_loss: 776.4786329269409
batch: 161
train_loss: 780.8488898277283
batch: 162
train_loss: 785.2491765022278
batch: 163
train_loss: 789.6036992073059
batch: 164
train_loss: 793.9310369491577
batch: 165
train_loss: 798.278872013092
batch: 166
train_loss: 802.5695447921753
batch: 167
train_loss: 806.8831562995911
batch: 168
train_loss: 811.1784682273865
batch: 169
train_loss: 815.4317941665649
batch: 170
train_loss: 819.7051038742065
batch: 171
train_loss: 823.9977812767029
batch: 172
train_loss: 828.2938256263733
batch: 173
train_loss: 832.6094288825989
batch: 174
train_loss: 836.8989863395691
batch: 175
train_loss: 841.2120313644409
batch: 176
train_loss: 845.5357484817505
batch: 177
train_loss: 849.8180985450745
batch: 178
train_loss: 854.0703029632568
batch: 179
train_loss: 858.3240103721619
batch: 180
train_loss: 862.569667339325
batch: 181
train_loss: 866.8489389419556
batch: 182
train_loss: 871.2134428024292
batch: 183
train_loss: 875.5164031982422
batch: 184
train_loss: 879.8021397590637
batch: 185
train_loss: 884.0786166191101
batch: 186
train_loss: 888.3479690551758
batch: 187
train_loss: 892.6390061378479
batch: 188
train_loss: 896.8745365142822
batch: 189
train_loss: 901.1170363426208
batch: 190
train_loss: 905.3850297927856
batch: 191
train_loss: 909.6570105552673
batch: 192
train_loss: 913.9141130447388
batch: 193
train_loss: 918.0860614776611
batch: 194
train_loss: 922.3019332885742
batch: 195
train_loss: 926.5745244026184
batch: 196
train_loss: 930.811749458313
batch: 197
train_loss: 934.9728927612305
batch: 198
train_loss: 939.1714520454407
batch: 199
train_loss: 943.3657059669495
| epoch   0 step      200 |    200 batches | lr 0.00025 | ms/batch 348.35 | loss  4.72 | bpc   6.80495
batch: 200
train_loss: 4.197242259979248
batch: 201
train_loss: 8.407150745391846
batch: 202
train_loss: 12.587378025054932
batch: 203
train_loss: 16.84506320953369
batch: 204
train_loss: 21.04596757888794
batch: 205
train_loss: 25.26498031616211
batch: 206
train_loss: 29.419568061828613
batch: 207
train_loss: 33.56382751464844
batch: 208
train_loss: 37.697585582733154
batch: 209
train_loss: 41.82210922241211
batch: 210
train_loss: 45.972007751464844
batch: 211
train_loss: 50.1464581489563
batch: 212
train_loss: 54.25794792175293
batch: 213
train_loss: 58.3700647354126
batch: 214
train_loss: 62.472044944763184
batch: 215
train_loss: 66.56731939315796
batch: 216
train_loss: 70.67381858825684
batch: 217
train_loss: 74.80446910858154
batch: 218
train_loss: 78.89701414108276
batch: 219
train_loss: 83.03740072250366
batch: 220
train_loss: 87.15943717956543
batch: 221
train_loss: 91.26213502883911
batch: 222
train_loss: 95.37156391143799
batch: 223
train_loss: 99.46000909805298
batch: 224
train_loss: 103.51992988586426
batch: 225
train_loss: 107.61229419708252
batch: 226
train_loss: 111.6754903793335
batch: 227
train_loss: 115.71664667129517
batch: 228
train_loss: 119.84272813796997
batch: 229
train_loss: 123.93933916091919
batch: 230
train_loss: 128.00145053863525
batch: 231
train_loss: 132.0890555381775
batch: 232
train_loss: 136.1985993385315
batch: 233
train_loss: 140.29849767684937
batch: 234
train_loss: 144.41189813613892
batch: 235
train_loss: 148.58864831924438
batch: 236
train_loss: 152.69706916809082
batch: 237
train_loss: 156.802481174469
batch: 238
train_loss: 160.85602951049805
batch: 239
train_loss: 164.91397619247437
batch: 240
train_loss: 168.95245933532715
batch: 241
train_loss: 173.0490026473999
batch: 242
train_loss: 177.12107944488525
batch: 243
train_loss: 181.1508026123047
batch: 244
train_loss: 185.18790864944458
batch: 245
train_loss: 189.24114322662354
batch: 246
train_loss: 193.2652564048767
batch: 247
train_loss: 197.3190941810608
batch: 248
train_loss: 201.2864830493927
batch: 249
train_loss: 205.27015852928162
batch: 250
train_loss: 209.26363945007324
batch: 251
train_loss: 213.21600770950317
batch: 252
train_loss: 217.17819952964783
batch: 253
train_loss: 221.12687182426453
batch: 254
train_loss: 225.08003568649292
batch: 255
train_loss: 228.9872395992279
batch: 256
train_loss: 232.94781637191772
batch: 257
train_loss: 236.8631751537323
batch: 258
train_loss: 240.81656885147095
batch: 259
train_loss: 244.74790263175964
batch: 260
train_loss: 248.65866684913635
batch: 261
train_loss: 252.6265845298767
batch: 262
train_loss: 256.5182855129242
batch: 263
train_loss: 260.42778396606445
batch: 264
train_loss: 264.3836336135864
batch: 265
train_loss: 268.34434151649475
batch: 266
train_loss: 272.25223183631897
batch: 267
train_loss: 276.1542978286743
batch: 268
train_loss: 280.0382432937622
batch: 269
train_loss: 283.9222762584686
batch: 270
train_loss: 287.837641954422
batch: 271
train_loss: 291.7505989074707
batch: 272
train_loss: 295.6224892139435
batch: 273
train_loss: 299.4388449192047
batch: 274
train_loss: 303.26681661605835
batch: 275
train_loss: 307.1829159259796
batch: 276
train_loss: 311.04640316963196
batch: 277
train_loss: 314.9207262992859
batch: 278
train_loss: 318.77637577056885
batch: 279
train_loss: 322.6044297218323
batch: 280
train_loss: 326.4308714866638
batch: 281
train_loss: 330.26535964012146
batch: 282
train_loss: 334.1125512123108
batch: 283
train_loss: 337.96622490882874
batch: 284
train_loss: 341.83234572410583
batch: 285
train_loss: 345.7116198539734
batch: 286
train_loss: 349.53746461868286
batch: 287
train_loss: 353.37443447113037
batch: 288
train_loss: 357.2209987640381
batch: 289
train_loss: 361.07302069664
batch: 290
train_loss: 364.90279746055603
batch: 291
train_loss: 368.733526468277
batch: 292
train_loss: 372.5890667438507
batch: 293
train_loss: 376.3439061641693
batch: 294
train_loss: 380.16920042037964
batch: 295
train_loss: 383.9827606678009
batch: 296
train_loss: 387.8567442893982
batch: 297
train_loss: 391.68729519844055
batch: 298
train_loss: 395.5487127304077
batch: 299
train_loss: 399.3937644958496
batch: 300
train_loss: 403.21310448646545
batch: 301
train_loss: 407.0420207977295
batch: 302
train_loss: 410.8300859928131
batch: 303
train_loss: 414.6946153640747
batch: 304
train_loss: 418.47866129875183
batch: 305
train_loss: 422.2230010032654
batch: 306
train_loss: 425.9791085720062
batch: 307
train_loss: 429.72065806388855
batch: 308
train_loss: 433.5028028488159
batch: 309
train_loss: 437.37222933769226
batch: 310
train_loss: 441.24729013442993
batch: 311
train_loss: 445.01854729652405
batch: 312
train_loss: 448.7751352787018
batch: 313
train_loss: 452.53994393348694
batch: 314
train_loss: 456.2773208618164
batch: 315
train_loss: 460.0072600841522
batch: 316
train_loss: 463.7275700569153
batch: 317
train_loss: 467.4404399394989
batch: 318
train_loss: 471.1351990699768
batch: 319
train_loss: 474.93365693092346
batch: 320
train_loss: 478.66136741638184
batch: 321
train_loss: 482.4148066043854
batch: 322
train_loss: 486.18258142471313
batch: 323
train_loss: 489.86271810531616
batch: 324
train_loss: 493.6135575771332
batch: 325
train_loss: 497.3623321056366
batch: 326
train_loss: 501.10644459724426
batch: 327
train_loss: 504.87997674942017
batch: 328
train_loss: 508.58390498161316
batch: 329
train_loss: 512.3549439907074
batch: 330
train_loss: 516.1396219730377
batch: 331
train_loss: 519.9024324417114
batch: 332
train_loss: 523.5925438404083
batch: 333
train_loss: 527.2654702663422
batch: 334
train_loss: 530.9579207897186
batch: 335
train_loss: 534.6188759803772
batch: 336
train_loss: 538.2573683261871
batch: 337
train_loss: 541.8918132781982
batch: 338
train_loss: 545.5098094940186
batch: 339
train_loss: 549.1698217391968
batch: 340
train_loss: 552.8040134906769
batch: 341
train_loss: 556.4388470649719
batch: 342
train_loss: 560.0941431522369
batch: 343
train_loss: 563.7824687957764
batch: 344
train_loss: 567.4299726486206
batch: 345
train_loss: 571.0600235462189
batch: 346
train_loss: 574.7502017021179
batch: 347
train_loss: 578.3416299819946
batch: 348
train_loss: 581.9537971019745
batch: 349
train_loss: 585.5877668857574
batch: 350
train_loss: 589.1577484607697
batch: 351
train_loss: 592.7915635108948
batch: 352
train_loss: 596.4443843364716
batch: 353
train_loss: 600.0043008327484
batch: 354
train_loss: 603.593186378479
batch: 355
train_loss: 607.1529438495636
batch: 356
train_loss: 610.732818365097
batch: 357
train_loss: 614.3389873504639
batch: 358
train_loss: 617.9946911334991
batch: 359
train_loss: 621.6195623874664
batch: 360
train_loss: 625.2730457782745
batch: 361
train_loss: 628.8800754547119
batch: 362
train_loss: 632.5176875591278
batch: 363
train_loss: 636.1920766830444
batch: 364
train_loss: 639.8810474872589
batch: 365
train_loss: 643.485978603363
batch: 366
train_loss: 647.0730392932892
batch: 367
train_loss: 650.6934387683868
batch: 368
train_loss: 654.3121001720428
batch: 369
train_loss: 657.9926435947418
batch: 370
train_loss: 661.6125631332397
batch: 371
train_loss: 665.2451133728027
batch: 372
train_loss: 668.8816010951996
batch: 373
train_loss: 672.4776117801666
batch: 374
train_loss: 676.1867249011993
batch: 375
train_loss: 679.8925757408142
batch: 376
train_loss: 683.5961303710938
batch: 377
train_loss: 687.2351739406586
batch: 378
train_loss: 690.9189713001251
batch: 379
train_loss: 694.6627013683319
batch: 380
train_loss: 698.3106603622437
batch: 381
train_loss: 702.0125153064728
batch: 382
train_loss: 705.7227561473846
batch: 383
train_loss: 709.343386888504
batch: 384
train_loss: 712.9505496025085
batch: 385
train_loss: 716.5957078933716
batch: 386
train_loss: 720.1754302978516
batch: 387
train_loss: 723.7652449607849
batch: 388
train_loss: 727.4619119167328
batch: 389
train_loss: 731.1048333644867
batch: 390
train_loss: 734.7686343193054
batch: 391
train_loss: 738.4243767261505
batch: 392
train_loss: 742.0341310501099
batch: 393
train_loss: 745.7075660228729
batch: 394
train_loss: 749.3783605098724
batch: 395
train_loss: 753.0386550426483
batch: 396
train_loss: 756.6319479942322
batch: 397
train_loss: 760.2438309192657
batch: 398
train_loss: 763.869677066803
batch: 399
train_loss: 767.4870448112488
| epoch   0 step      400 |    400 batches | lr 0.00025 | ms/batch 348.40 | loss  3.84 | bpc   5.53625
batch: 400
train_loss: 3.6083931922912598
batch: 401
train_loss: 7.279666900634766
batch: 402
train_loss: 10.896517992019653
batch: 403
train_loss: 14.509450435638428
batch: 404
train_loss: 18.196993112564087
batch: 405
train_loss: 21.871323823928833
batch: 406
train_loss: 25.541069984436035
batch: 407
train_loss: 29.142199754714966
batch: 408
train_loss: 32.78933572769165
batch: 409
train_loss: 36.416741132736206
batch: 410
train_loss: 39.998106956481934
batch: 411
train_loss: 43.65391540527344
batch: 412
train_loss: 47.27537155151367
batch: 413
train_loss: 50.8828980922699
batch: 414
train_loss: 54.413156270980835
batch: 415
train_loss: 57.9958770275116
batch: 416
train_loss: 61.531678915023804
batch: 417
train_loss: 65.15681385993958
batch: 418
train_loss: 68.68568253517151
batch: 419
train_loss: 72.20919346809387
batch: 420
train_loss: 75.7958357334137
batch: 421
train_loss: 79.35020804405212
batch: 422
train_loss: 82.8689877986908
batch: 423
train_loss: 86.40682411193848
batch: 424
train_loss: 90.0027379989624
batch: 425
train_loss: 93.6513295173645
batch: 426
train_loss: 97.27126336097717
batch: 427
train_loss: 100.8944764137268
batch: 428
train_loss: 104.4867537021637
batch: 429
train_loss: 108.11736965179443
batch: 430
train_loss: 111.71244096755981
batch: 431
train_loss: 115.32527351379395
batch: 432
train_loss: 118.9481143951416
batch: 433
train_loss: 122.52254271507263
batch: 434
train_loss: 126.17166900634766
batch: 435
train_loss: 129.7419502735138
batch: 436
train_loss: 133.38738083839417
batch: 437
train_loss: 136.9871952533722
batch: 438
train_loss: 140.51743054389954
batch: 439
train_loss: 144.05303311347961
batch: 440
train_loss: 147.64005613327026
batch: 441
train_loss: 151.25520515441895
batch: 442
train_loss: 154.88269543647766
batch: 443
train_loss: 158.51579093933105
batch: 444
train_loss: 162.14595246315002
batch: 445
train_loss: 165.730242729187
batch: 446
train_loss: 169.26711010932922
batch: 447
train_loss: 172.83502840995789
batch: 448
train_loss: 176.41762280464172
batch: 449
train_loss: 180.0118305683136
batch: 450
train_loss: 183.5352063179016
batch: 451
train_loss: 187.01202297210693
batch: 452
train_loss: 190.5493290424347
batch: 453
train_loss: 194.02990698814392
batch: 454
train_loss: 197.55242824554443
batch: 455
train_loss: 200.99776816368103
batch: 456
train_loss: 204.49925351142883
batch: 457
train_loss: 207.9704670906067
batch: 458
train_loss: 211.54680752754211
batch: 459
train_loss: 215.07474541664124
batch: 460
train_loss: 218.56763100624084
batch: 461
train_loss: 222.02982187271118
batch: 462
train_loss: 225.50678491592407
batch: 463
train_loss: 228.9728593826294
batch: 464
train_loss: 232.48246693611145
batch: 465
train_loss: 235.95767831802368
batch: 466
train_loss: 239.5207405090332
batch: 467
train_loss: 243.08289623260498
batch: 468
train_loss: 246.52563762664795
batch: 469
train_loss: 250.01753997802734
batch: 470
train_loss: 253.48231101036072
batch: 471
train_loss: 257.0187511444092
batch: 472
train_loss: 260.5576162338257
batch: 473
train_loss: 264.1110677719116
batch: 474
train_loss: 267.61189317703247
batch: 475
train_loss: 271.1452867984772
batch: 476
train_loss: 274.6827356815338
batch: 477
train_loss: 278.25598788261414
batch: 478
train_loss: 281.7084481716156
batch: 479
train_loss: 285.12043929100037
batch: 480
train_loss: 288.6880578994751
batch: 481
train_loss: 292.13687443733215
batch: 482
train_loss: 295.5199816226959
batch: 483
train_loss: 298.9133541584015
batch: 484
train_loss: 302.309889793396
batch: 485
train_loss: 305.8294723033905
batch: 486
train_loss: 309.2875897884369
batch: 487
train_loss: 312.6672706604004
batch: 488
train_loss: 316.06251883506775
batch: 489
train_loss: 319.5475926399231
batch: 490
train_loss: 323.0342471599579
batch: 491
train_loss: 326.57747316360474
batch: 492
train_loss: 330.0757772922516
batch: 493
train_loss: 333.665066242218
batch: 494
train_loss: 337.1509189605713
batch: 495
train_loss: 340.6151964664459
batch: 496
train_loss: 344.1120300292969
batch: 497
train_loss: 347.55344414711
batch: 498
train_loss: 351.0908808708191
batch: 499
train_loss: 354.66470289230347
batch: 500
train_loss: 358.23564553260803
batch: 501
train_loss: 361.68896293640137
batch: 502
train_loss: 365.28562664985657
batch: 503
train_loss: 368.85377526283264
batch: 504
train_loss: 372.353413105011
batch: 505
train_loss: 375.81222200393677
batch: 506
train_loss: 379.3278079032898
batch: 507
train_loss: 382.79635429382324
batch: 508
train_loss: 386.36469435691833
batch: 509
train_loss: 389.94148445129395
batch: 510
train_loss: 393.5045201778412
batch: 511
train_loss: 396.98958802223206
batch: 512
train_loss: 400.4832036495209
batch: 513
train_loss: 404.00598526000977
batch: 514
train_loss: 407.4950454235077
batch: 515
train_loss: 410.9323379993439
batch: 516
train_loss: 414.4743673801422
batch: 517
train_loss: 418.01375246047974
batch: 518
train_loss: 421.47856974601746
batch: 519
train_loss: 424.9345750808716
batch: 520
train_loss: 428.431143283844
batch: 521
train_loss: 431.8661789894104
batch: 522
train_loss: 435.3151412010193
batch: 523
train_loss: 438.7976942062378
batch: 524
train_loss: 442.29430890083313
batch: 525
train_loss: 445.71462750434875
batch: 526
train_loss: 449.18666100502014
batch: 527
train_loss: 452.69333958625793
batch: 528
train_loss: 456.1416697502136
batch: 529
train_loss: 459.6299395561218
batch: 530
train_loss: 463.05309653282166
batch: 531
train_loss: 466.5028898715973
batch: 532
train_loss: 470.015323638916
batch: 533
train_loss: 473.5304853916168
batch: 534
train_loss: 477.11054825782776
batch: 535
train_loss: 480.5573241710663
batch: 536
train_loss: 483.9888093471527
batch: 537
train_loss: 487.4967155456543
batch: 538
train_loss: 490.95325779914856
batch: 539
train_loss: 494.4288353919983
batch: 540
train_loss: 497.8644006252289
batch: 541
train_loss: 501.37458515167236
batch: 542
train_loss: 504.87709045410156
batch: 543
train_loss: 508.42214035987854
batch: 544
train_loss: 511.9772779941559
batch: 545
train_loss: 515.4888558387756
batch: 546
train_loss: 519.0849268436432
batch: 547
train_loss: 522.6484324932098
batch: 548
train_loss: 526.0612542629242
batch: 549
train_loss: 529.4678881168365
batch: 550
train_loss: 532.87744307518
batch: 551
train_loss: 536.4231967926025
batch: 552
train_loss: 539.9663450717926
batch: 553
train_loss: 543.456780910492
batch: 554
train_loss: 546.9713668823242
batch: 555
train_loss: 550.5079383850098
batch: 556
train_loss: 553.9746177196503
batch: 557
train_loss: 557.4089829921722
batch: 558
train_loss: 560.8846719264984
batch: 559
train_loss: 564.243371963501
batch: 560
train_loss: 567.696359872818
batch: 561
train_loss: 571.1509253978729
batch: 562
train_loss: 574.6697325706482
batch: 563
train_loss: 578.1481282711029
batch: 564
train_loss: 581.6081709861755
batch: 565
train_loss: 585.0407090187073
batch: 566
train_loss: 588.4054396152496
batch: 567
train_loss: 591.8583805561066
batch: 568
train_loss: 595.2927827835083
batch: 569
train_loss: 598.7348825931549
batch: 570
train_loss: 602.120851278305
batch: 571
train_loss: 605.6202731132507
batch: 572
train_loss: 609.0354363918304
batch: 573
train_loss: 612.4678874015808
batch: 574
train_loss: 615.9246201515198
batch: 575
train_loss: 619.3166489601135
batch: 576
train_loss: 622.6828360557556
batch: 577
train_loss: 626.0590498447418
batch: 578
train_loss: 629.4606812000275
batch: 579
train_loss: 632.90251994133
batch: 580
train_loss: 636.4624421596527
batch: 581
train_loss: 639.9959762096405
batch: 582
train_loss: 643.5024559497833
batch: 583
train_loss: 646.9983243942261
batch: 584
train_loss: 650.4647765159607
batch: 585
train_loss: 653.9434583187103
batch: 586
train_loss: 657.4155452251434
batch: 587
train_loss: 660.8895976543427
batch: 588
train_loss: 664.2653985023499
batch: 589
train_loss: 667.7059016227722
batch: 590
train_loss: 671.0538766384125
batch: 591
train_loss: 674.4826171398163
batch: 592
train_loss: 677.9184045791626
batch: 593
train_loss: 681.3411707878113
batch: 594
train_loss: 684.7193729877472
batch: 595
train_loss: 688.1852207183838
batch: 596
train_loss: 691.5473446846008
batch: 597
train_loss: 694.9444241523743
batch: 598
train_loss: 698.3074588775635
batch: 599
train_loss: 701.6858365535736
| epoch   0 step      600 |    600 batches | lr 0.00025 | ms/batch 347.74 | loss  3.51 | bpc   5.06159
batch: 600
train_loss: 3.469416618347168
batch: 601
train_loss: 6.94352650642395
batch: 602
train_loss: 10.428534030914307
batch: 603
train_loss: 13.913477659225464
batch: 604
train_loss: 17.373657703399658
batch: 605
train_loss: 20.848896503448486
batch: 606
train_loss: 24.26203727722168
batch: 607
train_loss: 27.716470956802368
batch: 608
train_loss: 31.137998580932617
batch: 609
train_loss: 34.58102631568909
batch: 610
train_loss: 37.914122104644775
batch: 611
train_loss: 41.333558559417725
batch: 612
train_loss: 44.74431133270264
batch: 613
train_loss: 48.12791132926941
batch: 614
train_loss: 51.509830474853516
batch: 615
train_loss: 54.906614542007446
batch: 616
train_loss: 58.23845195770264
batch: 617
train_loss: 61.585142374038696
batch: 618
train_loss: 64.98398065567017
batch: 619
train_loss: 68.39640307426453
batch: 620
train_loss: 71.85125017166138
batch: 621
train_loss: 75.3252260684967
batch: 622
train_loss: 78.80360150337219
batch: 623
train_loss: 82.22146272659302
batch: 624
train_loss: 85.66613411903381
batch: 625
train_loss: 89.06353855133057
batch: 626
train_loss: 92.54760766029358
batch: 627
train_loss: 95.98261070251465
batch: 628
train_loss: 99.43997526168823
batch: 629
train_loss: 102.92439675331116
batch: 630
train_loss: 106.43777847290039
batch: 631
train_loss: 109.93530488014221
batch: 632
train_loss: 113.45159769058228
batch: 633
train_loss: 116.95355367660522
batch: 634
train_loss: 120.47191548347473
batch: 635
train_loss: 124.00338387489319
batch: 636
train_loss: 127.43030405044556
batch: 637
train_loss: 130.93299555778503
batch: 638
train_loss: 134.4126489162445
batch: 639
train_loss: 137.97037839889526
batch: 640
train_loss: 141.48380994796753
batch: 641
train_loss: 144.90386295318604
batch: 642
train_loss: 148.32954025268555
batch: 643
train_loss: 151.72299218177795
batch: 644
train_loss: 155.0917513370514
batch: 645
train_loss: 158.5425670146942
batch: 646
train_loss: 162.04274940490723
batch: 647
train_loss: 165.50873517990112
batch: 648
train_loss: 168.9182162284851
batch: 649
train_loss: 172.422700881958
batch: 650
train_loss: 175.96085262298584
batch: 651
train_loss: 179.47578620910645
batch: 652
train_loss: 183.0371072292328
batch: 653
train_loss: 186.5974838733673
batch: 654
train_loss: 190.04372334480286
batch: 655
train_loss: 193.47616267204285
batch: 656
train_loss: 196.99608707427979
batch: 657
train_loss: 200.41851997375488
batch: 658
train_loss: 203.87393736839294
batch: 659
train_loss: 207.2965178489685
batch: 660
train_loss: 210.7360861301422
batch: 661
train_loss: 214.1374592781067
batch: 662
train_loss: 217.58534264564514
batch: 663
train_loss: 221.06617212295532
batch: 664
train_loss: 224.50662446022034
batch: 665
train_loss: 227.93238377571106
batch: 666
train_loss: 231.35847926139832
batch: 667
train_loss: 234.8912558555603
batch: 668
train_loss: 238.28928065299988
batch: 669
train_loss: 241.766352891922
batch: 670
train_loss: 245.20720601081848
batch: 671
train_loss: 248.6707899570465
batch: 672
train_loss: 252.0582251548767
batch: 673
train_loss: 255.4119794368744
batch: 674
train_loss: 258.8134551048279
batch: 675
train_loss: 262.1925024986267
batch: 676
train_loss: 265.5730299949646
batch: 677
train_loss: 268.9144959449768
batch: 678
train_loss: 272.3027129173279
batch: 679
train_loss: 275.69227957725525
batch: 680
train_loss: 279.1412036418915
batch: 681
train_loss: 282.4724192619324
batch: 682
train_loss: 285.86070942878723
batch: 683
train_loss: 289.3187792301178
batch: 684
train_loss: 292.6723999977112
batch: 685
train_loss: 296.02554273605347
batch: 686
train_loss: 299.41147923469543
batch: 687
train_loss: 302.90208745002747
batch: 688
train_loss: 306.2497413158417
batch: 689
train_loss: 309.64197754859924
batch: 690
train_loss: 313.0644483566284
batch: 691
train_loss: 316.49073219299316
batch: 692
train_loss: 319.90065121650696
batch: 693
train_loss: 323.2684202194214
batch: 694
train_loss: 326.7514953613281
batch: 695
train_loss: 330.2079405784607
batch: 696
train_loss: 333.5973205566406
batch: 697
train_loss: 336.99803376197815
batch: 698
train_loss: 340.4027576446533
batch: 699
train_loss: 343.8182888031006
batch: 700
train_loss: 347.2565565109253
batch: 701
train_loss: 350.54999923706055
batch: 702
train_loss: 353.9635558128357
batch: 703
train_loss: 357.32765102386475
batch: 704
train_loss: 360.72531247138977
batch: 705
train_loss: 364.14892983436584
batch: 706
train_loss: 367.5177278518677
batch: 707
train_loss: 370.90314197540283
batch: 708
train_loss: 374.30033659935
batch: 709
train_loss: 377.7322254180908
batch: 710
train_loss: 381.15135288238525
batch: 711
train_loss: 384.557076215744
batch: 712
train_loss: 387.98007440567017
batch: 713
train_loss: 391.3847153186798
batch: 714
train_loss: 394.70602202415466
batch: 715
train_loss: 398.09646797180176
batch: 716
train_loss: 401.51462602615356
batch: 717
train_loss: 404.84406042099
batch: 718
train_loss: 408.11817240715027
batch: 719
train_loss: 411.4601562023163
batch: 720
train_loss: 414.8460464477539
batch: 721
train_loss: 418.22813177108765
batch: 722
train_loss: 421.615923166275
batch: 723
train_loss: 425.0095512866974
batch: 724
train_loss: 428.4201989173889
batch: 725
train_loss: 431.75838923454285
batch: 726
train_loss: 435.12286353111267
batch: 727
train_loss: 438.4277198314667
batch: 728
train_loss: 441.74840903282166
batch: 729
train_loss: 445.1055197715759
batch: 730
train_loss: 448.41162872314453
batch: 731
train_loss: 451.70450043678284
batch: 732
train_loss: 454.9870038032532
batch: 733
train_loss: 458.22471284866333
batch: 734
train_loss: 461.60220098495483
batch: 735
train_loss: 464.91910457611084
batch: 736
train_loss: 468.3394651412964
batch: 737
train_loss: 471.7121570110321
batch: 738
train_loss: 475.0970621109009
batch: 739
train_loss: 478.459924697876
batch: 740
train_loss: 481.8055417537689
batch: 741
train_loss: 485.1445960998535
batch: 742
train_loss: 488.47012639045715
batch: 743
train_loss: 491.9045670032501
batch: 744
train_loss: 495.2343375682831
batch: 745
train_loss: 498.6166207790375
batch: 746
train_loss: 501.97995686531067
batch: 747
train_loss: 505.2774977684021
batch: 748
train_loss: 508.6290817260742
batch: 749
train_loss: 511.9130742549896
batch: 750
train_loss: 515.1732349395752
batch: 751
train_loss: 518.4801206588745
batch: 752
train_loss: 521.8146641254425
batch: 753
train_loss: 525.1725902557373
batch: 754
train_loss: 528.5488247871399
batch: 755
train_loss: 531.8884565830231
batch: 756
train_loss: 535.1954953670502
batch: 757
train_loss: 538.5196940898895
batch: 758
train_loss: 541.7571909427643
batch: 759
train_loss: 545.0799601078033
batch: 760
train_loss: 548.4359784126282
batch: 761
train_loss: 551.7392520904541
batch: 762
train_loss: 555.0669877529144
batch: 763
train_loss: 558.3721137046814
batch: 764
train_loss: 561.6955933570862
batch: 765
train_loss: 565.0937042236328
batch: 766
train_loss: 568.4408521652222
batch: 767
train_loss: 571.7971179485321
batch: 768
train_loss: 575.2054529190063
batch: 769
train_loss: 578.6524972915649
batch: 770
train_loss: 582.0757172107697
batch: 771
train_loss: 585.4228930473328
batch: 772
train_loss: 588.738495349884
batch: 773
train_loss: 592.1284670829773
batch: 774
train_loss: 595.445463180542
batch: 775
train_loss: 598.7441966533661
batch: 776
train_loss: 602.0991923809052
batch: 777
train_loss: 605.3869488239288
batch: 778
train_loss: 608.6797919273376
batch: 779
train_loss: 611.9593751430511
batch: 780
train_loss: 615.2148735523224
batch: 781
train_loss: 618.475766658783
batch: 782
train_loss: 621.7796635627747
batch: 783
train_loss: 625.0589773654938
batch: 784
train_loss: 628.3384566307068
batch: 785
train_loss: 631.6633048057556
batch: 786
train_loss: 634.9445776939392
batch: 787
train_loss: 638.2690441608429
batch: 788
train_loss: 641.5502247810364
batch: 789
train_loss: 644.9114735126495
batch: 790
train_loss: 648.2331228256226
batch: 791
train_loss: 651.444263458252
batch: 792
train_loss: 654.8143796920776
batch: 793
train_loss: 658.077615737915
batch: 794
train_loss: 661.4054822921753
batch: 795
train_loss: 664.7759428024292
batch: 796
train_loss: 668.0738081932068
batch: 797
train_loss: 671.3401391506195
batch: 798
train_loss: 674.6513049602509
batch: 799
train_loss: 677.937716960907
| epoch   0 step      800 |    800 batches | lr 0.00025 | ms/batch 346.44 | loss  3.39 | bpc   4.89029
batch: 800
train_loss: 3.2860257625579834
batch: 801
train_loss: 6.584538221359253
batch: 802
train_loss: 9.918262243270874
batch: 803
train_loss: 13.32666277885437
batch: 804
train_loss: 16.68266248703003
batch: 805
train_loss: 20.027320623397827
batch: 806
train_loss: 23.480738401412964
batch: 807
train_loss: 26.85225009918213
batch: 808
train_loss: 30.191018104553223
batch: 809
train_loss: 33.56365966796875
batch: 810
train_loss: 36.869892835617065
batch: 811
train_loss: 40.22424006462097
batch: 812
train_loss: 43.58654189109802
batch: 813
train_loss: 46.87413501739502
batch: 814
train_loss: 50.32807397842407
batch: 815
train_loss: 53.66788578033447
batch: 816
train_loss: 57.02345824241638
batch: 817
train_loss: 60.29983973503113
batch: 818
train_loss: 63.638718366622925
batch: 819
train_loss: 66.91439485549927
batch: 820
train_loss: 70.17709374427795
batch: 821
train_loss: 73.4771842956543
batch: 822
train_loss: 76.79306817054749
batch: 823
train_loss: 80.16911315917969
batch: 824
train_loss: 83.59154653549194
batch: 825
train_loss: 86.97639441490173
batch: 826
train_loss: 90.27867341041565
batch: 827
train_loss: 93.55193185806274
batch: 828
train_loss: 96.85445404052734
batch: 829
train_loss: 100.12708878517151
batch: 830
train_loss: 103.42310547828674
batch: 831
train_loss: 106.70949602127075
batch: 832
train_loss: 109.93952679634094
batch: 833
train_loss: 113.18086838722229
batch: 834
train_loss: 116.45852899551392
batch: 835
train_loss: 119.72505402565002
batch: 836
train_loss: 123.03861379623413
batch: 837
train_loss: 126.34994053840637
batch: 838
train_loss: 129.67316436767578
batch: 839
train_loss: 132.94973516464233
batch: 840
train_loss: 136.25202107429504
batch: 841
train_loss: 139.45771431922913
batch: 842
train_loss: 142.79936933517456
batch: 843
train_loss: 146.00166130065918
batch: 844
train_loss: 149.2420780658722
batch: 845
train_loss: 152.56760048866272
batch: 846
train_loss: 155.78979802131653
batch: 847
train_loss: 159.10796761512756
batch: 848
train_loss: 162.4704065322876
batch: 849
train_loss: 165.77793192863464
batch: 850
train_loss: 169.05299973487854
batch: 851
train_loss: 172.36408042907715
batch: 852
train_loss: 175.59796953201294
batch: 853
train_loss: 178.8678755760193
batch: 854
train_loss: 182.1742672920227
batch: 855
train_loss: 185.43266582489014
batch: 856
train_loss: 188.62534546852112
batch: 857
train_loss: 191.9016251564026
batch: 858
train_loss: 195.2113025188446
batch: 859
train_loss: 198.46609473228455
batch: 860
train_loss: 201.71023297309875
batch: 861
train_loss: 205.0004804134369
batch: 862
train_loss: 208.22647356987
batch: 863
train_loss: 211.45329022407532
batch: 864
train_loss: 214.6865475177765
batch: 865
train_loss: 217.89780163764954
batch: 866
train_loss: 221.0676565170288
batch: 867
train_loss: 224.26622676849365
batch: 868
train_loss: 227.50869822502136
batch: 869
train_loss: 230.78009557724
batch: 870
train_loss: 233.95001769065857
batch: 871
train_loss: 237.15840768814087
batch: 872
train_loss: 240.3475685119629
batch: 873
train_loss: 243.53415846824646
batch: 874
train_loss: 246.71025919914246
batch: 875
train_loss: 249.9532232284546
batch: 876
train_loss: 253.14435172080994
batch: 877
train_loss: 256.3176131248474
batch: 878
train_loss: 259.48110127449036
batch: 879
train_loss: 262.6319634914398
batch: 880
train_loss: 265.8115167617798
batch: 881
train_loss: 269.0231285095215
batch: 882
train_loss: 272.2229337692261
batch: 883
train_loss: 275.35465121269226
batch: 884
train_loss: 278.5128960609436
batch: 885
train_loss: 281.6858322620392
batch: 886
train_loss: 284.84818744659424
batch: 887
train_loss: 288.1547510623932
batch: 888
train_loss: 291.4333453178406
batch: 889
train_loss: 294.68496680259705
batch: 890
train_loss: 297.8315463066101
batch: 891
train_loss: 301.0666904449463
batch: 892
train_loss: 304.2792501449585
batch: 893
train_loss: 307.4456853866577
batch: 894
train_loss: 310.7044987678528
batch: 895
train_loss: 313.9123613834381
batch: 896
train_loss: 317.1328797340393
batch: 897
train_loss: 320.4267349243164
batch: 898
train_loss: 323.7837438583374
batch: 899
train_loss: 327.06561946868896
batch: 900
train_loss: 330.2971701622009
batch: 901
train_loss: 333.5663275718689
batch: 902
train_loss: 336.8707604408264
batch: 903
train_loss: 340.17987298965454
batch: 904
train_loss: 343.4612283706665
batch: 905
train_loss: 346.74434757232666
batch: 906
train_loss: 349.93297004699707
batch: 907
train_loss: 353.15508699417114
batch: 908
train_loss: 356.3628771305084
batch: 909
train_loss: 359.65642786026
batch: 910
train_loss: 362.9088227748871
batch: 911
train_loss: 366.1522650718689
batch: 912
train_loss: 369.3945496082306
batch: 913
train_loss: 372.69716334342957
batch: 914
train_loss: 375.9714722633362
batch: 915
train_loss: 379.2585244178772
batch: 916
train_loss: 382.5805230140686
batch: 917
train_loss: 385.87088561058044
batch: 918
train_loss: 389.11447834968567
batch: 919
train_loss: 392.3400115966797
batch: 920
train_loss: 395.6376440525055
batch: 921
train_loss: 398.90463614463806
batch: 922
train_loss: 402.2424383163452
batch: 923
train_loss: 405.554785490036
batch: 924
train_loss: 408.7669026851654
batch: 925
train_loss: 412.06898641586304
batch: 926
train_loss: 415.2993993759155
batch: 927
train_loss: 418.5083830356598
batch: 928
train_loss: 421.7856366634369
batch: 929
train_loss: 425.0255534648895
batch: 930
train_loss: 428.1948823928833
batch: 931
train_loss: 431.3670814037323
batch: 932
train_loss: 434.62182331085205
batch: 933
train_loss: 437.8450343608856
batch: 934
train_loss: 441.083669424057
batch: 935
train_loss: 444.3178918361664
batch: 936
train_loss: 447.6276259422302
batch: 937
train_loss: 450.8808958530426
batch: 938
train_loss: 454.1632215976715
batch: 939
train_loss: 457.4692542552948
batch: 940
train_loss: 460.6931138038635
batch: 941
train_loss: 463.93526458740234
batch: 942
train_loss: 467.1663055419922
batch: 943
train_loss: 470.4454095363617
batch: 944
train_loss: 473.6913697719574
batch: 945
train_loss: 476.92004108428955
batch: 946
train_loss: 480.1928508281708
batch: 947
train_loss: 483.4873592853546
batch: 948
train_loss: 486.7824673652649
batch: 949
train_loss: 489.99975848197937
batch: 950
train_loss: 493.3159456253052
batch: 951
train_loss: 496.6107439994812
batch: 952
train_loss: 499.8929817676544
batch: 953
train_loss: 503.1102900505066
batch: 954
train_loss: 506.35058331489563
batch: 955
train_loss: 509.59967947006226
batch: 956
train_loss: 512.7982728481293
batch: 957
train_loss: 516.077623128891
batch: 958
train_loss: 519.3438181877136
batch: 959
train_loss: 522.572607755661
batch: 960
train_loss: 525.7450664043427
batch: 961
train_loss: 529.0082361698151
batch: 962
train_loss: 532.2749230861664
batch: 963
train_loss: 535.6022989749908
batch: 964
train_loss: 538.9495785236359
batch: 965
train_loss: 542.3324429988861
batch: 966
train_loss: 545.676593542099
batch: 967
train_loss: 549.0014548301697
batch: 968
train_loss: 552.2571966648102
batch: 969
train_loss: 555.6932289600372
batch: 970
train_loss: 559.013610124588
batch: 971
train_loss: 562.2331118583679
batch: 972
train_loss: 565.5293183326721
batch: 973
train_loss: 568.7699418067932
batch: 974
train_loss: 572.0483915805817
batch: 975
train_loss: 575.3532574176788
batch: 976
train_loss: 578.6115338802338
batch: 977
train_loss: 581.8767926692963
batch: 978
train_loss: 585.1268789768219
batch: 979
train_loss: 588.3884453773499
batch: 980
train_loss: 591.5440216064453
batch: 981
train_loss: 594.8415842056274
batch: 982
train_loss: 598.0632586479187
batch: 983
train_loss: 601.3329424858093
batch: 984
train_loss: 604.6631877422333
batch: 985
train_loss: 607.9626729488373
batch: 986
train_loss: 611.1617176532745
batch: 987
train_loss: 614.3820192813873
batch: 988
train_loss: 617.6396899223328
batch: 989
train_loss: 620.9097549915314
batch: 990
train_loss: 624.1572554111481
batch: 991
train_loss: 627.2759265899658
batch: 992
train_loss: 630.4239087104797
batch: 993
train_loss: 633.6499330997467
batch: 994
train_loss: 636.8086099624634
batch: 995
train_loss: 640.0522518157959
batch: 996
train_loss: 643.3267304897308
batch: 997
train_loss: 646.5817124843597
batch: 998
train_loss: 649.7739396095276
batch: 999
train_loss: 653.0241434574127
| epoch   0 step     1000 |   1000 batches | lr 0.00025 | ms/batch 347.17 | loss  3.27 | bpc   4.71057
batch: 1000
train_loss: 3.261263132095337
batch: 1001
train_loss: 6.472772121429443
batch: 1002
train_loss: 9.722798109054565
batch: 1003
train_loss: 13.03593921661377
batch: 1004
train_loss: 16.25049614906311
batch: 1005
train_loss: 19.456711053848267
batch: 1006
train_loss: 22.63175082206726
batch: 1007
train_loss: 25.900251150131226
batch: 1008
train_loss: 29.171926975250244
batch: 1009
train_loss: 32.377177476882935
batch: 1010
train_loss: 35.600470781326294
batch: 1011
train_loss: 38.838953495025635
batch: 1012
train_loss: 42.104084730148315
batch: 1013
train_loss: 45.37846922874451
batch: 1014
train_loss: 48.62920784950256
batch: 1015
train_loss: 51.91963291168213
batch: 1016
train_loss: 55.03218078613281
batch: 1017
train_loss: 58.19890832901001
batch: 1018
train_loss: 61.34938907623291
batch: 1019
train_loss: 64.47449231147766
batch: 1020
train_loss: 67.58157849311829
batch: 1021
train_loss: 70.77592206001282
batch: 1022
train_loss: 73.96565747261047
batch: 1023
train_loss: 77.14805269241333
batch: 1024
train_loss: 80.39818620681763
batch: 1025
train_loss: 83.4944498538971
batch: 1026
train_loss: 86.62532353401184
batch: 1027
train_loss: 89.81916761398315
batch: 1028
train_loss: 92.97514081001282
batch: 1029
train_loss: 96.23437213897705
batch: 1030
train_loss: 99.50772261619568
batch: 1031
train_loss: 102.74158573150635
batch: 1032
train_loss: 105.93192481994629
batch: 1033
train_loss: 109.01028037071228
batch: 1034
train_loss: 112.20478892326355
batch: 1035
train_loss: 115.28286576271057
batch: 1036
train_loss: 118.41090369224548
batch: 1037
train_loss: 121.47783255577087
batch: 1038
train_loss: 124.66938853263855
batch: 1039
train_loss: 127.88380146026611
batch: 1040
train_loss: 131.02805519104004
batch: 1041
train_loss: 134.11703443527222
batch: 1042
train_loss: 137.26905226707458
batch: 1043
train_loss: 140.3883454799652
batch: 1044
train_loss: 143.53852796554565
batch: 1045
train_loss: 146.673437833786
batch: 1046
train_loss: 149.79567003250122
batch: 1047
train_loss: 152.88769507408142
batch: 1048
train_loss: 155.98390245437622
batch: 1049
train_loss: 159.09481477737427
batch: 1050
train_loss: 162.24054074287415
batch: 1051
train_loss: 165.35858368873596
batch: 1052
train_loss: 168.49596786499023
batch: 1053
train_loss: 171.57686710357666
batch: 1054
train_loss: 174.62109875679016
batch: 1055
train_loss: 177.69420075416565
batch: 1056
train_loss: 180.8236746788025
batch: 1057
train_loss: 183.9184980392456
batch: 1058
train_loss: 187.04842805862427
batch: 1059
train_loss: 190.13040113449097
batch: 1060
train_loss: 193.33502221107483
batch: 1061
train_loss: 196.48031497001648
batch: 1062
train_loss: 199.6268253326416
batch: 1063
train_loss: 202.73996233940125
batch: 1064
train_loss: 205.886559009552
batch: 1065
train_loss: 209.1097593307495
batch: 1066
train_loss: 212.2599904537201
batch: 1067
train_loss: 215.4621868133545
batch: 1068
train_loss: 218.67205023765564
batch: 1069
train_loss: 221.76271176338196
batch: 1070
train_loss: 224.91521668434143
batch: 1071
train_loss: 228.12770462036133
batch: 1072
train_loss: 231.34559178352356
batch: 1073
train_loss: 234.47131443023682
batch: 1074
train_loss: 237.64612746238708
batch: 1075
train_loss: 240.87740325927734
batch: 1076
train_loss: 244.07409977912903
batch: 1077
train_loss: 247.21623611450195
batch: 1078
train_loss: 250.32377409934998
batch: 1079
train_loss: 253.44577646255493
batch: 1080
train_loss: 256.6026177406311
batch: 1081
train_loss: 259.7884566783905
batch: 1082
train_loss: 262.92765712738037
batch: 1083
train_loss: 266.0840277671814
batch: 1084
train_loss: 269.2872543334961
batch: 1085
train_loss: 272.469605922699
batch: 1086
train_loss: 275.6041085720062
batch: 1087
train_loss: 278.7667078971863
batch: 1088
train_loss: 281.95373010635376
batch: 1089
train_loss: 285.03217339515686
batch: 1090
train_loss: 288.20186614990234
batch: 1091
train_loss: 291.30645847320557
batch: 1092
train_loss: 294.46196365356445
batch: 1093
train_loss: 297.5962827205658
batch: 1094
train_loss: 300.738489151001
batch: 1095
train_loss: 303.8974988460541
batch: 1096
train_loss: 307.0044858455658
batch: 1097
train_loss: 310.1964621543884
batch: 1098
train_loss: 313.34631872177124
batch: 1099
train_loss: 316.44625782966614
batch: 1100
train_loss: 319.62961292266846
batch: 1101
train_loss: 322.8518443107605
batch: 1102
train_loss: 325.9404637813568
batch: 1103
train_loss: 329.018944978714
batch: 1104
train_loss: 332.1904306411743
batch: 1105
train_loss: 335.346364736557
batch: 1106
train_loss: 338.41714882850647
batch: 1107
train_loss: 341.49650049209595
batch: 1108
train_loss: 344.57986664772034
batch: 1109
train_loss: 347.7371060848236
batch: 1110
train_loss: 350.86867570877075
batch: 1111
train_loss: 353.94909858703613
batch: 1112
train_loss: 357.05921506881714
batch: 1113
train_loss: 360.2200357913971
batch: 1114
train_loss: 363.2853624820709
batch: 1115
train_loss: 366.44851565361023
batch: 1116
train_loss: 369.5623712539673
batch: 1117
train_loss: 372.60870814323425
batch: 1118
train_loss: 375.7142732143402
batch: 1119
train_loss: 378.8447570800781
batch: 1120
train_loss: 381.9387412071228
batch: 1121
train_loss: 385.120721578598
batch: 1122
train_loss: 388.3049838542938
batch: 1123
train_loss: 391.42193698883057
batch: 1124
train_loss: 394.61781001091003
batch: 1125
train_loss: 397.81236696243286
batch: 1126
train_loss: 401.0122835636139
batch: 1127
train_loss: 404.19116830825806
batch: 1128
train_loss: 407.36597323417664
batch: 1129
train_loss: 410.5502345561981
batch: 1130
train_loss: 413.72761392593384
batch: 1131
train_loss: 416.8796081542969
batch: 1132
train_loss: 420.05438327789307
batch: 1133
train_loss: 423.17539978027344
batch: 1134
train_loss: 426.3516454696655
batch: 1135
train_loss: 429.4551246166229
batch: 1136
train_loss: 432.64181423187256
batch: 1137
train_loss: 435.8595771789551
batch: 1138
train_loss: 438.96268010139465
batch: 1139
train_loss: 442.1266050338745
batch: 1140
train_loss: 445.2580654621124
batch: 1141
train_loss: 448.3032503128052
batch: 1142
train_loss: 451.51101636886597
batch: 1143
train_loss: 454.6380383968353
batch: 1144
train_loss: 457.65992045402527
batch: 1145
train_loss: 460.73956274986267
batch: 1146
train_loss: 463.8326098918915
batch: 1147
train_loss: 466.9613950252533
batch: 1148
train_loss: 470.0265803337097
batch: 1149
train_loss: 473.12927436828613
batch: 1150
train_loss: 476.20477318763733
batch: 1151
train_loss: 479.2815783023834
batch: 1152
train_loss: 482.37885308265686
batch: 1153
train_loss: 485.48172879219055
batch: 1154
train_loss: 488.6580982208252
batch: 1155
train_loss: 491.82419872283936
batch: 1156
train_loss: 494.9253125190735
batch: 1157
train_loss: 498.0163152217865
batch: 1158
train_loss: 501.1192693710327
batch: 1159
train_loss: 504.2910680770874
batch: 1160
train_loss: 507.3403387069702
batch: 1161
train_loss: 510.487580537796
batch: 1162
train_loss: 513.554639339447
batch: 1163
train_loss: 516.7019600868225
batch: 1164
train_loss: 519.8373148441315
batch: 1165
train_loss: 522.9064204692841
batch: 1166
train_loss: 526.0766940116882
batch: 1167
train_loss: 529.2491679191589
batch: 1168
train_loss: 532.4195041656494
batch: 1169
train_loss: 535.5852575302124
batch: 1170
train_loss: 538.7022352218628
batch: 1171
train_loss: 541.8688018321991
batch: 1172
train_loss: 545.00932097435
batch: 1173
train_loss: 548.2268962860107
batch: 1174
train_loss: 551.4386129379272
batch: 1175
train_loss: 554.6732864379883
batch: 1176
train_loss: 557.9077062606812
batch: 1177
train_loss: 561.0206801891327
batch: 1178
train_loss: 564.1623780727386
batch: 1179
train_loss: 567.3179678916931
batch: 1180
train_loss: 570.3924612998962
batch: 1181
train_loss: 573.5741531848907
batch: 1182
train_loss: 576.7132675647736
batch: 1183
train_loss: 579.8296160697937
batch: 1184
train_loss: 582.9004445075989
batch: 1185
train_loss: 586.0325186252594
batch: 1186
train_loss: 589.2269377708435
batch: 1187
train_loss: 592.3835651874542
batch: 1188
train_loss: 595.5384266376495
batch: 1189
train_loss: 598.7306249141693
batch: 1190
train_loss: 601.9007091522217
batch: 1191
train_loss: 605.0679974555969
batch: 1192
train_loss: 608.2790987491608
batch: 1193
train_loss: 611.3910417556763
batch: 1194
train_loss: 614.5424928665161
batch: 1195
train_loss: 617.6837341785431
batch: 1196
train_loss: 620.8842575550079
batch: 1197
train_loss: 624.0767257213593
batch: 1198
train_loss: 627.2908868789673
batch: 1199
train_loss: 630.5544259548187
| epoch   0 step     1200 |   1200 batches | lr 0.000249 | ms/batch 340.25 | loss  3.15 | bpc   4.54849
batch: 1200
train_loss: 3.274017095565796
batch: 1201
train_loss: 6.575123310089111
batch: 1202
train_loss: 9.973546504974365
batch: 1203
train_loss: 13.220239877700806
batch: 1204
train_loss: 16.551105260849
batch: 1205
train_loss: 19.76380228996277
batch: 1206
train_loss: 22.862032413482666
batch: 1207
train_loss: 26.019532203674316
batch: 1208
train_loss: 29.184446811676025
batch: 1209
train_loss: 32.40557646751404
batch: 1210
train_loss: 35.55728220939636
batch: 1211
train_loss: 38.83952331542969
batch: 1212
train_loss: 42.03355050086975
batch: 1213
train_loss: 45.26119947433472
batch: 1214
train_loss: 48.47211813926697
batch: 1215
train_loss: 51.72061514854431
batch: 1216
train_loss: 54.95125651359558
batch: 1217
train_loss: 58.109015464782715
batch: 1218
train_loss: 61.259740352630615
batch: 1219
train_loss: 64.3479254245758
batch: 1220
train_loss: 67.39285516738892
batch: 1221
train_loss: 70.4432623386383
batch: 1222
train_loss: 73.57833933830261
batch: 1223
train_loss: 76.7099027633667
batch: 1224
train_loss: 79.71965098381042
batch: 1225
train_loss: 82.82753348350525
batch: 1226
train_loss: 85.91629076004028
batch: 1227
train_loss: 89.03490352630615
batch: 1228
train_loss: 92.12859869003296
batch: 1229
train_loss: 95.28766679763794
batch: 1230
train_loss: 98.49959444999695
batch: 1231
train_loss: 101.65762066841125
batch: 1232
train_loss: 104.76426291465759
batch: 1233
train_loss: 107.94459509849548
batch: 1234
train_loss: 111.11487317085266
batch: 1235
train_loss: 114.20571041107178
batch: 1236
train_loss: 117.33566808700562
batch: 1237
train_loss: 120.50989270210266
batch: 1238
train_loss: 123.63256335258484
batch: 1239
train_loss: 126.7984836101532
batch: 1240
train_loss: 129.98515939712524
batch: 1241
train_loss: 133.14646410942078
batch: 1242
train_loss: 136.3673357963562
batch: 1243
train_loss: 139.50384616851807
batch: 1244
train_loss: 142.53794646263123
batch: 1245
train_loss: 145.71333026885986
batch: 1246
train_loss: 148.88127779960632
batch: 1247
train_loss: 151.96448373794556
batch: 1248
train_loss: 155.07662439346313
batch: 1249
train_loss: 158.23362565040588
batch: 1250
train_loss: 161.31788325309753
batch: 1251
train_loss: 164.37911891937256
batch: 1252
train_loss: 167.51931738853455
batch: 1253
train_loss: 170.66776037216187
batch: 1254
train_loss: 173.82830429077148
batch: 1255
train_loss: 177.0098831653595
batch: 1256
train_loss: 180.14424896240234
batch: 1257
train_loss: 183.26536560058594
batch: 1258
train_loss: 186.40631484985352
batch: 1259
train_loss: 189.61226058006287
batch: 1260
train_loss: 192.74754881858826
batch: 1261
train_loss: 195.89660358428955
batch: 1262
train_loss: 199.00588989257812
batch: 1263
train_loss: 202.15574645996094
batch: 1264
train_loss: 205.27722573280334
batch: 1265
train_loss: 208.44800353050232
batch: 1266
train_loss: 211.55653619766235
batch: 1267
train_loss: 214.6692976951599
batch: 1268
train_loss: 217.79809021949768
batch: 1269
train_loss: 220.96468710899353
batch: 1270
train_loss: 224.13493847846985
batch: 1271
train_loss: 227.22098755836487
batch: 1272
train_loss: 230.341641664505
batch: 1273
train_loss: 233.44334387779236
batch: 1274
train_loss: 236.51110911369324
batch: 1275
train_loss: 239.59991931915283
batch: 1276
train_loss: 242.7487452030182
batch: 1277
train_loss: 245.85781145095825
batch: 1278
train_loss: 248.95132398605347
batch: 1279
train_loss: 252.04223227500916
batch: 1280
train_loss: 255.13318157196045
batch: 1281
train_loss: 258.31254386901855
batch: 1282
train_loss: 261.4448757171631
batch: 1283
train_loss: 264.6163387298584
batch: 1284
train_loss: 267.7137851715088
batch: 1285
train_loss: 270.8933117389679
batch: 1286
train_loss: 273.96023511886597
batch: 1287
train_loss: 277.1269037723541
batch: 1288
train_loss: 280.2727153301239
batch: 1289
train_loss: 283.37572264671326
batch: 1290
train_loss: 286.5297381877899
batch: 1291
train_loss: 289.66677832603455
batch: 1292
train_loss: 292.7352590560913
batch: 1293
train_loss: 295.8432722091675
batch: 1294
train_loss: 298.8843123912811
batch: 1295
train_loss: 302.05081033706665
batch: 1296
train_loss: 305.2216818332672
batch: 1297
train_loss: 308.3597631454468
batch: 1298
train_loss: 311.51346707344055
batch: 1299
train_loss: 314.65083718299866
batch: 1300
train_loss: 317.7843997478485
batch: 1301
train_loss: 320.994713306427
batch: 1302
train_loss: 324.1325521469116
batch: 1303
train_loss: 327.24990010261536
batch: 1304
train_loss: 330.35142040252686
batch: 1305
train_loss: 333.54481387138367
batch: 1306
train_loss: 336.6668689250946
batch: 1307
train_loss: 339.7945384979248
batch: 1308
train_loss: 342.84975600242615
batch: 1309
train_loss: 345.94641947746277
batch: 1310
train_loss: 349.10762643814087
batch: 1311
train_loss: 352.20886063575745
batch: 1312
train_loss: 355.338018655777
batch: 1313
train_loss: 358.4338231086731
batch: 1314
train_loss: 361.4944484233856
batch: 1315
train_loss: 364.6269814968109
batch: 1316
train_loss: 367.720006942749
batch: 1317
train_loss: 370.8102388381958
batch: 1318
train_loss: 373.95277190208435
batch: 1319
train_loss: 377.04122734069824
batch: 1320
train_loss: 380.10942816734314
batch: 1321
train_loss: 383.23007249832153
batch: 1322
train_loss: 386.34425711631775
batch: 1323
train_loss: 389.42768812179565
batch: 1324
train_loss: 392.4360547065735
batch: 1325
train_loss: 395.5867636203766
batch: 1326
train_loss: 398.7762379646301
batch: 1327
train_loss: 401.90548276901245
batch: 1328
train_loss: 404.9970483779907
batch: 1329
train_loss: 408.0680389404297
batch: 1330
train_loss: 411.12615609169006
batch: 1331
train_loss: 414.17612648010254
batch: 1332
train_loss: 417.2350261211395
batch: 1333
train_loss: 420.3487911224365
batch: 1334
train_loss: 423.40448927879333
batch: 1335
train_loss: 426.4176173210144
batch: 1336
train_loss: 429.5237452983856
batch: 1337
train_loss: 432.5539638996124
batch: 1338
train_loss: 435.6092083454132
batch: 1339
train_loss: 438.6749179363251
batch: 1340
train_loss: 441.7571949958801
batch: 1341
train_loss: 444.7141492366791
batch: 1342
train_loss: 447.7685923576355
batch: 1343
train_loss: 450.8787479400635
batch: 1344
train_loss: 453.9433217048645
batch: 1345
train_loss: 457.0745961666107
batch: 1346
train_loss: 460.04479718208313
batch: 1347
train_loss: 463.01349782943726
batch: 1348
train_loss: 466.04400873184204
batch: 1349
train_loss: 469.08414936065674
batch: 1350
train_loss: 472.0453591346741
batch: 1351
train_loss: 475.07049775123596
batch: 1352
train_loss: 478.09155559539795
batch: 1353
train_loss: 481.1120936870575
batch: 1354
train_loss: 484.1467807292938
batch: 1355
train_loss: 487.1710546016693
batch: 1356
train_loss: 490.1246199607849
batch: 1357
train_loss: 493.1631143093109
batch: 1358
train_loss: 496.12780261039734
batch: 1359
train_loss: 499.1372957229614
batch: 1360
train_loss: 502.15787506103516
batch: 1361
train_loss: 505.152423620224
batch: 1362
train_loss: 508.18695425987244
batch: 1363
train_loss: 511.2409439086914
batch: 1364
train_loss: 514.2482206821442
batch: 1365
train_loss: 517.3355002403259
batch: 1366
train_loss: 520.4084815979004
batch: 1367
train_loss: 523.3891108036041
batch: 1368
train_loss: 526.5018434524536
batch: 1369
train_loss: 529.5438239574432
batch: 1370
train_loss: 532.5843222141266
batch: 1371
train_loss: 535.639618396759
batch: 1372
train_loss: 538.7478544712067
batch: 1373
train_loss: 541.8064818382263
batch: 1374
train_loss: 544.8589751720428
batch: 1375
train_loss: 547.9063539505005
batch: 1376
train_loss: 550.9878907203674
batch: 1377
train_loss: 553.9336593151093
batch: 1378
train_loss: 556.9739217758179
batch: 1379
train_loss: 560.0121121406555
batch: 1380
train_loss: 562.9506099224091
batch: 1381
train_loss: 565.9210872650146
batch: 1382
train_loss: 568.913229227066
batch: 1383
train_loss: 571.8738312721252
batch: 1384
train_loss: 574.9299912452698
batch: 1385
train_loss: 577.9917497634888
batch: 1386
train_loss: 581.0351097583771
batch: 1387
train_loss: 584.0450537204742
batch: 1388
train_loss: 587.0275032520294
batch: 1389
train_loss: 590.0227286815643
batch: 1390
train_loss: 592.9467332363129
batch: 1391
train_loss: 595.9305400848389
batch: 1392
train_loss: 598.8866221904755
batch: 1393
train_loss: 601.8852689266205
batch: 1394
train_loss: 604.8555302619934
batch: 1395
train_loss: 607.9185664653778
batch: 1396
train_loss: 610.9572911262512
batch: 1397
train_loss: 614.0387179851532
batch: 1398
train_loss: 617.0804581642151
batch: 1399
train_loss: 620.1945993900299
| epoch   0 step     1400 |   1400 batches | lr 0.000249 | ms/batch 336.91 | loss  3.10 | bpc   4.47376
batch: 1400
train_loss: 3.093308210372925
batch: 1401
train_loss: 6.0768373012542725
batch: 1402
train_loss: 9.070416927337646
batch: 1403
train_loss: 12.081968784332275
batch: 1404
train_loss: 15.147039890289307
batch: 1405
train_loss: 18.171241283416748
batch: 1406
train_loss: 21.197062253952026
batch: 1407
train_loss: 24.268195629119873
batch: 1408
train_loss: 27.299579858779907
batch: 1409
train_loss: 30.35111713409424
batch: 1410
train_loss: 33.44290637969971
batch: 1411
train_loss: 36.47827649116516
batch: 1412
train_loss: 39.44429612159729
batch: 1413
train_loss: 42.485029220581055
batch: 1414
train_loss: 45.52708077430725
batch: 1415
train_loss: 48.56663990020752
batch: 1416
train_loss: 51.61057901382446
batch: 1417
train_loss: 54.60375452041626
batch: 1418
train_loss: 57.693049907684326
batch: 1419
train_loss: 60.70949673652649
batch: 1420
train_loss: 63.79448413848877
batch: 1421
train_loss: 66.9147698879242
batch: 1422
train_loss: 69.9420394897461
batch: 1423
train_loss: 72.94266366958618
batch: 1424
train_loss: 75.92785429954529
batch: 1425
train_loss: 78.95871424674988
batch: 1426
train_loss: 81.98616194725037
batch: 1427
train_loss: 84.974618434906
batch: 1428
train_loss: 87.98313307762146
batch: 1429
train_loss: 91.0013337135315
batch: 1430
train_loss: 93.99161982536316
batch: 1431
train_loss: 97.05982041358948
batch: 1432
train_loss: 100.0306830406189
batch: 1433
train_loss: 103.12895894050598
batch: 1434
train_loss: 106.21131134033203
batch: 1435
train_loss: 109.33631682395935
batch: 1436
train_loss: 112.45055317878723
batch: 1437
train_loss: 115.52091145515442
batch: 1438
train_loss: 118.63180637359619
batch: 1439
train_loss: 121.68791580200195
batch: 1440
train_loss: 124.67644047737122
batch: 1441
train_loss: 127.67548656463623
batch: 1442
train_loss: 130.65402960777283
batch: 1443
train_loss: 133.6301212310791
batch: 1444
train_loss: 136.67670702934265
batch: 1445
train_loss: 139.77809500694275
batch: 1446
train_loss: 142.9134397506714
batch: 1447
train_loss: 145.92998027801514
batch: 1448
train_loss: 149.03013730049133
batch: 1449
train_loss: 152.0684688091278
batch: 1450
train_loss: 154.98205471038818
batch: 1451
train_loss: 157.98258662223816
batch: 1452
train_loss: 161.00573563575745
batch: 1453
train_loss: 164.02487707138062
batch: 1454
train_loss: 166.98209595680237
batch: 1455
train_loss: 169.8824028968811
batch: 1456
train_loss: 172.86689019203186
batch: 1457
train_loss: 175.86273384094238
batch: 1458
train_loss: 178.78726291656494
batch: 1459
train_loss: 181.74063301086426
batch: 1460
train_loss: 184.66075682640076
batch: 1461
train_loss: 187.58886814117432
batch: 1462
train_loss: 190.4996337890625
batch: 1463
train_loss: 193.4442856311798
batch: 1464
train_loss: 196.40557384490967
batch: 1465
train_loss: 199.3298225402832
batch: 1466
train_loss: 202.2609100341797
batch: 1467
train_loss: 205.21090626716614
batch: 1468
train_loss: 208.13814449310303
batch: 1469
train_loss: 211.0810616016388
batch: 1470
train_loss: 213.99035954475403
batch: 1471
train_loss: 216.8896598815918
batch: 1472
train_loss: 219.83172988891602
batch: 1473
train_loss: 222.74583959579468
batch: 1474
train_loss: 225.7104721069336
batch: 1475
train_loss: 228.6567723751068
batch: 1476
train_loss: 231.66594982147217
batch: 1477
train_loss: 234.62293148040771
batch: 1478
train_loss: 237.62869381904602
batch: 1479
train_loss: 240.54543662071228
batch: 1480
train_loss: 243.49198722839355
batch: 1481
train_loss: 246.40868282318115
batch: 1482
train_loss: 249.4010648727417
batch: 1483
train_loss: 252.35386037826538
batch: 1484
train_loss: 255.2602915763855
batch: 1485
train_loss: 258.23126196861267
batch: 1486
train_loss: 261.1186363697052
batch: 1487
train_loss: 264.0677568912506
batch: 1488
train_loss: 267.0029139518738
batch: 1489
train_loss: 269.92173194885254
batch: 1490
train_loss: 272.8882417678833
batch: 1491
train_loss: 275.92912769317627
batch: 1492
train_loss: 278.8621265888214
batch: 1493
train_loss: 281.8462088108063
batch: 1494
train_loss: 284.89302277565
batch: 1495
train_loss: 287.8556418418884
batch: 1496
train_loss: 290.78631067276
batch: 1497
train_loss: 293.6895716190338
batch: 1498
train_loss: 296.5809862613678
batch: 1499
train_loss: 299.52633452415466
batch: 1500
train_loss: 302.48433542251587
batch: 1501
train_loss: 305.5347352027893
batch: 1502
train_loss: 308.4872958660126
batch: 1503
train_loss: 311.389799118042
batch: 1504
train_loss: 314.3247404098511
batch: 1505
train_loss: 317.2973418235779
batch: 1506
train_loss: 320.3495557308197
batch: 1507
train_loss: 323.33887243270874
batch: 1508
train_loss: 326.35785031318665
batch: 1509
train_loss: 329.4744076728821
batch: 1510
train_loss: 332.5311884880066
batch: 1511
train_loss: 335.56469678878784
batch: 1512
train_loss: 338.54323530197144
batch: 1513
train_loss: 341.4622731208801
batch: 1514
train_loss: 344.420450925827
batch: 1515
train_loss: 347.3428101539612
batch: 1516
train_loss: 350.32355189323425
batch: 1517
train_loss: 353.3042197227478
batch: 1518
train_loss: 356.2965302467346
batch: 1519
train_loss: 359.27707147598267
batch: 1520
train_loss: 362.21380496025085
batch: 1521
train_loss: 365.1590647697449
batch: 1522
train_loss: 368.05784583091736
batch: 1523
train_loss: 370.99986457824707
batch: 1524
train_loss: 373.9829981327057
batch: 1525
train_loss: 376.99028515815735
batch: 1526
train_loss: 379.9818186759949
batch: 1527
train_loss: 382.9599792957306
batch: 1528
train_loss: 385.9619016647339
batch: 1529
train_loss: 388.9742794036865
batch: 1530
train_loss: 391.9224109649658
batch: 1531
train_loss: 394.90583896636963
batch: 1532
train_loss: 397.87653160095215
batch: 1533
train_loss: 400.8690571784973
batch: 1534
train_loss: 403.8612334728241
batch: 1535
train_loss: 406.8425018787384
batch: 1536
train_loss: 409.85612630844116
batch: 1537
train_loss: 412.807555437088
batch: 1538
train_loss: 415.7277822494507
batch: 1539
train_loss: 418.7745008468628
batch: 1540
train_loss: 421.72779154777527
batch: 1541
train_loss: 424.7794830799103
batch: 1542
train_loss: 427.78099727630615
batch: 1543
train_loss: 430.72482419013977
batch: 1544
train_loss: 433.68541383743286
batch: 1545
train_loss: 436.67094922065735
batch: 1546
train_loss: 439.6898798942566
batch: 1547
train_loss: 442.62030720710754
batch: 1548
train_loss: 445.63862133026123
batch: 1549
train_loss: 448.6414439678192
batch: 1550
train_loss: 451.72693276405334
batch: 1551
train_loss: 454.6627633571625
batch: 1552
train_loss: 457.7730941772461
batch: 1553
train_loss: 460.87663745880127
batch: 1554
train_loss: 464.0042896270752
batch: 1555
train_loss: 467.16579413414
batch: 1556
train_loss: 470.16017270088196
batch: 1557
train_loss: 473.1157820224762
batch: 1558
train_loss: 476.0578649044037
batch: 1559
train_loss: 479.05668449401855
batch: 1560
train_loss: 482.097519159317
batch: 1561
train_loss: 485.0760078430176
batch: 1562
train_loss: 488.02393102645874
batch: 1563
train_loss: 491.05112528800964
batch: 1564
train_loss: 494.00566697120667
batch: 1565
train_loss: 496.94904828071594
batch: 1566
train_loss: 499.9250285625458
batch: 1567
train_loss: 502.931254863739
batch: 1568
train_loss: 505.96030473709106
batch: 1569
train_loss: 509.04129695892334
batch: 1570
train_loss: 512.0510694980621
batch: 1571
train_loss: 515.0540747642517
batch: 1572
train_loss: 518.0309827327728
batch: 1573
train_loss: 520.9276549816132
batch: 1574
train_loss: 523.8386151790619
batch: 1575
train_loss: 526.7741377353668
batch: 1576
train_loss: 529.7070565223694
batch: 1577
train_loss: 532.656231880188
batch: 1578
train_loss: 535.609959602356
batch: 1579
train_loss: 538.5908150672913
batch: 1580
train_loss: 541.5549759864807
batch: 1581
train_loss: 544.5422141551971
batch: 1582
train_loss: 547.4712798595428
batch: 1583
train_loss: 550.4056587219238
batch: 1584
train_loss: 553.331597328186
batch: 1585
train_loss: 556.2797410488129
batch: 1586
train_loss: 559.1618938446045
batch: 1587
train_loss: 562.1656274795532
batch: 1588
train_loss: 565.1719601154327
batch: 1589
train_loss: 568.064740896225
batch: 1590
train_loss: 571.0335144996643
batch: 1591
train_loss: 574.0315048694611
batch: 1592
train_loss: 576.8894972801208
batch: 1593
train_loss: 579.8087079524994
batch: 1594
train_loss: 582.8075950145721
batch: 1595
train_loss: 585.7220430374146
batch: 1596
train_loss: 588.6281094551086
batch: 1597
train_loss: 591.5838963985443
batch: 1598
train_loss: 594.5638899803162
batch: 1599
train_loss: 597.434867143631
| epoch   0 step     1600 |   1600 batches | lr 0.000249 | ms/batch 335.49 | loss  2.99 | bpc   4.30958
batch: 1600
train_loss: 2.9112398624420166
batch: 1601
train_loss: 5.819781064987183
batch: 1602
train_loss: 8.736354351043701
batch: 1603
train_loss: 11.687707901000977
batch: 1604
train_loss: 14.635309219360352
batch: 1605
train_loss: 17.61271572113037
batch: 1606
train_loss: 20.55033850669861
batch: 1607
train_loss: 23.518346071243286
batch: 1608
train_loss: 26.489338397979736
batch: 1609
train_loss: 29.409411191940308
batch: 1610
train_loss: 32.31373476982117
batch: 1611
train_loss: 35.316224336624146
batch: 1612
train_loss: 38.28242206573486
batch: 1613
train_loss: 41.36434364318848
batch: 1614
train_loss: 44.39576506614685
batch: 1615
train_loss: 47.33803343772888
batch: 1616
train_loss: 50.29652786254883
batch: 1617
train_loss: 53.27441883087158
batch: 1618
train_loss: 56.24704694747925
batch: 1619
train_loss: 59.21331262588501
batch: 1620
train_loss: 62.187504529953
batch: 1621
train_loss: 65.24995851516724
batch: 1622
train_loss: 68.25056147575378
batch: 1623
train_loss: 71.22064065933228
batch: 1624
train_loss: 74.229159116745
batch: 1625
train_loss: 77.22640347480774
batch: 1626
train_loss: 80.16501212120056
batch: 1627
train_loss: 83.19409465789795
batch: 1628
train_loss: 86.25847887992859
batch: 1629
train_loss: 89.3131012916565
batch: 1630
train_loss: 92.35773181915283
batch: 1631
train_loss: 95.42924976348877
batch: 1632
train_loss: 98.4977457523346
batch: 1633
train_loss: 101.57248401641846
batch: 1634
train_loss: 104.65499663352966
batch: 1635
train_loss: 107.76907062530518
batch: 1636
train_loss: 110.85723304748535
batch: 1637
train_loss: 113.96414232254028
batch: 1638
train_loss: 117.09378695487976
batch: 1639
train_loss: 120.13472557067871
batch: 1640
train_loss: 123.18103885650635
batch: 1641
train_loss: 126.26795887947083
batch: 1642
train_loss: 129.30103754997253
batch: 1643
train_loss: 132.26533341407776
batch: 1644
train_loss: 135.31770157814026
batch: 1645
train_loss: 138.2950382232666
batch: 1646
train_loss: 141.26578783988953
batch: 1647
train_loss: 144.20150804519653
batch: 1648
train_loss: 147.0876109600067
batch: 1649
train_loss: 149.98221158981323
batch: 1650
train_loss: 152.95061445236206
batch: 1651
train_loss: 155.8287422657013
batch: 1652
train_loss: 158.73212313652039
batch: 1653
train_loss: 161.75286602973938
batch: 1654
train_loss: 164.64139652252197
batch: 1655
train_loss: 167.56627011299133
batch: 1656
train_loss: 170.5448088645935
batch: 1657
train_loss: 173.48630499839783
batch: 1658
train_loss: 176.38932466506958
batch: 1659
train_loss: 179.3692343235016
batch: 1660
train_loss: 182.34557723999023
batch: 1661
train_loss: 185.29790902137756
batch: 1662
train_loss: 188.20928239822388
batch: 1663
train_loss: 191.19827461242676
batch: 1664
train_loss: 194.1006224155426
batch: 1665
train_loss: 197.00847816467285
batch: 1666
train_loss: 199.85903453826904
batch: 1667
train_loss: 202.74308514595032
batch: 1668
train_loss: 205.61591386795044
batch: 1669
train_loss: 208.5119912624359
batch: 1670
train_loss: 211.42038989067078
batch: 1671
train_loss: 214.35194730758667
batch: 1672
train_loss: 217.26213788986206
batch: 1673
train_loss: 220.17431449890137
batch: 1674
train_loss: 223.06851720809937
batch: 1675
train_loss: 226.02956914901733
batch: 1676
train_loss: 228.97187852859497
batch: 1677
train_loss: 231.84196424484253
batch: 1678
train_loss: 234.76385045051575
batch: 1679
train_loss: 237.7162697315216
batch: 1680
train_loss: 240.6583127975464
batch: 1681
train_loss: 243.52666592597961
batch: 1682
train_loss: 246.38164043426514
batch: 1683
train_loss: 249.40735507011414
batch: 1684
train_loss: 252.3352837562561
batch: 1685
train_loss: 255.2270154953003
batch: 1686
train_loss: 258.05507159233093
batch: 1687
train_loss: 260.9402139186859
batch: 1688
train_loss: 263.86137652397156
batch: 1689
train_loss: 266.86880230903625
batch: 1690
train_loss: 269.7798852920532
batch: 1691
train_loss: 272.7962396144867
batch: 1692
train_loss: 275.7797267436981
batch: 1693
train_loss: 278.6948175430298
batch: 1694
train_loss: 281.6544780731201
batch: 1695
train_loss: 284.50105381011963
batch: 1696
train_loss: 287.4127604961395
batch: 1697
train_loss: 290.4295132160187
batch: 1698
train_loss: 293.48144125938416
batch: 1699
train_loss: 296.47317576408386
batch: 1700
train_loss: 299.40118074417114
batch: 1701
train_loss: 302.30015802383423
batch: 1702
train_loss: 305.24926376342773
batch: 1703
train_loss: 308.2010614871979
batch: 1704
train_loss: 311.14242577552795
batch: 1705
train_loss: 314.07924699783325
batch: 1706
train_loss: 317.08326411247253
batch: 1707
train_loss: 320.0897903442383
batch: 1708
train_loss: 323.0424659252167
batch: 1709
train_loss: 326.04028367996216
batch: 1710
train_loss: 329.0050666332245
batch: 1711
train_loss: 331.92948150634766
batch: 1712
train_loss: 334.9194915294647
batch: 1713
train_loss: 337.80463123321533
batch: 1714
train_loss: 340.7544014453888
batch: 1715
train_loss: 343.6984121799469
batch: 1716
train_loss: 346.67595887184143
batch: 1717
train_loss: 349.6421654224396
batch: 1718
train_loss: 352.59555888175964
batch: 1719
train_loss: 355.5538046360016
batch: 1720
train_loss: 358.52463269233704
batch: 1721
train_loss: 361.4731388092041
batch: 1722
train_loss: 364.38288712501526
batch: 1723
train_loss: 367.2868344783783
batch: 1724
train_loss: 370.15100932121277
batch: 1725
train_loss: 373.14699625968933
batch: 1726
train_loss: 376.0797348022461
batch: 1727
train_loss: 378.9777834415436
batch: 1728
train_loss: 381.9188721179962
batch: 1729
train_loss: 384.8459162712097
batch: 1730
train_loss: 387.7715427875519
batch: 1731
train_loss: 390.69789719581604
batch: 1732
train_loss: 393.67309308052063
batch: 1733
train_loss: 396.6006534099579
batch: 1734
train_loss: 399.6290783882141
batch: 1735
train_loss: 402.60143542289734
batch: 1736
train_loss: 405.5198199748993
batch: 1737
train_loss: 408.47739148139954
batch: 1738
train_loss: 411.4306712150574
batch: 1739
train_loss: 414.3528254032135
batch: 1740
train_loss: 417.2939877510071
batch: 1741
train_loss: 420.1509540081024
batch: 1742
train_loss: 423.11738538742065
batch: 1743
train_loss: 425.9967772960663
batch: 1744
train_loss: 428.91986775398254
batch: 1745
train_loss: 431.7866153717041
batch: 1746
train_loss: 434.6966059207916
batch: 1747
train_loss: 437.55440402030945
batch: 1748
train_loss: 440.40989995002747
batch: 1749
train_loss: 443.3043484687805
batch: 1750
train_loss: 446.2763125896454
batch: 1751
train_loss: 449.22539472579956
batch: 1752
train_loss: 452.1343340873718
batch: 1753
train_loss: 455.09178018569946
batch: 1754
train_loss: 458.0227379798889
batch: 1755
train_loss: 460.9349069595337
batch: 1756
train_loss: 463.8233723640442
batch: 1757
train_loss: 466.79797649383545
batch: 1758
train_loss: 469.76081132888794
batch: 1759
train_loss: 472.7401030063629
batch: 1760
train_loss: 475.66486859321594
batch: 1761
train_loss: 478.6791431903839
batch: 1762
train_loss: 481.7056269645691
batch: 1763
train_loss: 484.7965602874756
batch: 1764
train_loss: 487.9723470211029
batch: 1765
train_loss: 491.03001379966736
batch: 1766
train_loss: 494.0475392341614
batch: 1767
train_loss: 496.98874497413635
batch: 1768
train_loss: 499.9924328327179
batch: 1769
train_loss: 503.00350522994995
batch: 1770
train_loss: 506.0133912563324
batch: 1771
train_loss: 509.03299498558044
batch: 1772
train_loss: 511.98553347587585
batch: 1773
train_loss: 515.0429513454437
batch: 1774
train_loss: 517.9545907974243
batch: 1775
train_loss: 520.8848674297333
batch: 1776
train_loss: 523.8475484848022
batch: 1777
train_loss: 526.7983770370483
batch: 1778
train_loss: 529.8122699260712
batch: 1779
train_loss: 532.7468202114105
batch: 1780
train_loss: 535.7256808280945
batch: 1781
train_loss: 538.6843190193176
batch: 1782
train_loss: 541.5991430282593
batch: 1783
train_loss: 544.5172946453094
batch: 1784
train_loss: 547.3836364746094
batch: 1785
train_loss: 550.2516067028046
batch: 1786
train_loss: 553.1871619224548
batch: 1787
train_loss: 556.0980536937714
batch: 1788
train_loss: 559.1457962989807
batch: 1789
train_loss: 562.0416071414948
batch: 1790
train_loss: 564.9555444717407
batch: 1791
train_loss: 567.867604970932
batch: 1792
train_loss: 570.8302848339081
batch: 1793
train_loss: 573.7369203567505
batch: 1794
train_loss: 576.7704012393951
batch: 1795
train_loss: 579.5849239826202
batch: 1796
train_loss: 582.6080000400543
batch: 1797
train_loss: 585.6413731575012
batch: 1798
train_loss: 588.5784652233124
batch: 1799
train_loss: 591.5460133552551
| epoch   0 step     1800 |   1800 batches | lr 0.000249 | ms/batch 335.63 | loss  2.96 | bpc   4.26710
batch: 1800
train_loss: 3.0599477291107178
batch: 1801
train_loss: 6.030585289001465
batch: 1802
train_loss: 9.086836814880371
batch: 1803
train_loss: 12.189417839050293
batch: 1804
train_loss: 15.253907680511475
batch: 1805
train_loss: 18.361510276794434
batch: 1806
train_loss: 21.39731502532959
batch: 1807
train_loss: 24.364439249038696
batch: 1808
train_loss: 27.256296634674072
batch: 1809
train_loss: 30.21599578857422
batch: 1810
train_loss: 33.354344844818115
batch: 1811
train_loss: 36.27215051651001
batch: 1812
train_loss: 39.201722860336304
batch: 1813
train_loss: 42.21745562553406
batch: 1814
train_loss: 45.09024953842163
batch: 1815
train_loss: 47.951409578323364
batch: 1816
train_loss: 50.81509232521057
batch: 1817
train_loss: 53.67591094970703
batch: 1818
train_loss: 56.547022104263306
batch: 1819
train_loss: 59.4661979675293
batch: 1820
train_loss: 62.426708936691284
batch: 1821
train_loss: 65.33420658111572
batch: 1822
train_loss: 68.2856662273407
batch: 1823
train_loss: 71.1780652999878
batch: 1824
train_loss: 74.0702006816864
batch: 1825
train_loss: 76.94957828521729
batch: 1826
train_loss: 79.82835364341736
batch: 1827
train_loss: 82.67539858818054
batch: 1828
train_loss: 85.5375006198883
batch: 1829
train_loss: 88.40360379219055
batch: 1830
train_loss: 91.2898256778717
batch: 1831
train_loss: 94.19339680671692
batch: 1832
train_loss: 97.15440392494202
batch: 1833
train_loss: 99.95418190956116
batch: 1834
train_loss: 102.75512981414795
batch: 1835
train_loss: 105.63648796081543
batch: 1836
train_loss: 108.5531485080719
batch: 1837
train_loss: 111.37291836738586
batch: 1838
train_loss: 114.1742959022522
batch: 1839
train_loss: 117.08619785308838
batch: 1840
train_loss: 119.92111134529114
batch: 1841
train_loss: 122.80610418319702
batch: 1842
train_loss: 125.7377507686615
batch: 1843
train_loss: 128.52602744102478
batch: 1844
train_loss: 131.3859326839447
batch: 1845
train_loss: 134.29969596862793
batch: 1846
train_loss: 137.24372744560242
batch: 1847
train_loss: 140.22472476959229
batch: 1848
train_loss: 143.12300944328308
batch: 1849
train_loss: 145.99366235733032
batch: 1850
train_loss: 148.86085629463196
batch: 1851
train_loss: 151.74620747566223
batch: 1852
train_loss: 154.71930837631226
batch: 1853
train_loss: 157.5950539112091
batch: 1854
train_loss: 160.58255243301392
batch: 1855
train_loss: 163.5813660621643
batch: 1856
train_loss: 166.51217985153198
batch: 1857
train_loss: 169.40492963790894
batch: 1858
train_loss: 172.38949847221375
batch: 1859
train_loss: 175.29368495941162
batch: 1860
train_loss: 178.13339591026306
batch: 1861
train_loss: 181.07436323165894
batch: 1862
train_loss: 183.95553517341614
batch: 1863
train_loss: 186.86518788337708
batch: 1864
train_loss: 189.74627470970154
batch: 1865
train_loss: 192.63029861450195
batch: 1866
train_loss: 195.6384847164154
batch: 1867
train_loss: 198.57596158981323
batch: 1868
train_loss: 201.47271084785461
batch: 1869
train_loss: 204.39262986183167
batch: 1870
train_loss: 207.28178787231445
batch: 1871
train_loss: 210.26839900016785
batch: 1872
train_loss: 213.31347489356995
batch: 1873
train_loss: 216.29304242134094
batch: 1874
train_loss: 219.22920894622803
batch: 1875
train_loss: 222.23515510559082
batch: 1876
train_loss: 225.1521716117859
batch: 1877
train_loss: 228.07797574996948
batch: 1878
train_loss: 230.97348499298096
batch: 1879
train_loss: 233.93892168998718
batch: 1880
train_loss: 236.950186252594
batch: 1881
train_loss: 239.89815759658813
batch: 1882
train_loss: 242.8053629398346
batch: 1883
train_loss: 245.7666528224945
batch: 1884
train_loss: 248.71875190734863
batch: 1885
train_loss: 251.71877026557922
batch: 1886
train_loss: 254.67599201202393
batch: 1887
train_loss: 257.60560488700867
batch: 1888
train_loss: 260.4708275794983
batch: 1889
train_loss: 263.2582516670227
batch: 1890
train_loss: 266.06809878349304
batch: 1891
train_loss: 268.84341049194336
batch: 1892
train_loss: 271.7398154735565
batch: 1893
train_loss: 274.6080267429352
batch: 1894
train_loss: 277.4132192134857
batch: 1895
train_loss: 280.45198130607605
batch: 1896
train_loss: 283.3798270225525
batch: 1897
train_loss: 286.30776500701904
batch: 1898
train_loss: 289.1805064678192
batch: 1899
train_loss: 292.1682255268097
batch: 1900
train_loss: 294.9993224143982
batch: 1901
train_loss: 297.85794591903687
batch: 1902
train_loss: 300.70644211769104
batch: 1903
train_loss: 303.6745808124542
batch: 1904
train_loss: 306.53423047065735
batch: 1905
train_loss: 309.3577914237976
batch: 1906
train_loss: 312.315717458725
batch: 1907
train_loss: 315.19113421440125
batch: 1908
train_loss: 318.0998635292053
batch: 1909
train_loss: 321.00790643692017
batch: 1910
train_loss: 323.9382667541504
batch: 1911
train_loss: 326.8846945762634
batch: 1912
train_loss: 329.77443838119507
batch: 1913
train_loss: 332.6470618247986
batch: 1914
train_loss: 335.4801826477051
batch: 1915
train_loss: 338.30688977241516
batch: 1916
train_loss: 341.23115634918213
batch: 1917
train_loss: 344.1031548976898
batch: 1918
train_loss: 346.98495292663574
batch: 1919
train_loss: 349.8390772342682
batch: 1920
train_loss: 352.6792962551117
batch: 1921
train_loss: 355.59812688827515
batch: 1922
train_loss: 358.44507694244385
batch: 1923
train_loss: 361.3298501968384
batch: 1924
train_loss: 364.1357502937317
batch: 1925
train_loss: 366.9576723575592
batch: 1926
train_loss: 369.8118555545807
batch: 1927
train_loss: 372.6231212615967
batch: 1928
train_loss: 375.4544153213501
batch: 1929
train_loss: 378.21293354034424
batch: 1930
train_loss: 381.02635622024536
batch: 1931
train_loss: 383.874525308609
batch: 1932
train_loss: 386.8135323524475
batch: 1933
train_loss: 389.73080015182495
batch: 1934
train_loss: 392.6746118068695
batch: 1935
train_loss: 395.5434935092926
batch: 1936
train_loss: 398.4600293636322
batch: 1937
train_loss: 401.34723472595215
batch: 1938
train_loss: 404.2939021587372
batch: 1939
train_loss: 407.18475365638733
batch: 1940
train_loss: 410.04015159606934
batch: 1941
train_loss: 412.9676389694214
batch: 1942
train_loss: 415.8716986179352
batch: 1943
train_loss: 418.71204233169556
batch: 1944
train_loss: 421.4576766490936
batch: 1945
train_loss: 424.3055474758148
batch: 1946
train_loss: 427.103346824646
batch: 1947
train_loss: 429.9036681652069
batch: 1948
train_loss: 432.7831823825836
batch: 1949
train_loss: 435.65680170059204
batch: 1950
train_loss: 438.5479896068573
batch: 1951
train_loss: 441.45188760757446
batch: 1952
train_loss: 444.24537086486816
batch: 1953
train_loss: 447.1130886077881
batch: 1954
train_loss: 449.99265718460083
batch: 1955
train_loss: 452.84829354286194
batch: 1956
train_loss: 455.73710441589355
batch: 1957
train_loss: 458.58120679855347
batch: 1958
train_loss: 461.4849328994751
batch: 1959
train_loss: 464.35859298706055
batch: 1960
train_loss: 467.1610014438629
batch: 1961
train_loss: 470.04361295700073
batch: 1962
train_loss: 472.8419699668884
batch: 1963
train_loss: 475.60335087776184
batch: 1964
train_loss: 478.41853284835815
batch: 1965
train_loss: 481.2732963562012
batch: 1966
train_loss: 484.1293947696686
batch: 1967
train_loss: 487.0080552101135
batch: 1968
train_loss: 489.8383755683899
batch: 1969
train_loss: 492.67223048210144
batch: 1970
train_loss: 495.5408492088318
batch: 1971
train_loss: 498.3207371234894
batch: 1972
train_loss: 501.20692014694214
batch: 1973
train_loss: 504.0093469619751
batch: 1974
train_loss: 506.7623836994171
batch: 1975
train_loss: 509.5204668045044
batch: 1976
train_loss: 512.3039929866791
batch: 1977
train_loss: 515.1458368301392
batch: 1978
train_loss: 517.9582402706146
batch: 1979
train_loss: 520.8336424827576
batch: 1980
train_loss: 523.6921467781067
batch: 1981
train_loss: 526.4940991401672
batch: 1982
train_loss: 529.3551204204559
batch: 1983
train_loss: 532.1390867233276
batch: 1984
train_loss: 534.9307162761688
batch: 1985
train_loss: 537.7588453292847
batch: 1986
train_loss: 540.5852069854736
batch: 1987
train_loss: 543.4158544540405
batch: 1988
train_loss: 546.1471645832062
batch: 1989
train_loss: 548.9292407035828
batch: 1990
train_loss: 551.6998360157013
batch: 1991
train_loss: 554.4989666938782
batch: 1992
train_loss: 557.2922511100769
batch: 1993
train_loss: 560.1021440029144
batch: 1994
train_loss: 562.9165802001953
batch: 1995
train_loss: 565.7696416378021
batch: 1996
train_loss: 568.555267572403
batch: 1997
train_loss: 571.3960297107697
batch: 1998
train_loss: 574.2509198188782
batch: 1999
train_loss: 576.9828908443451
| epoch   0 step     2000 |   2000 batches | lr 0.000248 | ms/batch 335.74 | loss  2.88 | bpc   4.16205
batch: 2000
train_loss: 2.755486488342285
batch: 2001
train_loss: 5.53204870223999
batch: 2002
train_loss: 8.341766357421875
batch: 2003
train_loss: 11.068006992340088
batch: 2004
train_loss: 13.875770807266235
batch: 2005
train_loss: 16.69999933242798
batch: 2006
train_loss: 19.542274236679077
batch: 2007
train_loss: 22.32981777191162
batch: 2008
train_loss: 25.145811796188354
batch: 2009
train_loss: 27.94239616394043
batch: 2010
train_loss: 30.786616563796997
batch: 2011
train_loss: 33.62644577026367
batch: 2012
train_loss: 36.47775053977966
batch: 2013
train_loss: 39.25857996940613
batch: 2014
train_loss: 42.030856132507324
batch: 2015
train_loss: 44.838295221328735
batch: 2016
train_loss: 47.760812520980835
batch: 2017
train_loss: 50.6384482383728
batch: 2018
train_loss: 53.4747269153595
batch: 2019
train_loss: 56.24631905555725
batch: 2020
train_loss: 58.995585441589355
batch: 2021
train_loss: 61.782949686050415
batch: 2022
train_loss: 64.54177260398865
batch: 2023
train_loss: 67.31572270393372
batch: 2024
train_loss: 70.16930055618286
batch: 2025
train_loss: 73.11994910240173
batch: 2026
train_loss: 76.01937699317932
batch: 2027
train_loss: 78.9451630115509
batch: 2028
train_loss: 81.82022380828857
batch: 2029
train_loss: 84.67334651947021
batch: 2030
train_loss: 87.45214247703552
batch: 2031
train_loss: 90.28861880302429
batch: 2032
train_loss: 93.13045406341553
batch: 2033
train_loss: 95.94601774215698
batch: 2034
train_loss: 98.86624073982239
batch: 2035
train_loss: 101.7697377204895
batch: 2036
train_loss: 104.72933864593506
batch: 2037
train_loss: 107.57271838188171
batch: 2038
train_loss: 110.39528298377991
batch: 2039
train_loss: 113.27495670318604
batch: 2040
train_loss: 116.2586600780487
batch: 2041
train_loss: 119.21379566192627
batch: 2042
train_loss: 122.07523012161255
batch: 2043
train_loss: 124.9612021446228
batch: 2044
train_loss: 127.81380033493042
batch: 2045
train_loss: 130.75160694122314
batch: 2046
train_loss: 133.68317031860352
batch: 2047
train_loss: 136.55896663665771
batch: 2048
train_loss: 139.46226572990417
batch: 2049
train_loss: 142.44802021980286
batch: 2050
train_loss: 145.359854221344
batch: 2051
train_loss: 148.30671906471252
batch: 2052
train_loss: 151.1834089756012
batch: 2053
train_loss: 154.05582427978516
batch: 2054
train_loss: 156.9864161014557
batch: 2055
train_loss: 159.8927845954895
batch: 2056
train_loss: 162.83860063552856
batch: 2057
train_loss: 165.68247056007385
batch: 2058
train_loss: 168.59066319465637
batch: 2059
train_loss: 171.4059386253357
batch: 2060
train_loss: 174.26307606697083
batch: 2061
train_loss: 177.05584716796875
batch: 2062
train_loss: 179.85270476341248
batch: 2063
train_loss: 182.64284896850586
batch: 2064
train_loss: 185.6018030643463
batch: 2065
train_loss: 188.42331743240356
batch: 2066
train_loss: 191.27486371994019
batch: 2067
train_loss: 194.0355579853058
batch: 2068
train_loss: 196.7865710258484
batch: 2069
train_loss: 199.56610321998596
batch: 2070
train_loss: 202.30616569519043
batch: 2071
train_loss: 205.08746075630188
batch: 2072
train_loss: 207.8520007133484
batch: 2073
train_loss: 210.7026445865631
batch: 2074
train_loss: 213.5477774143219
batch: 2075
train_loss: 216.3430302143097
batch: 2076
train_loss: 219.144357919693
batch: 2077
train_loss: 221.96698379516602
batch: 2078
train_loss: 224.8130657672882
batch: 2079
train_loss: 227.51591992378235
batch: 2080
train_loss: 230.22435092926025
batch: 2081
train_loss: 233.0528085231781
batch: 2082
train_loss: 235.84179973602295
batch: 2083
train_loss: 238.60443115234375
batch: 2084
train_loss: 241.34774565696716
batch: 2085
train_loss: 244.1154441833496
batch: 2086
train_loss: 246.8962996006012
batch: 2087
train_loss: 249.59568214416504
batch: 2088
train_loss: 252.3828103542328
batch: 2089
train_loss: 255.2246162891388
batch: 2090
train_loss: 258.0019311904907
batch: 2091
train_loss: 260.83842420578003
batch: 2092
train_loss: 263.6949932575226
batch: 2093
train_loss: 266.412451505661
batch: 2094
train_loss: 269.15480613708496
batch: 2095
train_loss: 271.94610118865967
batch: 2096
train_loss: 274.67252588272095
batch: 2097
train_loss: 277.48759722709656
batch: 2098
train_loss: 280.17584109306335
batch: 2099
train_loss: 283.02225399017334
batch: 2100
train_loss: 285.7370502948761
batch: 2101
train_loss: 288.56057262420654
batch: 2102
train_loss: 291.3433430194855
batch: 2103
train_loss: 294.08892011642456
batch: 2104
train_loss: 296.902489900589
batch: 2105
train_loss: 299.7074644565582
batch: 2106
train_loss: 302.45191717147827
batch: 2107
train_loss: 305.13779520988464
batch: 2108
train_loss: 307.97862124443054
batch: 2109
train_loss: 310.7742214202881
batch: 2110
train_loss: 313.54009342193604
batch: 2111
train_loss: 316.3507363796234
batch: 2112
train_loss: 319.12218260765076
batch: 2113
train_loss: 321.9392149448395
batch: 2114
train_loss: 324.7594735622406
batch: 2115
train_loss: 327.5855493545532
batch: 2116
train_loss: 330.39966559410095
batch: 2117
train_loss: 333.19203305244446
batch: 2118
train_loss: 336.09036445617676
batch: 2119
train_loss: 338.91541051864624
batch: 2120
train_loss: 341.7507436275482
batch: 2121
train_loss: 344.60595703125
batch: 2122
train_loss: 347.4240732192993
batch: 2123
train_loss: 350.2070779800415
batch: 2124
train_loss: 353.00497174263
batch: 2125
train_loss: 355.84848380088806
batch: 2126
train_loss: 358.68028497695923
batch: 2127
train_loss: 361.48521614074707
batch: 2128
train_loss: 364.2584722042084
batch: 2129
train_loss: 367.03809928894043
batch: 2130
train_loss: 369.81153082847595
batch: 2131
train_loss: 372.66346287727356
batch: 2132
train_loss: 375.53589153289795
batch: 2133
train_loss: 378.31425738334656
batch: 2134
train_loss: 381.079069852829
batch: 2135
train_loss: 383.8263626098633
batch: 2136
train_loss: 386.557742357254
batch: 2137
train_loss: 389.4443025588989
batch: 2138
train_loss: 392.267121553421
batch: 2139
train_loss: 395.1021132469177
batch: 2140
train_loss: 397.91660237312317
batch: 2141
train_loss: 400.7713963985443
batch: 2142
train_loss: 403.52966499328613
batch: 2143
train_loss: 406.2999246120453
batch: 2144
train_loss: 409.0790355205536
batch: 2145
train_loss: 411.9333264827728
batch: 2146
train_loss: 414.71651673316956
batch: 2147
train_loss: 417.47043657302856
batch: 2148
train_loss: 420.2815387248993
batch: 2149
train_loss: 423.03365659713745
batch: 2150
train_loss: 425.82514667510986
batch: 2151
train_loss: 428.6690580844879
batch: 2152
train_loss: 431.5083885192871
batch: 2153
train_loss: 434.30117559432983
batch: 2154
train_loss: 437.10645937919617
batch: 2155
train_loss: 439.95754647254944
batch: 2156
train_loss: 442.87309193611145
batch: 2157
train_loss: 445.69392824172974
batch: 2158
train_loss: 448.5682280063629
batch: 2159
train_loss: 451.39114212989807
batch: 2160
train_loss: 454.2742578983307
batch: 2161
train_loss: 457.12668204307556
batch: 2162
train_loss: 459.92947816848755
batch: 2163
train_loss: 462.75353813171387
batch: 2164
train_loss: 465.7422947883606
batch: 2165
train_loss: 468.727091550827
batch: 2166
train_loss: 471.74275636672974
batch: 2167
train_loss: 474.53354930877686
batch: 2168
train_loss: 477.43178629875183
batch: 2169
train_loss: 480.3280129432678
batch: 2170
train_loss: 483.1275200843811
batch: 2171
train_loss: 485.99318385124207
batch: 2172
train_loss: 488.88430738449097
batch: 2173
train_loss: 491.67001485824585
batch: 2174
train_loss: 494.53232502937317
batch: 2175
train_loss: 497.27110958099365
batch: 2176
train_loss: 500.147851228714
batch: 2177
train_loss: 502.979350566864
batch: 2178
train_loss: 505.8338112831116
batch: 2179
train_loss: 508.6659183502197
batch: 2180
train_loss: 511.51795077323914
batch: 2181
train_loss: 514.4179801940918
batch: 2182
train_loss: 517.3796842098236
batch: 2183
train_loss: 520.2338650226593
batch: 2184
train_loss: 523.0589814186096
batch: 2185
train_loss: 525.8894908428192
batch: 2186
train_loss: 528.6922781467438
batch: 2187
train_loss: 531.5474848747253
batch: 2188
train_loss: 534.3541867733002
batch: 2189
train_loss: 537.157032251358
batch: 2190
train_loss: 539.94566655159
batch: 2191
train_loss: 542.6861670017242
batch: 2192
train_loss: 545.5617377758026
batch: 2193
train_loss: 548.3984696865082
batch: 2194
train_loss: 551.2279098033905
batch: 2195
train_loss: 554.0062572956085
batch: 2196
train_loss: 556.8715369701385
batch: 2197
train_loss: 559.8234665393829
batch: 2198
train_loss: 562.727480173111
batch: 2199
train_loss: 565.4968287944794
| epoch   0 step     2200 |   2200 batches | lr 0.000248 | ms/batch 335.54 | loss  2.83 | bpc   4.07920
batch: 2200
train_loss: 2.778414726257324
batch: 2201
train_loss: 5.568162441253662
batch: 2202
train_loss: 8.358656406402588
batch: 2203
train_loss: 11.120896816253662
batch: 2204
train_loss: 13.896957635879517
batch: 2205
train_loss: 16.66949701309204
batch: 2206
train_loss: 19.58440113067627
batch: 2207
train_loss: 22.453482627868652
batch: 2208
train_loss: 25.348478078842163
batch: 2209
train_loss: 28.24492621421814
batch: 2210
train_loss: 31.023422718048096
batch: 2211
train_loss: 33.84985876083374
batch: 2212
train_loss: 36.6977424621582
batch: 2213
train_loss: 39.541054010391235
batch: 2214
train_loss: 42.414194107055664
batch: 2215
train_loss: 45.21557092666626
batch: 2216
train_loss: 47.963759899139404
batch: 2217
train_loss: 50.75493621826172
batch: 2218
train_loss: 53.53122806549072
batch: 2219
train_loss: 56.373021602630615
batch: 2220
train_loss: 59.17860794067383
batch: 2221
train_loss: 61.87849521636963
batch: 2222
train_loss: 64.70988702774048
batch: 2223
train_loss: 67.52545714378357
batch: 2224
train_loss: 70.35127472877502
batch: 2225
train_loss: 73.23587727546692
batch: 2226
train_loss: 76.09194445610046
batch: 2227
train_loss: 78.89826679229736
batch: 2228
train_loss: 81.73674392700195
batch: 2229
train_loss: 84.51247191429138
batch: 2230
train_loss: 87.31152653694153
batch: 2231
train_loss: 90.10189652442932
batch: 2232
train_loss: 92.96873593330383
batch: 2233
train_loss: 95.84689474105835
batch: 2234
train_loss: 98.77863049507141
batch: 2235
train_loss: 101.62453436851501
batch: 2236
train_loss: 104.42121315002441
batch: 2237
train_loss: 107.20264291763306
batch: 2238
train_loss: 109.98101949691772
batch: 2239
train_loss: 112.91615962982178
batch: 2240
train_loss: 115.81789636611938
batch: 2241
train_loss: 118.70235323905945
batch: 2242
train_loss: 121.52651643753052
batch: 2243
train_loss: 124.35763216018677
batch: 2244
train_loss: 127.18713760375977
batch: 2245
train_loss: 130.06362509727478
batch: 2246
train_loss: 132.89859056472778
batch: 2247
train_loss: 135.72281289100647
batch: 2248
train_loss: 138.4724476337433
batch: 2249
train_loss: 141.30425024032593
batch: 2250
train_loss: 144.18340849876404
batch: 2251
train_loss: 147.18768072128296
batch: 2252
train_loss: 150.0316777229309
batch: 2253
train_loss: 152.83728122711182
batch: 2254
train_loss: 155.64402961730957
batch: 2255
train_loss: 158.50436925888062
batch: 2256
train_loss: 161.3262345790863
batch: 2257
train_loss: 164.18858337402344
batch: 2258
train_loss: 167.04156041145325
batch: 2259
train_loss: 169.8389744758606
batch: 2260
train_loss: 172.7179889678955
batch: 2261
train_loss: 175.487726688385
batch: 2262
train_loss: 178.22628116607666
batch: 2263
train_loss: 181.08340120315552
batch: 2264
train_loss: 183.8861517906189
batch: 2265
train_loss: 186.69374299049377
batch: 2266
train_loss: 189.51241874694824
batch: 2267
train_loss: 192.36222553253174
batch: 2268
train_loss: 195.19091486930847
batch: 2269
train_loss: 198.1075074672699
batch: 2270
train_loss: 200.97315335273743
batch: 2271
train_loss: 203.83449530601501
batch: 2272
train_loss: 206.7568724155426
batch: 2273
train_loss: 209.52835178375244
batch: 2274
train_loss: 212.37970280647278
batch: 2275
train_loss: 215.3154661655426
batch: 2276
train_loss: 218.19709873199463
batch: 2277
train_loss: 221.01033401489258
batch: 2278
train_loss: 223.9577238559723
batch: 2279
train_loss: 226.8457624912262
batch: 2280
train_loss: 229.6914837360382
batch: 2281
train_loss: 232.58634972572327
batch: 2282
train_loss: 235.41599369049072
batch: 2283
train_loss: 238.15251231193542
batch: 2284
train_loss: 240.92052364349365
batch: 2285
train_loss: 243.7710452079773
batch: 2286
train_loss: 246.58157515525818
batch: 2287
train_loss: 249.33258819580078
batch: 2288
train_loss: 252.19204926490784
batch: 2289
train_loss: 255.02451753616333
batch: 2290
train_loss: 257.8202557563782
batch: 2291
train_loss: 260.66187500953674
batch: 2292
train_loss: 263.4565815925598
batch: 2293
train_loss: 266.25774598121643
batch: 2294
train_loss: 269.0654709339142
batch: 2295
train_loss: 271.8224573135376
batch: 2296
train_loss: 274.623925447464
batch: 2297
train_loss: 277.3965241909027
batch: 2298
train_loss: 280.20164251327515
batch: 2299
train_loss: 283.04550981521606
batch: 2300
train_loss: 285.8489842414856
batch: 2301
train_loss: 288.67872643470764
batch: 2302
train_loss: 291.50425720214844
batch: 2303
train_loss: 294.39914536476135
batch: 2304
train_loss: 297.25440859794617
batch: 2305
train_loss: 300.0070595741272
batch: 2306
train_loss: 302.7072842121124
batch: 2307
train_loss: 305.5799512863159
batch: 2308
train_loss: 308.3683874607086
batch: 2309
train_loss: 311.19976115226746
batch: 2310
train_loss: 314.08720803260803
batch: 2311
train_loss: 316.85167479515076
batch: 2312
train_loss: 319.6744840145111
batch: 2313
train_loss: 322.52302622795105
batch: 2314
train_loss: 325.4512550830841
batch: 2315
train_loss: 328.2710406780243
batch: 2316
train_loss: 331.1283712387085
batch: 2317
train_loss: 333.9610891342163
batch: 2318
train_loss: 336.79686522483826
batch: 2319
train_loss: 339.59061217308044
batch: 2320
train_loss: 342.42871499061584
batch: 2321
train_loss: 345.1930603981018
batch: 2322
train_loss: 348.05369329452515
batch: 2323
train_loss: 350.80912804603577
batch: 2324
train_loss: 353.5461890697479
batch: 2325
train_loss: 356.3250756263733
batch: 2326
train_loss: 359.21129631996155
batch: 2327
train_loss: 362.02634477615356
batch: 2328
train_loss: 364.8135988712311
batch: 2329
train_loss: 367.56019020080566
batch: 2330
train_loss: 370.28933238983154
batch: 2331
train_loss: 373.03468322753906
batch: 2332
train_loss: 375.8003981113434
batch: 2333
train_loss: 378.6120574474335
batch: 2334
train_loss: 381.4374985694885
batch: 2335
train_loss: 384.17485308647156
batch: 2336
train_loss: 387.0095143318176
batch: 2337
train_loss: 389.92484164237976
batch: 2338
train_loss: 392.79587721824646
batch: 2339
train_loss: 395.60316491127014
batch: 2340
train_loss: 398.4640669822693
batch: 2341
train_loss: 401.2583601474762
batch: 2342
train_loss: 404.03891944885254
batch: 2343
train_loss: 406.8831715583801
batch: 2344
train_loss: 409.6811716556549
batch: 2345
train_loss: 412.46517515182495
batch: 2346
train_loss: 415.26025342941284
batch: 2347
train_loss: 418.0591299533844
batch: 2348
train_loss: 420.86187982559204
batch: 2349
train_loss: 423.7341969013214
batch: 2350
train_loss: 426.51573395729065
batch: 2351
train_loss: 429.23082852363586
batch: 2352
train_loss: 432.10037636756897
batch: 2353
train_loss: 434.82834792137146
batch: 2354
train_loss: 437.63526916503906
batch: 2355
train_loss: 440.4130642414093
batch: 2356
train_loss: 443.0585231781006
batch: 2357
train_loss: 445.7372887134552
batch: 2358
train_loss: 448.4274456501007
batch: 2359
train_loss: 451.1073384284973
batch: 2360
train_loss: 453.81592655181885
batch: 2361
train_loss: 456.54110765457153
batch: 2362
train_loss: 459.3297827243805
batch: 2363
train_loss: 462.11802530288696
batch: 2364
train_loss: 464.92751383781433
batch: 2365
train_loss: 467.70102977752686
batch: 2366
train_loss: 470.49386191368103
batch: 2367
train_loss: 473.2850263118744
batch: 2368
train_loss: 476.04951429367065
batch: 2369
train_loss: 478.8747022151947
batch: 2370
train_loss: 481.7080113887787
batch: 2371
train_loss: 484.5423038005829
batch: 2372
train_loss: 487.53556847572327
batch: 2373
train_loss: 490.3998577594757
batch: 2374
train_loss: 493.2167007923126
batch: 2375
train_loss: 496.0371150970459
batch: 2376
train_loss: 498.91187167167664
batch: 2377
train_loss: 501.72296953201294
batch: 2378
train_loss: 504.6589651107788
batch: 2379
train_loss: 507.4856262207031
batch: 2380
train_loss: 510.3377432823181
batch: 2381
train_loss: 513.2378222942352
batch: 2382
train_loss: 516.039027929306
batch: 2383
train_loss: 518.9880583286285
batch: 2384
train_loss: 521.8510138988495
batch: 2385
train_loss: 524.7407605648041
batch: 2386
train_loss: 527.614807844162
batch: 2387
train_loss: 530.4883229732513
batch: 2388
train_loss: 533.2398674488068
batch: 2389
train_loss: 536.073816537857
batch: 2390
train_loss: 538.9014382362366
batch: 2391
train_loss: 541.6668112277985
batch: 2392
train_loss: 544.4323666095734
batch: 2393
train_loss: 547.1592183113098
batch: 2394
train_loss: 549.8688566684723
batch: 2395
train_loss: 552.6144177913666
batch: 2396
train_loss: 555.4647266864777
batch: 2397
train_loss: 558.2889921665192
batch: 2398
train_loss: 561.0632426738739
batch: 2399
train_loss: 563.7461137771606
| epoch   0 step     2400 |   2400 batches | lr 0.000248 | ms/batch 342.94 | loss  2.82 | bpc   4.06657
batch: 2400
train_loss: 2.739896535873413
batch: 2401
train_loss: 5.532480955123901
batch: 2402
train_loss: 8.33705472946167
batch: 2403
train_loss: 11.058648824691772
batch: 2404
train_loss: 13.851433515548706
batch: 2405
train_loss: 16.625147342681885
batch: 2406
train_loss: 19.332684993743896
batch: 2407
train_loss: 22.115474462509155
batch: 2408
train_loss: 24.912707328796387
batch: 2409
train_loss: 27.656564712524414
batch: 2410
train_loss: 30.350515604019165
batch: 2411
train_loss: 33.161643981933594
batch: 2412
train_loss: 35.93043923377991
batch: 2413
train_loss: 38.58255314826965
batch: 2414
train_loss: 41.3414523601532
batch: 2415
train_loss: 44.074225425720215
batch: 2416
train_loss: 46.784600496292114
batch: 2417
train_loss: 49.583467960357666
batch: 2418
train_loss: 52.3747775554657
batch: 2419
train_loss: 55.19796419143677
batch: 2420
train_loss: 57.983272075653076
batch: 2421
train_loss: 60.70721364021301
batch: 2422
train_loss: 63.500991106033325
batch: 2423
train_loss: 66.27551651000977
batch: 2424
train_loss: 69.04985904693604
batch: 2425
train_loss: 71.81881713867188
batch: 2426
train_loss: 74.60581064224243
batch: 2427
train_loss: 77.34897136688232
batch: 2428
train_loss: 80.09061527252197
batch: 2429
train_loss: 82.8734073638916
batch: 2430
train_loss: 85.64610362052917
batch: 2431
train_loss: 88.3800950050354
batch: 2432
train_loss: 91.14570832252502
batch: 2433
train_loss: 93.95914793014526
batch: 2434
train_loss: 96.76283931732178
batch: 2435
train_loss: 99.5008544921875
batch: 2436
train_loss: 102.2362916469574
batch: 2437
train_loss: 105.03948545455933
batch: 2438
train_loss: 107.77020144462585
batch: 2439
train_loss: 110.48207998275757
batch: 2440
train_loss: 113.24734091758728
batch: 2441
train_loss: 116.00118064880371
batch: 2442
train_loss: 118.7095410823822
batch: 2443
train_loss: 121.44452142715454
batch: 2444
train_loss: 124.22135806083679
batch: 2445
train_loss: 126.962970495224
batch: 2446
train_loss: 129.73280143737793
batch: 2447
train_loss: 132.43372988700867
batch: 2448
train_loss: 135.14414501190186
batch: 2449
train_loss: 137.86649703979492
batch: 2450
train_loss: 140.61885404586792
batch: 2451
train_loss: 143.3100938796997
batch: 2452
train_loss: 146.1068413257599
batch: 2453
train_loss: 148.95485401153564
batch: 2454
train_loss: 151.71770811080933
batch: 2455
train_loss: 154.4846260547638
batch: 2456
train_loss: 157.26638269424438
batch: 2457
train_loss: 159.97542071342468
batch: 2458
train_loss: 162.78442239761353
batch: 2459
train_loss: 165.54310178756714
batch: 2460
train_loss: 168.2606496810913
batch: 2461
train_loss: 171.01710200309753
batch: 2462
train_loss: 173.76638102531433
batch: 2463
train_loss: 176.5729522705078
batch: 2464
train_loss: 179.33314538002014
batch: 2465
train_loss: 182.1083745956421
batch: 2466
train_loss: 184.87484097480774
batch: 2467
train_loss: 187.80057644844055
batch: 2468
train_loss: 190.69945096969604
batch: 2469
train_loss: 193.44180846214294
batch: 2470
train_loss: 196.2592122554779
batch: 2471
train_loss: 198.99959897994995
batch: 2472
train_loss: 201.73633217811584
batch: 2473
train_loss: 204.56851887702942
batch: 2474
train_loss: 207.32606482505798
batch: 2475
train_loss: 209.98461604118347
batch: 2476
train_loss: 212.81302976608276
batch: 2477
train_loss: 215.65331268310547
batch: 2478
train_loss: 218.40449833869934
batch: 2479
train_loss: 221.25795888900757
batch: 2480
train_loss: 224.0200390815735
batch: 2481
train_loss: 226.75586342811584
batch: 2482
train_loss: 229.49553632736206
batch: 2483
train_loss: 232.26956725120544
batch: 2484
train_loss: 235.0691750049591
batch: 2485
train_loss: 237.8837695121765
batch: 2486
train_loss: 240.61573195457458
batch: 2487
train_loss: 243.3523519039154
batch: 2488
train_loss: 246.06535649299622
batch: 2489
train_loss: 248.77947759628296
batch: 2490
train_loss: 251.6026909351349
batch: 2491
train_loss: 254.36686730384827
batch: 2492
train_loss: 257.1216473579407
batch: 2493
train_loss: 259.9264335632324
batch: 2494
train_loss: 262.6429090499878
batch: 2495
train_loss: 265.40993785858154
batch: 2496
train_loss: 268.10584235191345
batch: 2497
train_loss: 270.882333278656
batch: 2498
train_loss: 273.63976979255676
batch: 2499
train_loss: 276.3666648864746
batch: 2500
train_loss: 279.1322329044342
batch: 2501
train_loss: 281.8267550468445
batch: 2502
train_loss: 284.5321762561798
batch: 2503
train_loss: 287.28901958465576
batch: 2504
train_loss: 290.0107238292694
batch: 2505
train_loss: 292.800799369812
batch: 2506
train_loss: 295.5244038105011
batch: 2507
train_loss: 298.29625272750854
batch: 2508
train_loss: 301.05666399002075
batch: 2509
train_loss: 303.85505986213684
batch: 2510
train_loss: 306.7157063484192
batch: 2511
train_loss: 309.4756751060486
batch: 2512
train_loss: 312.30459332466125
batch: 2513
train_loss: 315.130508184433
batch: 2514
train_loss: 317.96537733078003
batch: 2515
train_loss: 320.8415307998657
batch: 2516
train_loss: 323.59108543395996
batch: 2517
train_loss: 326.4342873096466
batch: 2518
train_loss: 329.22611451148987
batch: 2519
train_loss: 332.06207847595215
batch: 2520
train_loss: 334.8743133544922
batch: 2521
train_loss: 337.83775091171265
batch: 2522
train_loss: 340.6640405654907
batch: 2523
train_loss: 343.553941488266
batch: 2524
train_loss: 346.2726867198944
batch: 2525
train_loss: 348.957234621048
batch: 2526
train_loss: 351.7260830402374
batch: 2527
train_loss: 354.53847789764404
batch: 2528
train_loss: 357.27864360809326
batch: 2529
train_loss: 360.0155100822449
batch: 2530
train_loss: 362.8980541229248
batch: 2531
train_loss: 365.6870002746582
batch: 2532
train_loss: 368.43061423301697
batch: 2533
train_loss: 371.2405688762665
batch: 2534
train_loss: 374.1289129257202
batch: 2535
train_loss: 376.9480617046356
batch: 2536
train_loss: 379.6788022518158
batch: 2537
train_loss: 382.41617226600647
batch: 2538
train_loss: 385.21625328063965
batch: 2539
train_loss: 387.9941804409027
batch: 2540
train_loss: 390.81473755836487
batch: 2541
train_loss: 393.6551923751831
batch: 2542
train_loss: 396.4583020210266
batch: 2543
train_loss: 399.296843290329
batch: 2544
train_loss: 402.0659284591675
batch: 2545
train_loss: 404.99531173706055
batch: 2546
train_loss: 407.8346347808838
batch: 2547
train_loss: 410.6882667541504
batch: 2548
train_loss: 413.4548444747925
batch: 2549
train_loss: 416.2022545337677
batch: 2550
train_loss: 419.02287769317627
batch: 2551
train_loss: 421.7793505191803
batch: 2552
train_loss: 424.5668320655823
batch: 2553
train_loss: 427.4010052680969
batch: 2554
train_loss: 430.20459246635437
batch: 2555
train_loss: 433.0926444530487
batch: 2556
train_loss: 435.8736937046051
batch: 2557
train_loss: 438.72483110427856
batch: 2558
train_loss: 441.5222342014313
batch: 2559
train_loss: 444.3762083053589
batch: 2560
train_loss: 447.13587260246277
batch: 2561
train_loss: 449.8580114841461
batch: 2562
train_loss: 452.61423921585083
batch: 2563
train_loss: 455.5211012363434
batch: 2564
train_loss: 458.35831117630005
batch: 2565
train_loss: 461.2351293563843
batch: 2566
train_loss: 464.0192279815674
batch: 2567
train_loss: 466.79514932632446
batch: 2568
train_loss: 469.6005115509033
batch: 2569
train_loss: 472.3584580421448
batch: 2570
train_loss: 475.1182289123535
batch: 2571
train_loss: 477.93471455574036
batch: 2572
train_loss: 480.6825089454651
batch: 2573
train_loss: 483.4604353904724
batch: 2574
train_loss: 486.31632256507874
batch: 2575
train_loss: 489.04098176956177
batch: 2576
train_loss: 491.84124660491943
batch: 2577
train_loss: 494.674067735672
batch: 2578
train_loss: 497.45300126075745
batch: 2579
train_loss: 500.33983159065247
batch: 2580
train_loss: 503.17813324928284
batch: 2581
train_loss: 505.9793140888214
batch: 2582
train_loss: 508.7953152656555
batch: 2583
train_loss: 511.61950516700745
batch: 2584
train_loss: 514.5283765792847
batch: 2585
train_loss: 517.3086211681366
batch: 2586
train_loss: 520.0621545314789
batch: 2587
train_loss: 522.831978559494
batch: 2588
train_loss: 525.6381211280823
batch: 2589
train_loss: 528.411548614502
batch: 2590
train_loss: 531.0976145267487
batch: 2591
train_loss: 533.8938946723938
batch: 2592
train_loss: 536.6686770915985
batch: 2593
train_loss: 539.4829392433167
batch: 2594
train_loss: 542.250999212265
batch: 2595
train_loss: 544.9772427082062
batch: 2596
train_loss: 547.7109575271606
batch: 2597
train_loss: 550.3743059635162
batch: 2598
train_loss: 553.0475199222565
batch: 2599
train_loss: 555.7354502677917
| epoch   0 step     2600 |   2600 batches | lr 0.000247 | ms/batch 347.27 | loss  2.78 | bpc   4.00878
batch: 2600
train_loss: 2.8041417598724365
batch: 2601
train_loss: 5.516430854797363
batch: 2602
train_loss: 8.301730871200562
batch: 2603
train_loss: 11.0277419090271
batch: 2604
train_loss: 13.819279193878174
batch: 2605
train_loss: 16.591305017471313
batch: 2606
train_loss: 19.31514596939087
batch: 2607
train_loss: 22.055652856826782
batch: 2608
train_loss: 24.851530075073242
batch: 2609
train_loss: 27.552067279815674
batch: 2610
train_loss: 30.364257097244263
batch: 2611
train_loss: 33.238353967666626
batch: 2612
train_loss: 35.95517015457153
batch: 2613
train_loss: 38.68647289276123
batch: 2614
train_loss: 41.47748637199402
batch: 2615
train_loss: 44.27497577667236
batch: 2616
train_loss: 46.93317246437073
batch: 2617
train_loss: 49.66223430633545
batch: 2618
train_loss: 52.43911647796631
batch: 2619
train_loss: 55.112353563308716
batch: 2620
train_loss: 57.85782027244568
batch: 2621
train_loss: 60.644094467163086
batch: 2622
train_loss: 63.447086334228516
batch: 2623
train_loss: 66.23567295074463
batch: 2624
train_loss: 69.10949063301086
batch: 2625
train_loss: 71.927561044693
batch: 2626
train_loss: 74.73038792610168
batch: 2627
train_loss: 77.52486634254456
batch: 2628
train_loss: 80.20497941970825
batch: 2629
train_loss: 83.05171275138855
batch: 2630
train_loss: 85.84104990959167
batch: 2631
train_loss: 88.55197739601135
batch: 2632
train_loss: 91.25457835197449
batch: 2633
train_loss: 93.99425935745239
batch: 2634
train_loss: 96.73698258399963
batch: 2635
train_loss: 99.47058868408203
batch: 2636
train_loss: 102.19097876548767
batch: 2637
train_loss: 104.9024305343628
batch: 2638
train_loss: 107.74680709838867
batch: 2639
train_loss: 110.47760915756226
batch: 2640
train_loss: 113.19537568092346
batch: 2641
train_loss: 115.93730401992798
batch: 2642
train_loss: 118.71180415153503
batch: 2643
train_loss: 121.51368236541748
batch: 2644
train_loss: 124.4452657699585
batch: 2645
train_loss: 127.16304850578308
batch: 2646
train_loss: 129.88890671730042
batch: 2647
train_loss: 132.69454312324524
batch: 2648
train_loss: 135.41027784347534
batch: 2649
train_loss: 138.12740063667297
batch: 2650
train_loss: 140.93769669532776
batch: 2651
train_loss: 143.70749235153198
batch: 2652
train_loss: 146.54433369636536
batch: 2653
train_loss: 149.3339548110962
batch: 2654
train_loss: 152.06250190734863
batch: 2655
train_loss: 154.79148697853088
batch: 2656
train_loss: 157.51615452766418
batch: 2657
train_loss: 160.38032698631287
batch: 2658
train_loss: 163.22306323051453
batch: 2659
train_loss: 166.03900122642517
batch: 2660
train_loss: 168.74986672401428
batch: 2661
train_loss: 171.5794267654419
batch: 2662
train_loss: 174.2899055480957
batch: 2663
train_loss: 177.0900375843048
batch: 2664
train_loss: 179.85817289352417
batch: 2665
train_loss: 182.61640787124634
batch: 2666
train_loss: 185.39813566207886
batch: 2667
train_loss: 188.14061903953552
batch: 2668
train_loss: 191.01983642578125
batch: 2669
train_loss: 193.86336970329285
batch: 2670
train_loss: 196.6489396095276
batch: 2671
train_loss: 199.37277054786682
batch: 2672
train_loss: 202.13785409927368
batch: 2673
train_loss: 204.9462025165558
batch: 2674
train_loss: 207.7053747177124
batch: 2675
train_loss: 210.47304272651672
batch: 2676
train_loss: 213.21279191970825
batch: 2677
train_loss: 215.85846042633057
batch: 2678
train_loss: 218.642098903656
batch: 2679
train_loss: 221.42121863365173
batch: 2680
train_loss: 224.19207191467285
batch: 2681
train_loss: 226.92283058166504
batch: 2682
train_loss: 229.63020277023315
batch: 2683
train_loss: 232.3951017856598
batch: 2684
train_loss: 235.16194891929626
batch: 2685
train_loss: 237.9775595664978
batch: 2686
train_loss: 240.62806272506714
batch: 2687
train_loss: 243.28320741653442
batch: 2688
train_loss: 246.10016202926636
batch: 2689
train_loss: 248.79386925697327
batch: 2690
train_loss: 251.44291949272156
batch: 2691
train_loss: 254.0402090549469
batch: 2692
train_loss: 256.674072265625
batch: 2693
train_loss: 259.35600328445435
batch: 2694
train_loss: 262.0927314758301
batch: 2695
train_loss: 264.7613778114319
batch: 2696
train_loss: 267.5267539024353
batch: 2697
train_loss: 270.1723585128784
batch: 2698
train_loss: 272.9092228412628
batch: 2699
train_loss: 275.6409001350403
batch: 2700
train_loss: 278.29489159584045
batch: 2701
train_loss: 280.97096943855286
batch: 2702
train_loss: 283.66742515563965
batch: 2703
train_loss: 286.32069659233093
batch: 2704
train_loss: 289.08621740341187
batch: 2705
train_loss: 291.7772514820099
batch: 2706
train_loss: 294.58276414871216
batch: 2707
train_loss: 297.3048174381256
batch: 2708
train_loss: 299.9899253845215
batch: 2709
train_loss: 302.8071038722992
batch: 2710
train_loss: 305.6155288219452
batch: 2711
train_loss: 308.3539934158325
batch: 2712
train_loss: 311.12487602233887
batch: 2713
train_loss: 313.86173725128174
batch: 2714
train_loss: 316.6297619342804
batch: 2715
train_loss: 319.44625306129456
batch: 2716
train_loss: 322.2400178909302
batch: 2717
train_loss: 324.88376092910767
batch: 2718
train_loss: 327.6399829387665
batch: 2719
train_loss: 330.4484498500824
batch: 2720
train_loss: 333.17185974121094
batch: 2721
train_loss: 335.9474937915802
batch: 2722
train_loss: 338.6022295951843
batch: 2723
train_loss: 341.36239409446716
batch: 2724
train_loss: 344.090842962265
batch: 2725
train_loss: 346.80108523368835
batch: 2726
train_loss: 349.4370393753052
batch: 2727
train_loss: 352.1672661304474
batch: 2728
train_loss: 354.92019391059875
batch: 2729
train_loss: 357.5906653404236
batch: 2730
train_loss: 360.18853545188904
batch: 2731
train_loss: 362.910621881485
batch: 2732
train_loss: 365.6250374317169
batch: 2733
train_loss: 368.32743978500366
batch: 2734
train_loss: 371.08654141426086
batch: 2735
train_loss: 373.77540373802185
batch: 2736
train_loss: 376.522647857666
batch: 2737
train_loss: 379.1691656112671
batch: 2738
train_loss: 381.9002559185028
batch: 2739
train_loss: 384.57673740386963
batch: 2740
train_loss: 387.26095747947693
batch: 2741
train_loss: 389.929710149765
batch: 2742
train_loss: 392.6748809814453
batch: 2743
train_loss: 395.43305468559265
batch: 2744
train_loss: 398.08723425865173
batch: 2745
train_loss: 400.6814115047455
batch: 2746
train_loss: 403.3653643131256
batch: 2747
train_loss: 405.95944142341614
batch: 2748
train_loss: 408.60126304626465
batch: 2749
train_loss: 411.29323172569275
batch: 2750
train_loss: 413.9725775718689
batch: 2751
train_loss: 416.6486961841583
batch: 2752
train_loss: 419.374587059021
batch: 2753
train_loss: 422.0460751056671
batch: 2754
train_loss: 424.7862288951874
batch: 2755
train_loss: 427.5687940120697
batch: 2756
train_loss: 430.34300899505615
batch: 2757
train_loss: 433.1143755912781
batch: 2758
train_loss: 435.76969480514526
batch: 2759
train_loss: 438.58229637145996
batch: 2760
train_loss: 441.3450815677643
batch: 2761
train_loss: 444.11272501945496
batch: 2762
train_loss: 446.8546311855316
batch: 2763
train_loss: 449.68293738365173
batch: 2764
train_loss: 452.53799390792847
batch: 2765
train_loss: 455.3252902030945
batch: 2766
train_loss: 458.0565679073334
batch: 2767
train_loss: 460.7542691230774
batch: 2768
train_loss: 463.5143778324127
batch: 2769
train_loss: 466.3295474052429
batch: 2770
train_loss: 469.16372084617615
batch: 2771
train_loss: 472.0402262210846
batch: 2772
train_loss: 474.89130306243896
batch: 2773
train_loss: 477.70960211753845
batch: 2774
train_loss: 480.58972239494324
batch: 2775
train_loss: 483.45052886009216
batch: 2776
train_loss: 486.30002546310425
batch: 2777
train_loss: 489.11546444892883
batch: 2778
train_loss: 491.76572251319885
batch: 2779
train_loss: 494.5108869075775
batch: 2780
train_loss: 497.3614172935486
batch: 2781
train_loss: 500.16140699386597
batch: 2782
train_loss: 502.9824786186218
batch: 2783
train_loss: 505.66602635383606
batch: 2784
train_loss: 508.4410650730133
batch: 2785
train_loss: 511.28849267959595
batch: 2786
train_loss: 514.0895133018494
batch: 2787
train_loss: 516.8405566215515
batch: 2788
train_loss: 519.5892255306244
batch: 2789
train_loss: 522.3446576595306
batch: 2790
train_loss: 525.1560008525848
batch: 2791
train_loss: 528.0311527252197
batch: 2792
train_loss: 530.9177508354187
batch: 2793
train_loss: 533.6807327270508
batch: 2794
train_loss: 536.3774793148041
batch: 2795
train_loss: 539.1995823383331
batch: 2796
train_loss: 542.0052597522736
batch: 2797
train_loss: 544.8964152336121
batch: 2798
train_loss: 547.8450214862823
batch: 2799
train_loss: 550.7255308628082
| epoch   0 step     2800 |   2800 batches | lr 0.000247 | ms/batch 347.05 | loss  2.75 | bpc   3.97264
batch: 2800
train_loss: 2.723881244659424
batch: 2801
train_loss: 5.45580267906189
batch: 2802
train_loss: 8.14800214767456
batch: 2803
train_loss: 10.885812997817993
batch: 2804
train_loss: 13.610305309295654
batch: 2805
train_loss: 16.389926433563232
batch: 2806
train_loss: 19.15628933906555
batch: 2807
train_loss: 21.95487117767334
batch: 2808
train_loss: 24.728116512298584
batch: 2809
train_loss: 27.551742553710938
batch: 2810
train_loss: 30.325713872909546
batch: 2811
train_loss: 33.166046142578125
batch: 2812
train_loss: 35.93596410751343
batch: 2813
train_loss: 38.72886824607849
batch: 2814
train_loss: 41.44164228439331
batch: 2815
train_loss: 44.22346353530884
batch: 2816
train_loss: 47.04037117958069
batch: 2817
train_loss: 49.92698931694031
batch: 2818
train_loss: 52.70285606384277
batch: 2819
train_loss: 55.46729922294617
batch: 2820
train_loss: 58.24756979942322
batch: 2821
train_loss: 61.18172597885132
batch: 2822
train_loss: 63.88270115852356
batch: 2823
train_loss: 66.59066867828369
batch: 2824
train_loss: 69.36802840232849
batch: 2825
train_loss: 72.19198417663574
batch: 2826
train_loss: 74.9693353176117
batch: 2827
train_loss: 77.70384383201599
batch: 2828
train_loss: 80.47121715545654
batch: 2829
train_loss: 83.21395540237427
batch: 2830
train_loss: 86.06294798851013
batch: 2831
train_loss: 88.92045736312866
batch: 2832
train_loss: 91.71326184272766
batch: 2833
train_loss: 94.59242463111877
batch: 2834
train_loss: 97.47152709960938
batch: 2835
train_loss: 100.32782745361328
batch: 2836
train_loss: 103.12311148643494
batch: 2837
train_loss: 105.79577445983887
batch: 2838
train_loss: 108.49755692481995
batch: 2839
train_loss: 111.26703977584839
batch: 2840
train_loss: 114.02474880218506
batch: 2841
train_loss: 116.76631236076355
batch: 2842
train_loss: 119.52709674835205
batch: 2843
train_loss: 122.2943160533905
batch: 2844
train_loss: 125.05183696746826
batch: 2845
train_loss: 127.78866910934448
batch: 2846
train_loss: 130.43489122390747
batch: 2847
train_loss: 133.06093764305115
batch: 2848
train_loss: 135.83928322792053
batch: 2849
train_loss: 138.42053985595703
batch: 2850
train_loss: 141.08751487731934
batch: 2851
train_loss: 143.85596823692322
batch: 2852
train_loss: 146.60701894760132
batch: 2853
train_loss: 149.38815355300903
batch: 2854
train_loss: 152.17670392990112
batch: 2855
train_loss: 154.91084790229797
batch: 2856
train_loss: 157.71022939682007
batch: 2857
train_loss: 160.4949653148651
batch: 2858
train_loss: 163.2172622680664
batch: 2859
train_loss: 165.93743300437927
batch: 2860
train_loss: 168.66264986991882
batch: 2861
train_loss: 171.3194682598114
batch: 2862
train_loss: 174.04318857192993
batch: 2863
train_loss: 176.84211230278015
batch: 2864
train_loss: 179.58323860168457
batch: 2865
train_loss: 182.31723499298096
batch: 2866
train_loss: 185.1904046535492
batch: 2867
train_loss: 187.91301441192627
batch: 2868
train_loss: 190.6041157245636
batch: 2869
train_loss: 193.3125777244568
batch: 2870
train_loss: 196.01480150222778
batch: 2871
train_loss: 198.88758397102356
batch: 2872
train_loss: 201.72142267227173
batch: 2873
train_loss: 204.43942546844482
batch: 2874
train_loss: 207.10334277153015
batch: 2875
train_loss: 209.76532983779907
batch: 2876
train_loss: 212.485013961792
batch: 2877
train_loss: 215.24396181106567
batch: 2878
train_loss: 217.93639540672302
batch: 2879
train_loss: 220.80466651916504
batch: 2880
train_loss: 223.61864352226257
batch: 2881
train_loss: 226.45322918891907
batch: 2882
train_loss: 229.24882078170776
batch: 2883
train_loss: 232.07536935806274
batch: 2884
train_loss: 234.903906583786
batch: 2885
train_loss: 237.75979590415955
batch: 2886
train_loss: 240.5327911376953
batch: 2887
train_loss: 243.41657042503357
batch: 2888
train_loss: 246.1312611103058
batch: 2889
train_loss: 248.85057854652405
batch: 2890
train_loss: 251.51857209205627
batch: 2891
train_loss: 254.20407152175903
batch: 2892
train_loss: 256.9204332828522
batch: 2893
train_loss: 259.54552698135376
batch: 2894
train_loss: 262.17458033561707
batch: 2895
train_loss: 264.8661904335022
batch: 2896
train_loss: 267.5294578075409
batch: 2897
train_loss: 270.1925051212311
batch: 2898
train_loss: 272.76797342300415
batch: 2899
train_loss: 275.3724491596222
batch: 2900
train_loss: 277.9352412223816
batch: 2901
train_loss: 280.6491310596466
batch: 2902
train_loss: 283.35283279418945
batch: 2903
train_loss: 285.9963867664337
batch: 2904
train_loss: 288.6344156265259
batch: 2905
train_loss: 291.2185745239258
batch: 2906
train_loss: 293.8486897945404
batch: 2907
train_loss: 296.5214161872864
batch: 2908
train_loss: 299.2753303050995
batch: 2909
train_loss: 301.9552824497223
batch: 2910
train_loss: 304.54370045661926
batch: 2911
train_loss: 307.2556834220886
batch: 2912
train_loss: 309.8809678554535
batch: 2913
train_loss: 312.6337990760803
batch: 2914
train_loss: 315.2703003883362
batch: 2915
train_loss: 318.01538825035095
batch: 2916
train_loss: 320.75504422187805
batch: 2917
train_loss: 323.40241503715515
batch: 2918
train_loss: 326.0656497478485
batch: 2919
train_loss: 328.719899892807
batch: 2920
train_loss: 331.3876097202301
batch: 2921
train_loss: 334.0097668170929
batch: 2922
train_loss: 336.7252526283264
batch: 2923
train_loss: 339.4364688396454
batch: 2924
train_loss: 342.1674904823303
batch: 2925
train_loss: 344.80743050575256
batch: 2926
train_loss: 347.4081599712372
batch: 2927
train_loss: 350.1500358581543
batch: 2928
train_loss: 352.8609952926636
batch: 2929
train_loss: 355.6951267719269
batch: 2930
train_loss: 358.4723708629608
batch: 2931
train_loss: 361.19784903526306
batch: 2932
train_loss: 363.91542983055115
batch: 2933
train_loss: 366.5955514907837
batch: 2934
train_loss: 369.27151799201965
batch: 2935
train_loss: 371.9622094631195
batch: 2936
train_loss: 374.7261164188385
batch: 2937
train_loss: 377.4941565990448
batch: 2938
train_loss: 380.22618103027344
batch: 2939
train_loss: 382.99412178993225
batch: 2940
train_loss: 385.75165486335754
batch: 2941
train_loss: 388.48950147628784
batch: 2942
train_loss: 391.05825901031494
batch: 2943
train_loss: 393.74307012557983
batch: 2944
train_loss: 396.37846398353577
batch: 2945
train_loss: 399.0593991279602
batch: 2946
train_loss: 401.775358915329
batch: 2947
train_loss: 404.42347049713135
batch: 2948
train_loss: 407.056925535202
batch: 2949
train_loss: 409.76473474502563
batch: 2950
train_loss: 412.4362380504608
batch: 2951
train_loss: 415.0649423599243
batch: 2952
train_loss: 417.7201073169708
batch: 2953
train_loss: 420.3234751224518
batch: 2954
train_loss: 423.0115668773651
batch: 2955
train_loss: 425.7363512516022
batch: 2956
train_loss: 428.41860604286194
batch: 2957
train_loss: 431.0778133869171
batch: 2958
train_loss: 433.8203616142273
batch: 2959
train_loss: 436.56354546546936
batch: 2960
train_loss: 439.259206533432
batch: 2961
train_loss: 441.8959174156189
batch: 2962
train_loss: 444.5614583492279
batch: 2963
train_loss: 447.2078456878662
batch: 2964
train_loss: 449.89756178855896
batch: 2965
train_loss: 452.6255877017975
batch: 2966
train_loss: 455.3600072860718
batch: 2967
train_loss: 458.1520462036133
batch: 2968
train_loss: 460.8209993839264
batch: 2969
train_loss: 463.47673082351685
batch: 2970
train_loss: 466.12577414512634
batch: 2971
train_loss: 468.85095596313477
batch: 2972
train_loss: 471.47195863723755
batch: 2973
train_loss: 474.141236782074
batch: 2974
train_loss: 476.83549189567566
batch: 2975
train_loss: 479.6385123729706
batch: 2976
train_loss: 482.36222100257874
batch: 2977
train_loss: 485.10757207870483
batch: 2978
train_loss: 487.7801423072815
batch: 2979
train_loss: 490.3958070278168
batch: 2980
train_loss: 493.12505173683167
batch: 2981
train_loss: 495.8279023170471
batch: 2982
train_loss: 498.51039457321167
batch: 2983
train_loss: 501.216495513916
batch: 2984
train_loss: 503.9237554073334
batch: 2985
train_loss: 506.70149207115173
batch: 2986
train_loss: 509.44652247428894
batch: 2987
train_loss: 512.1610848903656
batch: 2988
train_loss: 514.8883261680603
batch: 2989
train_loss: 517.5048835277557
batch: 2990
train_loss: 520.2249648571014
batch: 2991
train_loss: 523.0530710220337
batch: 2992
train_loss: 525.864503622055
batch: 2993
train_loss: 528.5933151245117
batch: 2994
train_loss: 531.4210438728333
batch: 2995
train_loss: 534.2950854301453
batch: 2996
train_loss: 537.0895791053772
batch: 2997
train_loss: 539.8465013504028
batch: 2998
train_loss: 542.7385184764862
batch: 2999
train_loss: 545.4943971633911
| epoch   0 step     3000 |   3000 batches | lr 0.000247 | ms/batch 347.78 | loss  2.73 | bpc   3.93491
batch: 3000
train_loss: 2.7853636741638184
batch: 3001
train_loss: 5.472788333892822
batch: 3002
train_loss: 8.09943175315857
batch: 3003
train_loss: 10.808724880218506
batch: 3004
train_loss: 13.617945194244385
batch: 3005
train_loss: 16.31918978691101
batch: 3006
train_loss: 19.091593027114868
batch: 3007
train_loss: 21.83972930908203
batch: 3008
train_loss: 24.571162939071655
batch: 3009
train_loss: 27.38443946838379
batch: 3010
train_loss: 30.20762038230896
batch: 3011
train_loss: 33.05470871925354
batch: 3012
train_loss: 35.937753677368164
batch: 3013
train_loss: 38.73821043968201
batch: 3014
train_loss: 41.55084037780762
batch: 3015
train_loss: 44.39383888244629
batch: 3016
train_loss: 47.04833650588989
batch: 3017
train_loss: 49.77673029899597
batch: 3018
train_loss: 52.5712513923645
batch: 3019
train_loss: 55.35015821456909
batch: 3020
train_loss: 57.96763563156128
batch: 3021
train_loss: 60.764594316482544
batch: 3022
train_loss: 63.44151830673218
batch: 3023
train_loss: 66.27942633628845
batch: 3024
train_loss: 69.06572031974792
batch: 3025
train_loss: 71.84130096435547
batch: 3026
train_loss: 74.63830018043518
batch: 3027
train_loss: 77.31847810745239
batch: 3028
train_loss: 79.90702652931213
batch: 3029
train_loss: 82.62399506568909
batch: 3030
train_loss: 85.37352776527405
batch: 3031
train_loss: 88.07737469673157
batch: 3032
train_loss: 90.79415822029114
batch: 3033
train_loss: 93.59690523147583
batch: 3034
train_loss: 96.40597152709961
batch: 3035
train_loss: 99.17283964157104
batch: 3036
train_loss: 101.89561033248901
batch: 3037
train_loss: 104.63461089134216
batch: 3038
train_loss: 107.41818261146545
batch: 3039
train_loss: 110.09584617614746
batch: 3040
train_loss: 112.81323885917664
batch: 3041
train_loss: 115.58237504959106
batch: 3042
train_loss: 118.26138186454773
batch: 3043
train_loss: 121.0362651348114
batch: 3044
train_loss: 123.77248167991638
batch: 3045
train_loss: 126.5328917503357
batch: 3046
train_loss: 129.28213572502136
batch: 3047
train_loss: 132.03156900405884
batch: 3048
train_loss: 134.67795419692993
batch: 3049
train_loss: 137.36372637748718
batch: 3050
train_loss: 140.08603835105896
batch: 3051
train_loss: 142.79741501808167
batch: 3052
train_loss: 145.51580095291138
batch: 3053
train_loss: 148.26365566253662
batch: 3054
train_loss: 150.98146748542786
batch: 3055
train_loss: 153.64917922019958
batch: 3056
train_loss: 156.36977410316467
batch: 3057
train_loss: 158.99918866157532
batch: 3058
train_loss: 161.64778351783752
batch: 3059
train_loss: 164.2898235321045
batch: 3060
train_loss: 166.99510979652405
batch: 3061
train_loss: 169.6974639892578
batch: 3062
train_loss: 172.4205675125122
batch: 3063
train_loss: 175.04762268066406
batch: 3064
train_loss: 177.6646978855133
batch: 3065
train_loss: 180.27307081222534
batch: 3066
train_loss: 183.0165901184082
batch: 3067
train_loss: 185.71947073936462
batch: 3068
train_loss: 188.31551694869995
batch: 3069
train_loss: 190.9502682685852
batch: 3070
train_loss: 193.66776847839355
batch: 3071
train_loss: 196.4322328567505
batch: 3072
train_loss: 199.1251926422119
batch: 3073
train_loss: 201.82307958602905
batch: 3074
train_loss: 204.515878200531
batch: 3075
train_loss: 207.19880986213684
batch: 3076
train_loss: 209.78891587257385
batch: 3077
train_loss: 212.3739025592804
batch: 3078
train_loss: 215.0233941078186
batch: 3079
train_loss: 217.86659407615662
batch: 3080
train_loss: 220.55045986175537
batch: 3081
train_loss: 223.2328040599823
batch: 3082
train_loss: 225.95919132232666
batch: 3083
train_loss: 228.6637966632843
batch: 3084
train_loss: 231.40858554840088
batch: 3085
train_loss: 234.09772515296936
batch: 3086
train_loss: 236.75043201446533
batch: 3087
train_loss: 239.49055361747742
batch: 3088
train_loss: 242.23253798484802
batch: 3089
train_loss: 244.88450121879578
batch: 3090
train_loss: 247.55431294441223
batch: 3091
train_loss: 250.19114232063293
batch: 3092
train_loss: 252.74274897575378
batch: 3093
train_loss: 255.49130630493164
batch: 3094
train_loss: 258.1747784614563
batch: 3095
train_loss: 260.78657746315
batch: 3096
train_loss: 263.53374004364014
batch: 3097
train_loss: 266.2845814228058
batch: 3098
train_loss: 269.01484632492065
batch: 3099
train_loss: 271.7577631473541
batch: 3100
train_loss: 274.453250169754
batch: 3101
train_loss: 277.1742525100708
batch: 3102
train_loss: 279.8599078655243
batch: 3103
train_loss: 282.55301809310913
batch: 3104
train_loss: 285.29585909843445
batch: 3105
train_loss: 287.9320168495178
batch: 3106
train_loss: 290.7005994319916
batch: 3107
train_loss: 293.42676877975464
batch: 3108
train_loss: 296.11210799217224
batch: 3109
train_loss: 298.7827219963074
batch: 3110
train_loss: 301.5761420726776
batch: 3111
train_loss: 304.3642792701721
batch: 3112
train_loss: 307.1336119174957
batch: 3113
train_loss: 309.8679287433624
batch: 3114
train_loss: 312.64689087867737
batch: 3115
train_loss: 315.42094445228577
batch: 3116
train_loss: 318.14192056655884
batch: 3117
train_loss: 320.842408657074
batch: 3118
train_loss: 323.6417467594147
batch: 3119
train_loss: 326.3236346244812
batch: 3120
train_loss: 329.01231265068054
batch: 3121
train_loss: 331.7290458679199
batch: 3122
train_loss: 334.5127875804901
batch: 3123
train_loss: 337.3623032569885
batch: 3124
train_loss: 340.1161255836487
batch: 3125
train_loss: 342.83700585365295
batch: 3126
train_loss: 345.52145075798035
batch: 3127
train_loss: 348.2098333835602
batch: 3128
train_loss: 350.8601279258728
batch: 3129
train_loss: 353.4385578632355
batch: 3130
train_loss: 356.06751704216003
batch: 3131
train_loss: 358.6861641407013
batch: 3132
train_loss: 361.38031005859375
batch: 3133
train_loss: 363.98410201072693
batch: 3134
train_loss: 366.693261384964
batch: 3135
train_loss: 369.36134934425354
batch: 3136
train_loss: 371.9730896949768
batch: 3137
train_loss: 374.60831212997437
batch: 3138
train_loss: 377.2677240371704
batch: 3139
train_loss: 379.96963930130005
batch: 3140
train_loss: 382.6910650730133
batch: 3141
train_loss: 385.45538091659546
batch: 3142
train_loss: 388.0623526573181
batch: 3143
train_loss: 390.7568235397339
batch: 3144
train_loss: 393.457279920578
batch: 3145
train_loss: 396.15056562423706
batch: 3146
train_loss: 398.8499767780304
batch: 3147
train_loss: 401.5388171672821
batch: 3148
train_loss: 404.19133043289185
batch: 3149
train_loss: 406.87945461273193
batch: 3150
train_loss: 409.5240514278412
batch: 3151
train_loss: 412.2617697715759
batch: 3152
train_loss: 414.9251708984375
batch: 3153
train_loss: 417.5486671924591
batch: 3154
train_loss: 420.229266166687
batch: 3155
train_loss: 422.8753728866577
batch: 3156
train_loss: 425.5760154724121
batch: 3157
train_loss: 428.3275043964386
batch: 3158
train_loss: 431.001558303833
batch: 3159
train_loss: 433.59879541397095
batch: 3160
train_loss: 436.2762875556946
batch: 3161
train_loss: 438.9750692844391
batch: 3162
train_loss: 441.5784213542938
batch: 3163
train_loss: 444.3624837398529
batch: 3164
train_loss: 447.0903887748718
batch: 3165
train_loss: 449.8028917312622
batch: 3166
train_loss: 452.5071430206299
batch: 3167
train_loss: 455.24464082717896
batch: 3168
train_loss: 458.0716128349304
batch: 3169
train_loss: 460.83029866218567
batch: 3170
train_loss: 463.72091817855835
batch: 3171
train_loss: 466.5162408351898
batch: 3172
train_loss: 469.22986125946045
batch: 3173
train_loss: 471.90334129333496
batch: 3174
train_loss: 474.6051547527313
batch: 3175
train_loss: 477.2857837677002
batch: 3176
train_loss: 480.01623344421387
batch: 3177
train_loss: 482.72058391571045
batch: 3178
train_loss: 485.39904022216797
batch: 3179
train_loss: 488.021258354187
batch: 3180
train_loss: 490.7551918029785
batch: 3181
train_loss: 493.34822058677673
batch: 3182
train_loss: 496.0135464668274
batch: 3183
train_loss: 498.79403495788574
batch: 3184
train_loss: 501.4955794811249
batch: 3185
train_loss: 504.1158866882324
batch: 3186
train_loss: 506.8458306789398
batch: 3187
train_loss: 509.50651240348816
batch: 3188
train_loss: 512.0936300754547
batch: 3189
train_loss: 514.7544543743134
batch: 3190
train_loss: 517.3946673870087
batch: 3191
train_loss: 520.0704355239868
batch: 3192
train_loss: 522.6030294895172
batch: 3193
train_loss: 525.1601963043213
batch: 3194
train_loss: 527.782190322876
batch: 3195
train_loss: 530.4610834121704
batch: 3196
train_loss: 533.0495882034302
batch: 3197
train_loss: 535.6697444915771
batch: 3198
train_loss: 538.2722618579865
batch: 3199
train_loss: 540.873997926712
| epoch   0 step     3200 |   3200 batches | lr 0.000246 | ms/batch 347.70 | loss  2.70 | bpc   3.90158
batch: 3200
train_loss: 2.604504346847534
batch: 3201
train_loss: 5.1789562702178955
batch: 3202
train_loss: 7.881949186325073
batch: 3203
train_loss: 10.461106538772583
batch: 3204
train_loss: 13.058722972869873
batch: 3205
train_loss: 15.713122606277466
batch: 3206
train_loss: 18.27838897705078
batch: 3207
train_loss: 20.95893168449402
batch: 3208
train_loss: 23.562499046325684
batch: 3209
train_loss: 26.154884815216064
batch: 3210
train_loss: 28.728535413742065
batch: 3211
train_loss: 31.438520669937134
batch: 3212
train_loss: 34.16999673843384
batch: 3213
train_loss: 36.89858436584473
batch: 3214
train_loss: 39.60533261299133
batch: 3215
train_loss: 42.38621640205383
batch: 3216
train_loss: 45.175339221954346
batch: 3217
train_loss: 47.971495151519775
batch: 3218
train_loss: 50.6375515460968
batch: 3219
train_loss: 53.275479555130005
batch: 3220
train_loss: 56.038222551345825
batch: 3221
train_loss: 58.786288261413574
batch: 3222
train_loss: 61.494730710983276
batch: 3223
train_loss: 64.1845772266388
batch: 3224
train_loss: 66.84427523612976
batch: 3225
train_loss: 69.46960473060608
batch: 3226
train_loss: 72.20436596870422
batch: 3227
train_loss: 74.9068660736084
batch: 3228
train_loss: 77.58074283599854
batch: 3229
train_loss: 80.2985897064209
batch: 3230
train_loss: 82.92580962181091
batch: 3231
train_loss: 85.58200216293335
batch: 3232
train_loss: 88.24235415458679
batch: 3233
train_loss: 90.85994458198547
batch: 3234
train_loss: 93.59591770172119
batch: 3235
train_loss: 96.32564043998718
batch: 3236
train_loss: 99.15977311134338
batch: 3237
train_loss: 101.85389852523804
batch: 3238
train_loss: 104.49930548667908
batch: 3239
train_loss: 107.33019399642944
batch: 3240
train_loss: 110.02785325050354
batch: 3241
train_loss: 112.7313985824585
batch: 3242
train_loss: 115.39869427680969
batch: 3243
train_loss: 118.07012438774109
batch: 3244
train_loss: 120.7002124786377
batch: 3245
train_loss: 123.42546129226685
batch: 3246
train_loss: 126.07984828948975
batch: 3247
train_loss: 128.76716232299805
batch: 3248
train_loss: 131.51104712486267
batch: 3249
train_loss: 134.2314977645874
batch: 3250
train_loss: 136.9772560596466
batch: 3251
train_loss: 139.57784795761108
batch: 3252
train_loss: 142.2204384803772
batch: 3253
train_loss: 144.9450283050537
batch: 3254
train_loss: 147.70335936546326
batch: 3255
train_loss: 150.42096829414368
batch: 3256
train_loss: 153.09746193885803
batch: 3257
train_loss: 155.8624939918518
batch: 3258
train_loss: 158.58859181404114
batch: 3259
train_loss: 161.44746112823486
batch: 3260
train_loss: 164.2358238697052
batch: 3261
train_loss: 167.14900827407837
batch: 3262
train_loss: 169.90157771110535
batch: 3263
train_loss: 172.67625975608826
batch: 3264
train_loss: 175.35328030586243
batch: 3265
train_loss: 178.14541721343994
batch: 3266
train_loss: 180.8722641468048
batch: 3267
train_loss: 183.6186785697937
batch: 3268
train_loss: 186.21060729026794
batch: 3269
train_loss: 188.85300970077515
batch: 3270
train_loss: 191.5974178314209
batch: 3271
train_loss: 194.38178133964539
batch: 3272
train_loss: 197.09682393074036
batch: 3273
train_loss: 199.83501052856445
batch: 3274
train_loss: 202.5450291633606
batch: 3275
train_loss: 205.32626867294312
batch: 3276
train_loss: 208.02497458457947
batch: 3277
train_loss: 210.66608881950378
batch: 3278
train_loss: 213.28279638290405
batch: 3279
train_loss: 215.9362189769745
batch: 3280
train_loss: 218.61894965171814
batch: 3281
train_loss: 221.21638107299805
batch: 3282
train_loss: 223.87429785728455
batch: 3283
train_loss: 226.53208422660828
batch: 3284
train_loss: 229.18113327026367
batch: 3285
train_loss: 231.91716694831848
batch: 3286
train_loss: 234.78333377838135
batch: 3287
train_loss: 237.5421142578125
batch: 3288
train_loss: 240.2315375804901
batch: 3289
train_loss: 242.9640440940857
batch: 3290
train_loss: 245.62395095825195
batch: 3291
train_loss: 248.28260278701782
batch: 3292
train_loss: 250.97853446006775
batch: 3293
train_loss: 253.7004849910736
batch: 3294
train_loss: 256.36989974975586
batch: 3295
train_loss: 258.9797623157501
batch: 3296
train_loss: 261.60225462913513
batch: 3297
train_loss: 264.26551246643066
batch: 3298
train_loss: 266.9382243156433
batch: 3299
train_loss: 269.6411201953888
batch: 3300
train_loss: 272.24858570098877
batch: 3301
train_loss: 274.89605712890625
batch: 3302
train_loss: 277.5689375400543
batch: 3303
train_loss: 280.25517177581787
batch: 3304
train_loss: 283.11758399009705
batch: 3305
train_loss: 285.8421332836151
batch: 3306
train_loss: 288.5205056667328
batch: 3307
train_loss: 291.2196545600891
batch: 3308
train_loss: 293.81401801109314
batch: 3309
train_loss: 296.41554069519043
batch: 3310
train_loss: 299.0850431919098
batch: 3311
train_loss: 301.8079798221588
batch: 3312
train_loss: 304.39158606529236
batch: 3313
train_loss: 307.09988617897034
batch: 3314
train_loss: 309.791951417923
batch: 3315
train_loss: 312.4510281085968
batch: 3316
train_loss: 315.14446091651917
batch: 3317
train_loss: 317.68012738227844
batch: 3318
train_loss: 320.2773861885071
batch: 3319
train_loss: 322.97934103012085
batch: 3320
train_loss: 325.6244466304779
batch: 3321
train_loss: 328.3841407299042
batch: 3322
train_loss: 331.07672095298767
batch: 3323
train_loss: 333.7117729187012
batch: 3324
train_loss: 336.4169728755951
batch: 3325
train_loss: 339.18558621406555
batch: 3326
train_loss: 341.89292430877686
batch: 3327
train_loss: 344.57571172714233
batch: 3328
train_loss: 347.27894377708435
batch: 3329
train_loss: 350.0324764251709
batch: 3330
train_loss: 352.8183362483978
batch: 3331
train_loss: 355.58341240882874
batch: 3332
train_loss: 358.31390261650085
batch: 3333
train_loss: 361.10203218460083
batch: 3334
train_loss: 363.965487241745
batch: 3335
train_loss: 366.696311712265
batch: 3336
train_loss: 369.3543257713318
batch: 3337
train_loss: 372.00774693489075
batch: 3338
train_loss: 374.7584686279297
batch: 3339
train_loss: 377.5171959400177
batch: 3340
train_loss: 380.2674217224121
batch: 3341
train_loss: 382.9284346103668
batch: 3342
train_loss: 385.5762252807617
batch: 3343
train_loss: 388.30649614334106
batch: 3344
train_loss: 391.0271382331848
batch: 3345
train_loss: 393.9141438007355
batch: 3346
train_loss: 396.7713358402252
batch: 3347
train_loss: 399.49730801582336
batch: 3348
train_loss: 402.24776554107666
batch: 3349
train_loss: 405.0439999103546
batch: 3350
train_loss: 407.772403717041
batch: 3351
train_loss: 410.5408320426941
batch: 3352
train_loss: 413.35491967201233
batch: 3353
train_loss: 416.07261753082275
batch: 3354
train_loss: 418.6719722747803
batch: 3355
train_loss: 421.43445205688477
batch: 3356
train_loss: 424.165549993515
batch: 3357
train_loss: 426.79404878616333
batch: 3358
train_loss: 429.5590658187866
batch: 3359
train_loss: 432.23985505104065
batch: 3360
train_loss: 434.91832160949707
batch: 3361
train_loss: 437.6086947917938
batch: 3362
train_loss: 440.3504750728607
batch: 3363
train_loss: 443.0151171684265
batch: 3364
train_loss: 445.65629839897156
batch: 3365
train_loss: 448.2751717567444
batch: 3366
train_loss: 450.93056535720825
batch: 3367
train_loss: 453.6149687767029
batch: 3368
train_loss: 456.2380225658417
batch: 3369
train_loss: 458.92978262901306
batch: 3370
train_loss: 461.73657989501953
batch: 3371
train_loss: 464.46329402923584
batch: 3372
train_loss: 467.16053581237793
batch: 3373
train_loss: 469.8492486476898
batch: 3374
train_loss: 472.4657142162323
batch: 3375
train_loss: 475.1679615974426
batch: 3376
train_loss: 477.80273485183716
batch: 3377
train_loss: 480.53998851776123
batch: 3378
train_loss: 483.2639305591583
batch: 3379
train_loss: 485.92481660842896
batch: 3380
train_loss: 488.57836723327637
batch: 3381
train_loss: 491.3181703090668
batch: 3382
train_loss: 494.1232488155365
batch: 3383
train_loss: 496.931499004364
batch: 3384
train_loss: 499.7216262817383
batch: 3385
train_loss: 502.4069459438324
batch: 3386
train_loss: 505.0866391658783
batch: 3387
train_loss: 507.8306839466095
batch: 3388
train_loss: 510.5675814151764
batch: 3389
train_loss: 513.2222862243652
batch: 3390
train_loss: 515.8332161903381
batch: 3391
train_loss: 518.416451215744
batch: 3392
train_loss: 521.0624494552612
batch: 3393
train_loss: 523.7417843341827
batch: 3394
train_loss: 526.3942172527313
batch: 3395
train_loss: 528.9728872776031
batch: 3396
train_loss: 531.6542854309082
batch: 3397
train_loss: 534.2879724502563
batch: 3398
train_loss: 536.9717075824738
batch: 3399
train_loss: 539.6353545188904
| epoch   0 step     3400 |   3400 batches | lr 0.000246 | ms/batch 339.22 | loss  2.70 | bpc   3.89265
batch: 3400
train_loss: 2.722195625305176
batch: 3401
train_loss: 5.459928750991821
batch: 3402
train_loss: 8.216090202331543
batch: 3403
train_loss: 10.984202146530151
batch: 3404
train_loss: 13.736241340637207
batch: 3405
train_loss: 16.3857262134552
batch: 3406
train_loss: 19.09676456451416
batch: 3407
train_loss: 21.776079177856445
batch: 3408
train_loss: 24.40385389328003
batch: 3409
train_loss: 27.11438488960266
batch: 3410
train_loss: 29.760273218154907
batch: 3411
train_loss: 32.528303146362305
batch: 3412
train_loss: 35.23167157173157
batch: 3413
train_loss: 37.995184659957886
batch: 3414
train_loss: 40.752408027648926
batch: 3415
train_loss: 43.539630651474
batch: 3416
train_loss: 46.28301000595093
batch: 3417
train_loss: 49.057040214538574
batch: 3418
train_loss: 51.832730531692505
batch: 3419
train_loss: 54.55263328552246
batch: 3420
train_loss: 57.27662372589111
batch: 3421
train_loss: 59.88007926940918
batch: 3422
train_loss: 62.59881377220154
batch: 3423
train_loss: 65.28068423271179
batch: 3424
train_loss: 67.9536485671997
batch: 3425
train_loss: 70.6251175403595
batch: 3426
train_loss: 73.23310256004333
batch: 3427
train_loss: 75.90233778953552
batch: 3428
train_loss: 78.53412961959839
batch: 3429
train_loss: 81.22448134422302
batch: 3430
train_loss: 83.94363975524902
batch: 3431
train_loss: 86.65476655960083
batch: 3432
train_loss: 89.32928609848022
batch: 3433
train_loss: 92.01670241355896
batch: 3434
train_loss: 94.62720108032227
batch: 3435
train_loss: 97.36164402961731
batch: 3436
train_loss: 100.00748586654663
batch: 3437
train_loss: 102.77649974822998
batch: 3438
train_loss: 105.44019794464111
batch: 3439
train_loss: 108.03067636489868
batch: 3440
train_loss: 110.63910388946533
batch: 3441
train_loss: 113.26314043998718
batch: 3442
train_loss: 115.8660683631897
batch: 3443
train_loss: 118.46351075172424
batch: 3444
train_loss: 121.0923101902008
batch: 3445
train_loss: 123.60628509521484
batch: 3446
train_loss: 126.21155071258545
batch: 3447
train_loss: 128.9026198387146
batch: 3448
train_loss: 131.47943949699402
batch: 3449
train_loss: 134.00787925720215
batch: 3450
train_loss: 136.64794254302979
batch: 3451
train_loss: 139.2675609588623
batch: 3452
train_loss: 141.9116988182068
batch: 3453
train_loss: 144.6588945388794
batch: 3454
train_loss: 147.13385772705078
batch: 3455
train_loss: 149.60910558700562
batch: 3456
train_loss: 152.14897060394287
batch: 3457
train_loss: 154.6567451953888
batch: 3458
train_loss: 157.17095756530762
batch: 3459
train_loss: 159.73486232757568
batch: 3460
train_loss: 162.32362699508667
batch: 3461
train_loss: 164.91584706306458
batch: 3462
train_loss: 167.44892954826355
batch: 3463
train_loss: 170.01011657714844
batch: 3464
train_loss: 172.63601994514465
batch: 3465
train_loss: 175.24549436569214
batch: 3466
train_loss: 177.86433148384094
batch: 3467
train_loss: 180.54670357704163
batch: 3468
train_loss: 183.27491116523743
batch: 3469
train_loss: 185.96464681625366
batch: 3470
train_loss: 188.63235020637512
batch: 3471
train_loss: 191.20738172531128
batch: 3472
train_loss: 193.78061270713806
batch: 3473
train_loss: 196.28231287002563
batch: 3474
train_loss: 198.85080480575562
batch: 3475
train_loss: 201.50839829444885
batch: 3476
train_loss: 204.1439893245697
batch: 3477
train_loss: 206.79518675804138
batch: 3478
train_loss: 209.4525923728943
batch: 3479
train_loss: 211.9782178401947
batch: 3480
train_loss: 214.55969071388245
batch: 3481
train_loss: 217.24962544441223
batch: 3482
train_loss: 219.82842469215393
batch: 3483
train_loss: 222.36749696731567
batch: 3484
train_loss: 224.9485583305359
batch: 3485
train_loss: 227.54840207099915
batch: 3486
train_loss: 230.06932497024536
batch: 3487
train_loss: 232.6579465866089
batch: 3488
train_loss: 235.1514174938202
batch: 3489
train_loss: 237.73915791511536
batch: 3490
train_loss: 240.3199007511139
batch: 3491
train_loss: 242.82006907463074
batch: 3492
train_loss: 245.3502278327942
batch: 3493
train_loss: 247.9494025707245
batch: 3494
train_loss: 250.556782245636
batch: 3495
train_loss: 253.2234501838684
batch: 3496
train_loss: 255.7908070087433
batch: 3497
train_loss: 258.34047770500183
batch: 3498
train_loss: 260.92034339904785
batch: 3499
train_loss: 263.5106942653656
batch: 3500
train_loss: 266.12095046043396
batch: 3501
train_loss: 268.66982102394104
batch: 3502
train_loss: 271.30742383003235
batch: 3503
train_loss: 273.82954621315
batch: 3504
train_loss: 276.35721158981323
batch: 3505
train_loss: 278.94835233688354
batch: 3506
train_loss: 281.5646333694458
batch: 3507
train_loss: 284.13595271110535
batch: 3508
train_loss: 286.67250967025757
batch: 3509
train_loss: 289.28602623939514
batch: 3510
train_loss: 291.9823422431946
batch: 3511
train_loss: 294.68122696876526
batch: 3512
train_loss: 297.37363362312317
batch: 3513
train_loss: 300.10216069221497
batch: 3514
train_loss: 302.72673058509827
batch: 3515
train_loss: 305.3386654853821
batch: 3516
train_loss: 307.9158651828766
batch: 3517
train_loss: 310.6025731563568
batch: 3518
train_loss: 313.4217383861542
batch: 3519
train_loss: 316.2067565917969
batch: 3520
train_loss: 318.8330466747284
batch: 3521
train_loss: 321.45714807510376
batch: 3522
train_loss: 324.03015422821045
batch: 3523
train_loss: 326.7038915157318
batch: 3524
train_loss: 329.31326174736023
batch: 3525
train_loss: 331.76860332489014
batch: 3526
train_loss: 334.28687286376953
batch: 3527
train_loss: 336.86842012405396
batch: 3528
train_loss: 339.29936170578003
batch: 3529
train_loss: 341.9042155742645
batch: 3530
train_loss: 344.60915327072144
batch: 3531
train_loss: 347.2354505062103
batch: 3532
train_loss: 349.8384144306183
batch: 3533
train_loss: 352.41690707206726
batch: 3534
train_loss: 355.0235300064087
batch: 3535
train_loss: 357.6138050556183
batch: 3536
train_loss: 360.1860806941986
batch: 3537
train_loss: 362.76947140693665
batch: 3538
train_loss: 365.3017773628235
batch: 3539
train_loss: 367.8131625652313
batch: 3540
train_loss: 370.4168322086334
batch: 3541
train_loss: 373.03356671333313
batch: 3542
train_loss: 375.73952436447144
batch: 3543
train_loss: 378.3884553909302
batch: 3544
train_loss: 380.93959403038025
batch: 3545
train_loss: 383.52590703964233
batch: 3546
train_loss: 386.1008687019348
batch: 3547
train_loss: 388.6942458152771
batch: 3548
train_loss: 391.291396856308
batch: 3549
train_loss: 393.9227509498596
batch: 3550
train_loss: 396.4849944114685
batch: 3551
train_loss: 399.117244720459
batch: 3552
train_loss: 401.80008244514465
batch: 3553
train_loss: 404.4119257926941
batch: 3554
train_loss: 407.0920605659485
batch: 3555
train_loss: 409.7416818141937
batch: 3556
train_loss: 412.4721393585205
batch: 3557
train_loss: 415.1273899078369
batch: 3558
train_loss: 417.73806166648865
batch: 3559
train_loss: 420.4161846637726
batch: 3560
train_loss: 423.1342477798462
batch: 3561
train_loss: 425.80132818222046
batch: 3562
train_loss: 428.54622745513916
batch: 3563
train_loss: 431.2022087574005
batch: 3564
train_loss: 433.80188059806824
batch: 3565
train_loss: 436.4588108062744
batch: 3566
train_loss: 439.12588930130005
batch: 3567
train_loss: 441.70320868492126
batch: 3568
train_loss: 444.3537540435791
batch: 3569
train_loss: 446.9796335697174
batch: 3570
train_loss: 449.6786251068115
batch: 3571
train_loss: 452.46326541900635
batch: 3572
train_loss: 455.026034116745
batch: 3573
train_loss: 457.65269351005554
batch: 3574
train_loss: 460.3210828304291
batch: 3575
train_loss: 463.0437352657318
batch: 3576
train_loss: 465.67342376708984
batch: 3577
train_loss: 468.298796415329
batch: 3578
train_loss: 470.97700214385986
batch: 3579
train_loss: 473.6650764942169
batch: 3580
train_loss: 476.3434684276581
batch: 3581
train_loss: 478.86927938461304
batch: 3582
train_loss: 481.43077993392944
batch: 3583
train_loss: 484.07890367507935
batch: 3584
train_loss: 486.8377528190613
batch: 3585
train_loss: 489.50293588638306
batch: 3586
train_loss: 492.1292426586151
batch: 3587
train_loss: 494.7185072898865
batch: 3588
train_loss: 497.29809641838074
batch: 3589
train_loss: 499.83745288848877
batch: 3590
train_loss: 502.3456962108612
batch: 3591
train_loss: 504.9650731086731
batch: 3592
train_loss: 507.6501040458679
batch: 3593
train_loss: 510.2673907279968
batch: 3594
train_loss: 512.8997387886047
batch: 3595
train_loss: 515.5366666316986
batch: 3596
train_loss: 518.2065579891205
batch: 3597
train_loss: 520.8737666606903
batch: 3598
train_loss: 523.5752048492432
batch: 3599
train_loss: 526.1490905284882
| epoch   0 step     3600 |   3600 batches | lr 0.000245 | ms/batch 337.05 | loss  2.63 | bpc   3.79536
batch: 3600
train_loss: 2.7110698223114014
batch: 3601
train_loss: 5.398830413818359
batch: 3602
train_loss: 8.0939621925354
batch: 3603
train_loss: 10.716570854187012
batch: 3604
train_loss: 13.320927619934082
batch: 3605
train_loss: 16.067579984664917
batch: 3606
train_loss: 18.71839141845703
batch: 3607
train_loss: 21.379502296447754
batch: 3608
train_loss: 24.091252326965332
batch: 3609
train_loss: 26.788328170776367
batch: 3610
train_loss: 29.459978818893433
batch: 3611
train_loss: 32.11578726768494
batch: 3612
train_loss: 34.91013503074646
batch: 3613
train_loss: 37.741663455963135
batch: 3614
train_loss: 40.65552091598511
batch: 3615
train_loss: 43.38597297668457
batch: 3616
train_loss: 45.99442958831787
batch: 3617
train_loss: 48.684284687042236
batch: 3618
train_loss: 51.34912443161011
batch: 3619
train_loss: 54.06449556350708
batch: 3620
train_loss: 56.638121366500854
batch: 3621
train_loss: 59.44900894165039
batch: 3622
train_loss: 62.128087759017944
batch: 3623
train_loss: 64.81847310066223
batch: 3624
train_loss: 67.47879123687744
batch: 3625
train_loss: 70.07189011573792
batch: 3626
train_loss: 72.61000633239746
batch: 3627
train_loss: 75.26652336120605
batch: 3628
train_loss: 77.89468908309937
batch: 3629
train_loss: 80.58234691619873
batch: 3630
train_loss: 83.23541259765625
batch: 3631
train_loss: 85.79673314094543
batch: 3632
train_loss: 88.36725187301636
batch: 3633
train_loss: 90.98239254951477
batch: 3634
train_loss: 93.60488486289978
batch: 3635
train_loss: 96.19906616210938
batch: 3636
train_loss: 98.75253081321716
batch: 3637
train_loss: 101.41449046134949
batch: 3638
train_loss: 104.12591099739075
batch: 3639
train_loss: 106.72375392913818
batch: 3640
train_loss: 109.48731207847595
batch: 3641
train_loss: 112.1795597076416
batch: 3642
train_loss: 114.82450699806213
batch: 3643
train_loss: 117.46534538269043
batch: 3644
train_loss: 120.15639448165894
batch: 3645
train_loss: 122.85890746116638
batch: 3646
train_loss: 125.61321568489075
batch: 3647
train_loss: 128.3461709022522
batch: 3648
train_loss: 130.97187638282776
batch: 3649
train_loss: 133.64945888519287
batch: 3650
train_loss: 136.3151273727417
batch: 3651
train_loss: 139.07855081558228
batch: 3652
train_loss: 141.74973130226135
batch: 3653
train_loss: 144.45279121398926
batch: 3654
train_loss: 147.07464146614075
batch: 3655
train_loss: 149.6801884174347
batch: 3656
train_loss: 152.3836772441864
batch: 3657
train_loss: 155.10183024406433
batch: 3658
train_loss: 157.93624305725098
batch: 3659
train_loss: 160.61512088775635
batch: 3660
train_loss: 163.3273777961731
batch: 3661
train_loss: 166.16628694534302
batch: 3662
train_loss: 168.9585177898407
batch: 3663
train_loss: 171.6852970123291
batch: 3664
train_loss: 174.5152518749237
batch: 3665
train_loss: 177.2638509273529
batch: 3666
train_loss: 180.01932191848755
batch: 3667
train_loss: 182.7637004852295
batch: 3668
train_loss: 185.44654774665833
batch: 3669
train_loss: 188.2403106689453
batch: 3670
train_loss: 190.94678854942322
batch: 3671
train_loss: 193.6843318939209
batch: 3672
train_loss: 196.31978297233582
batch: 3673
train_loss: 199.09395670890808
batch: 3674
train_loss: 201.8568561077118
batch: 3675
train_loss: 204.59348440170288
batch: 3676
train_loss: 207.29583525657654
batch: 3677
train_loss: 210.06298542022705
batch: 3678
train_loss: 212.62709665298462
batch: 3679
train_loss: 215.32593822479248
batch: 3680
train_loss: 217.8957598209381
batch: 3681
train_loss: 220.43781304359436
batch: 3682
train_loss: 223.10616946220398
batch: 3683
train_loss: 225.94412279129028
batch: 3684
train_loss: 228.71881890296936
batch: 3685
train_loss: 231.4462127685547
batch: 3686
train_loss: 234.07479858398438
batch: 3687
train_loss: 236.68042469024658
batch: 3688
train_loss: 239.28222680091858
batch: 3689
train_loss: 241.99707078933716
batch: 3690
train_loss: 244.70421242713928
batch: 3691
train_loss: 247.32324290275574
batch: 3692
train_loss: 249.94891929626465
batch: 3693
train_loss: 252.66722011566162
batch: 3694
train_loss: 255.38205575942993
batch: 3695
train_loss: 257.98358964920044
batch: 3696
train_loss: 260.5979199409485
batch: 3697
train_loss: 263.16466784477234
batch: 3698
train_loss: 265.8352618217468
batch: 3699
train_loss: 268.51525139808655
batch: 3700
train_loss: 271.0929675102234
batch: 3701
train_loss: 273.77649092674255
batch: 3702
train_loss: 276.4548897743225
batch: 3703
train_loss: 279.0500955581665
batch: 3704
train_loss: 281.74658966064453
batch: 3705
train_loss: 284.6069643497467
batch: 3706
train_loss: 287.34730195999146
batch: 3707
train_loss: 290.15522718429565
batch: 3708
train_loss: 292.8511040210724
batch: 3709
train_loss: 295.54525113105774
batch: 3710
train_loss: 298.29765796661377
batch: 3711
train_loss: 301.07018780708313
batch: 3712
train_loss: 303.8573257923126
batch: 3713
train_loss: 306.6272838115692
batch: 3714
train_loss: 309.3493070602417
batch: 3715
train_loss: 311.9219722747803
batch: 3716
train_loss: 314.71158242225647
batch: 3717
train_loss: 317.4725058078766
batch: 3718
train_loss: 320.1996307373047
batch: 3719
train_loss: 322.89310669898987
batch: 3720
train_loss: 325.5919153690338
batch: 3721
train_loss: 328.21457290649414
batch: 3722
train_loss: 330.9066832065582
batch: 3723
train_loss: 333.59599590301514
batch: 3724
train_loss: 336.25621604919434
batch: 3725
train_loss: 338.89305663108826
batch: 3726
train_loss: 341.64399433135986
batch: 3727
train_loss: 344.35033226013184
batch: 3728
train_loss: 346.91031432151794
batch: 3729
train_loss: 349.4914581775665
batch: 3730
train_loss: 352.19664549827576
batch: 3731
train_loss: 354.8317201137543
batch: 3732
train_loss: 357.4413170814514
batch: 3733
train_loss: 360.0864520072937
batch: 3734
train_loss: 362.827246427536
batch: 3735
train_loss: 365.577002286911
batch: 3736
train_loss: 368.1432180404663
batch: 3737
train_loss: 370.72913789749146
batch: 3738
train_loss: 373.3854556083679
batch: 3739
train_loss: 376.13775300979614
batch: 3740
train_loss: 378.8148012161255
batch: 3741
train_loss: 381.5597813129425
batch: 3742
train_loss: 384.2930498123169
batch: 3743
train_loss: 386.88508653640747
batch: 3744
train_loss: 389.52934527397156
batch: 3745
train_loss: 392.078049659729
batch: 3746
train_loss: 394.69414591789246
batch: 3747
train_loss: 397.35410618782043
batch: 3748
train_loss: 399.894478559494
batch: 3749
train_loss: 402.51305389404297
batch: 3750
train_loss: 405.2316610813141
batch: 3751
train_loss: 407.821994304657
batch: 3752
train_loss: 410.4939968585968
batch: 3753
train_loss: 413.1819760799408
batch: 3754
train_loss: 415.82450556755066
batch: 3755
train_loss: 418.43784761428833
batch: 3756
train_loss: 421.02128314971924
batch: 3757
train_loss: 423.7070527076721
batch: 3758
train_loss: 426.39157700538635
batch: 3759
train_loss: 429.14671444892883
batch: 3760
train_loss: 431.6956362724304
batch: 3761
train_loss: 434.43801641464233
batch: 3762
train_loss: 437.1493675708771
batch: 3763
train_loss: 439.80991220474243
batch: 3764
train_loss: 442.50418972969055
batch: 3765
train_loss: 445.3186128139496
batch: 3766
train_loss: 448.0142295360565
batch: 3767
train_loss: 450.7405285835266
batch: 3768
train_loss: 453.51579427719116
batch: 3769
train_loss: 456.3024275302887
batch: 3770
train_loss: 459.04443883895874
batch: 3771
train_loss: 461.68004083633423
batch: 3772
train_loss: 464.481258392334
batch: 3773
train_loss: 467.1640830039978
batch: 3774
train_loss: 469.7926061153412
batch: 3775
train_loss: 472.4731261730194
batch: 3776
train_loss: 475.0457081794739
batch: 3777
train_loss: 477.7113711833954
batch: 3778
train_loss: 480.289466381073
batch: 3779
train_loss: 482.96582531929016
batch: 3780
train_loss: 485.60303950309753
batch: 3781
train_loss: 488.39279294013977
batch: 3782
train_loss: 491.0705487728119
batch: 3783
train_loss: 493.80014538764954
batch: 3784
train_loss: 496.3995108604431
batch: 3785
train_loss: 499.0385105609894
batch: 3786
train_loss: 501.73122096061707
batch: 3787
train_loss: 504.5054602622986
batch: 3788
train_loss: 507.2682411670685
batch: 3789
train_loss: 509.9867651462555
batch: 3790
train_loss: 512.6610386371613
batch: 3791
train_loss: 515.4129393100739
batch: 3792
train_loss: 518.218786239624
batch: 3793
train_loss: 520.9724843502045
batch: 3794
train_loss: 523.7132909297943
batch: 3795
train_loss: 526.4197163581848
batch: 3796
train_loss: 529.1016438007355
batch: 3797
train_loss: 531.8142492771149
batch: 3798
train_loss: 534.4869766235352
batch: 3799
train_loss: 537.2763075828552
| epoch   0 step     3800 |   3800 batches | lr 0.000244 | ms/batch 337.49 | loss  2.69 | bpc   3.87563
batch: 3800
train_loss: 2.784658670425415
batch: 3801
train_loss: 5.5795652866363525
batch: 3802
train_loss: 8.363301753997803
batch: 3803
train_loss: 11.11288595199585
batch: 3804
train_loss: 13.86570930480957
batch: 3805
train_loss: 16.61893939971924
batch: 3806
train_loss: 19.293970584869385
batch: 3807
train_loss: 21.98883295059204
batch: 3808
train_loss: 24.591400861740112
batch: 3809
train_loss: 27.24695611000061
batch: 3810
train_loss: 29.894951343536377
batch: 3811
train_loss: 32.564594745635986
batch: 3812
train_loss: 35.32729482650757
batch: 3813
train_loss: 38.13192844390869
batch: 3814
train_loss: 40.83570432662964
batch: 3815
train_loss: 43.56389880180359
batch: 3816
train_loss: 46.33152508735657
batch: 3817
train_loss: 48.972070932388306
batch: 3818
train_loss: 51.62748670578003
batch: 3819
train_loss: 54.35161328315735
batch: 3820
train_loss: 57.015984535217285
batch: 3821
train_loss: 59.67768597602844
batch: 3822
train_loss: 62.28284978866577
batch: 3823
train_loss: 64.87549829483032
batch: 3824
train_loss: 67.40441465377808
batch: 3825
train_loss: 69.96984672546387
batch: 3826
train_loss: 72.686763048172
batch: 3827
train_loss: 75.33316612243652
batch: 3828
train_loss: 78.00732183456421
batch: 3829
train_loss: 80.70978426933289
batch: 3830
train_loss: 83.35085964202881
batch: 3831
train_loss: 85.97074699401855
batch: 3832
train_loss: 88.62515544891357
batch: 3833
train_loss: 91.28776240348816
batch: 3834
train_loss: 93.91542053222656
batch: 3835
train_loss: 96.5422203540802
batch: 3836
train_loss: 99.19067358970642
batch: 3837
train_loss: 101.77110266685486
batch: 3838
train_loss: 104.34472799301147
batch: 3839
train_loss: 106.98830437660217
batch: 3840
train_loss: 109.5662043094635
batch: 3841
train_loss: 112.13920998573303
batch: 3842
train_loss: 114.73761749267578
batch: 3843
train_loss: 117.30537533760071
batch: 3844
train_loss: 119.88715600967407
batch: 3845
train_loss: 122.576669216156
batch: 3846
train_loss: 125.16713452339172
batch: 3847
train_loss: 127.69086527824402
batch: 3848
train_loss: 130.2819962501526
batch: 3849
train_loss: 132.79818964004517
batch: 3850
train_loss: 135.35113668441772
batch: 3851
train_loss: 137.96141481399536
batch: 3852
train_loss: 140.56015419960022
batch: 3853
train_loss: 143.09834718704224
batch: 3854
train_loss: 145.6507933139801
batch: 3855
train_loss: 148.25182008743286
batch: 3856
train_loss: 150.87595963478088
batch: 3857
train_loss: 153.51769185066223
batch: 3858
train_loss: 156.15965676307678
batch: 3859
train_loss: 158.7836718559265
batch: 3860
train_loss: 161.32107257843018
batch: 3861
train_loss: 163.9191029071808
batch: 3862
train_loss: 166.47957515716553
batch: 3863
train_loss: 169.16368675231934
batch: 3864
train_loss: 171.8395938873291
batch: 3865
train_loss: 174.50481867790222
batch: 3866
train_loss: 177.02407217025757
batch: 3867
train_loss: 179.64705419540405
batch: 3868
train_loss: 182.3399477005005
batch: 3869
train_loss: 184.9364790916443
batch: 3870
train_loss: 187.74733114242554
batch: 3871
train_loss: 190.4223244190216
batch: 3872
train_loss: 192.99065256118774
batch: 3873
train_loss: 195.61188626289368
batch: 3874
train_loss: 198.2358853816986
batch: 3875
train_loss: 200.90364265441895
batch: 3876
train_loss: 203.5781614780426
batch: 3877
train_loss: 206.15387964248657
batch: 3878
train_loss: 208.699214220047
batch: 3879
train_loss: 211.3145408630371
batch: 3880
train_loss: 213.90914368629456
batch: 3881
train_loss: 216.47600603103638
batch: 3882
train_loss: 219.0904712677002
batch: 3883
train_loss: 221.76888179779053
batch: 3884
train_loss: 224.41774249076843
batch: 3885
train_loss: 227.02758860588074
batch: 3886
train_loss: 229.5980134010315
batch: 3887
train_loss: 232.2139871120453
batch: 3888
train_loss: 234.87850522994995
batch: 3889
train_loss: 237.5157971382141
batch: 3890
train_loss: 240.04643964767456
batch: 3891
train_loss: 242.7717900276184
batch: 3892
train_loss: 245.50108432769775
batch: 3893
train_loss: 248.16462206840515
batch: 3894
train_loss: 250.84436106681824
batch: 3895
train_loss: 253.4773714542389
batch: 3896
train_loss: 256.1591854095459
batch: 3897
train_loss: 258.90278029441833
batch: 3898
train_loss: 261.5699610710144
batch: 3899
train_loss: 264.3276619911194
batch: 3900
train_loss: 267.13284158706665
batch: 3901
train_loss: 269.7418212890625
batch: 3902
train_loss: 272.38604521751404
batch: 3903
train_loss: 274.92562532424927
batch: 3904
train_loss: 277.4398138523102
batch: 3905
train_loss: 280.08762216567993
batch: 3906
train_loss: 282.7755615711212
batch: 3907
train_loss: 285.4745121002197
batch: 3908
train_loss: 288.07279443740845
batch: 3909
train_loss: 290.65979075431824
batch: 3910
train_loss: 293.2187623977661
batch: 3911
train_loss: 295.7229850292206
batch: 3912
train_loss: 298.2152216434479
batch: 3913
train_loss: 300.74034452438354
batch: 3914
train_loss: 303.3158116340637
batch: 3915
train_loss: 305.9630253314972
batch: 3916
train_loss: 308.53863286972046
batch: 3917
train_loss: 311.1283299922943
batch: 3918
train_loss: 313.7013289928436
batch: 3919
train_loss: 316.21788001060486
batch: 3920
train_loss: 318.72371459007263
batch: 3921
train_loss: 321.28017592430115
batch: 3922
train_loss: 323.90286231040955
batch: 3923
train_loss: 326.47278928756714
batch: 3924
train_loss: 328.97105741500854
batch: 3925
train_loss: 331.5416054725647
batch: 3926
train_loss: 334.123482465744
batch: 3927
train_loss: 336.6970670223236
batch: 3928
train_loss: 339.20271039009094
batch: 3929
train_loss: 341.76991081237793
batch: 3930
train_loss: 344.358918428421
batch: 3931
train_loss: 346.9422950744629
batch: 3932
train_loss: 349.6126375198364
batch: 3933
train_loss: 352.21789360046387
batch: 3934
train_loss: 354.79054737091064
batch: 3935
train_loss: 357.4253101348877
batch: 3936
train_loss: 360.08752703666687
batch: 3937
train_loss: 362.6812016963959
batch: 3938
train_loss: 365.37298822402954
batch: 3939
train_loss: 367.9654026031494
batch: 3940
train_loss: 370.6185758113861
batch: 3941
train_loss: 373.16719484329224
batch: 3942
train_loss: 375.73308968544006
batch: 3943
train_loss: 378.3084316253662
batch: 3944
train_loss: 380.9115445613861
batch: 3945
train_loss: 383.55630898475647
batch: 3946
train_loss: 386.11970353126526
batch: 3947
train_loss: 388.69794631004333
batch: 3948
train_loss: 391.41469621658325
batch: 3949
train_loss: 394.01472520828247
batch: 3950
train_loss: 396.5664885044098
batch: 3951
train_loss: 399.13186144828796
batch: 3952
train_loss: 401.70390033721924
batch: 3953
train_loss: 404.32157135009766
batch: 3954
train_loss: 406.8441641330719
batch: 3955
train_loss: 409.4549095630646
batch: 3956
train_loss: 412.1456551551819
batch: 3957
train_loss: 414.79647970199585
batch: 3958
train_loss: 417.3839476108551
batch: 3959
train_loss: 419.98894000053406
batch: 3960
train_loss: 422.6474506855011
batch: 3961
train_loss: 425.3315088748932
batch: 3962
train_loss: 427.8965084552765
batch: 3963
train_loss: 430.4566595554352
batch: 3964
train_loss: 433.0175380706787
batch: 3965
train_loss: 435.709352016449
batch: 3966
train_loss: 438.46465706825256
batch: 3967
train_loss: 441.17712783813477
batch: 3968
train_loss: 443.8959403038025
batch: 3969
train_loss: 446.58717799186707
batch: 3970
train_loss: 449.2773365974426
batch: 3971
train_loss: 451.8442542552948
batch: 3972
train_loss: 454.3634076118469
batch: 3973
train_loss: 456.9426221847534
batch: 3974
train_loss: 459.582151889801
batch: 3975
train_loss: 462.15662956237793
batch: 3976
train_loss: 464.7736361026764
batch: 3977
train_loss: 467.388142824173
batch: 3978
train_loss: 469.95061111450195
batch: 3979
train_loss: 472.51385045051575
batch: 3980
train_loss: 474.9940323829651
batch: 3981
train_loss: 477.4563081264496
batch: 3982
train_loss: 479.9597010612488
batch: 3983
train_loss: 482.5599377155304
batch: 3984
train_loss: 485.21680545806885
batch: 3985
train_loss: 487.7696464061737
batch: 3986
train_loss: 490.31875348091125
batch: 3987
train_loss: 492.9195282459259
batch: 3988
train_loss: 495.5633509159088
batch: 3989
train_loss: 498.19896054267883
batch: 3990
train_loss: 500.7991487979889
batch: 3991
train_loss: 503.37632298469543
batch: 3992
train_loss: 505.9529354572296
batch: 3993
train_loss: 508.61314034461975
batch: 3994
train_loss: 511.21933150291443
batch: 3995
train_loss: 513.8677566051483
batch: 3996
train_loss: 516.5353562831879
batch: 3997
train_loss: 519.1299827098846
batch: 3998
train_loss: 521.7866196632385
batch: 3999
train_loss: 524.4846754074097
| epoch   0 step     4000 |   4000 batches | lr 0.000244 | ms/batch 335.94 | loss  2.62 | bpc   3.78336
