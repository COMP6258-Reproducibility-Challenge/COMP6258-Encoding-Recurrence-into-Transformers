Running SLURM prolog script on pink54.cluster.local
===============================================================================
Job started on Fri May 17 09:52:49 BST 2024
Job ID          : 6000147
Job name        : run_enwik8_rsa_small5.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa_small5.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink54
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 7
    - n_head : 8
    - d_head : 8
    - d_embed : 8
    - d_model : 8
    - d_inner : 32
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 32
    - eval_tgt_len : 16
    - ext_len : 0
    - mem_len : 32
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 24000
    - n_nonemb_param : 22036
====================================================================================================
#params = 24000
#non emb params = 22036
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.308676242828369
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.612464427947998
batch: 2
train_loss: 15.914350032806396
batch: 3
train_loss: 21.22187900543213
batch: 4
train_loss: 26.51562786102295
batch: 5
train_loss: 31.8128023147583
batch: 6
train_loss: 37.105180740356445
batch: 7
train_loss: 42.395357608795166
batch: 8
train_loss: 47.682836055755615
batch: 9
train_loss: 52.96023893356323
batch: 10
train_loss: 58.24455642700195
batch: 11
train_loss: 63.518372535705566
batch: 12
train_loss: 68.78788805007935
batch: 13
train_loss: 74.0510025024414
batch: 14
train_loss: 79.31312704086304
batch: 15
train_loss: 84.56618452072144
batch: 16
train_loss: 89.81805324554443
batch: 17
train_loss: 95.06533861160278
batch: 18
train_loss: 100.30630350112915
batch: 19
train_loss: 105.54376888275146
batch: 20
train_loss: 110.77989149093628
batch: 21
train_loss: 116.01223754882812
batch: 22
train_loss: 121.23942279815674
batch: 23
train_loss: 126.4643201828003
batch: 24
train_loss: 131.68036603927612
batch: 25
train_loss: 136.89602184295654
batch: 26
train_loss: 142.10626459121704
batch: 27
train_loss: 147.3138837814331
batch: 28
train_loss: 152.51614236831665
batch: 29
train_loss: 157.7179675102234
batch: 30
train_loss: 162.915198802948
batch: 31
train_loss: 168.10898876190186
batch: 32
train_loss: 173.30133485794067
batch: 33
train_loss: 178.4888801574707
batch: 34
train_loss: 183.672465801239
batch: 35
train_loss: 188.85439825057983
batch: 36
train_loss: 194.0304675102234
batch: 37
train_loss: 199.19632530212402
batch: 38
train_loss: 204.36558532714844
batch: 39
train_loss: 209.53942680358887
batch: 40
train_loss: 214.7099370956421
batch: 41
train_loss: 219.8773069381714
batch: 42
train_loss: 225.03650617599487
batch: 43
train_loss: 230.19028091430664
batch: 44
train_loss: 235.3480567932129
batch: 45
train_loss: 240.50001859664917
batch: 46
train_loss: 245.64312744140625
batch: 47
train_loss: 250.78338050842285
batch: 48
train_loss: 255.92325353622437
batch: 49
train_loss: 261.05263090133667
batch: 50
train_loss: 266.1865038871765
batch: 51
train_loss: 271.3299894332886
batch: 52
train_loss: 276.44896268844604
batch: 53
train_loss: 281.5670371055603
batch: 54
train_loss: 286.6829991340637
batch: 55
train_loss: 291.7996277809143
batch: 56
train_loss: 296.91073274612427
batch: 57
train_loss: 302.0025634765625
batch: 58
train_loss: 307.1036419868469
batch: 59
train_loss: 312.21645498275757
batch: 60
train_loss: 317.3091707229614
batch: 61
train_loss: 322.4029173851013
batch: 62
train_loss: 327.49681186676025
batch: 63
train_loss: 332.5851192474365
batch: 64
train_loss: 337.6617522239685
batch: 65
train_loss: 342.75049209594727
batch: 66
train_loss: 347.8393702507019
batch: 67
train_loss: 352.919762134552
batch: 68
train_loss: 358.00688552856445
batch: 69
train_loss: 363.0747113227844
batch: 70
train_loss: 368.1307964324951
batch: 71
train_loss: 373.1923370361328
batch: 72
train_loss: 378.2538685798645
batch: 73
train_loss: 383.30461502075195
batch: 74
train_loss: 388.35041761398315
batch: 75
train_loss: 393.40757513046265
batch: 76
train_loss: 398.4628973007202
batch: 77
train_loss: 403.50471591949463
batch: 78
train_loss: 408.54577922821045
batch: 79
train_loss: 413.58884382247925
batch: 80
train_loss: 418.630410194397
batch: 81
train_loss: 423.68499660491943
batch: 82
train_loss: 428.72386598587036
batch: 83
train_loss: 433.75346994400024
batch: 84
train_loss: 438.7663803100586
batch: 85
train_loss: 443.7984080314636
batch: 86
train_loss: 448.81399059295654
batch: 87
train_loss: 453.8386917114258
batch: 88
train_loss: 458.8444666862488
batch: 89
train_loss: 463.8236837387085
batch: 90
train_loss: 468.81574010849
batch: 91
train_loss: 473.812961101532
batch: 92
train_loss: 478.80751180648804
batch: 93
train_loss: 483.78229331970215
batch: 94
train_loss: 488.77955389022827
batch: 95
train_loss: 493.76752185821533
batch: 96
train_loss: 498.7498106956482
batch: 97
train_loss: 503.7212567329407
batch: 98
train_loss: 508.68592500686646
batch: 99
train_loss: 513.6703882217407
batch: 100
train_loss: 518.6381101608276
batch: 101
train_loss: 523.5968012809753
batch: 102
train_loss: 528.5548014640808
batch: 103
train_loss: 533.5366401672363
batch: 104
train_loss: 538.5041222572327
batch: 105
train_loss: 543.4610013961792
batch: 106
train_loss: 548.3997101783752
batch: 107
train_loss: 553.3394560813904
batch: 108
train_loss: 558.3149676322937
batch: 109
train_loss: 563.263256072998
batch: 110
train_loss: 568.2082080841064
batch: 111
train_loss: 573.1472749710083
batch: 112
train_loss: 578.0857844352722
batch: 113
train_loss: 583.0224270820618
batch: 114
train_loss: 587.9520831108093
batch: 115
train_loss: 592.8897361755371
batch: 116
train_loss: 597.8221354484558
batch: 117
train_loss: 602.7556228637695
batch: 118
train_loss: 607.6798548698425
batch: 119
train_loss: 612.5905475616455
batch: 120
train_loss: 617.5041298866272
batch: 121
train_loss: 622.4196381568909
batch: 122
train_loss: 627.3328657150269
batch: 123
train_loss: 632.2304582595825
batch: 124
train_loss: 637.1231908798218
batch: 125
train_loss: 642.0672326087952
batch: 126
train_loss: 646.9612240791321
batch: 127
train_loss: 651.8549222946167
batch: 128
train_loss: 656.7185797691345
batch: 129
train_loss: 661.5905132293701
batch: 130
train_loss: 666.4673509597778
batch: 131
train_loss: 671.3468570709229
batch: 132
train_loss: 676.2268767356873
batch: 133
train_loss: 681.0794544219971
batch: 134
train_loss: 685.9298286437988
batch: 135
train_loss: 690.7888398170471
batch: 136
train_loss: 695.6683473587036
batch: 137
train_loss: 700.5365676879883
batch: 138
train_loss: 705.3763828277588
batch: 139
train_loss: 710.2263808250427
batch: 140
train_loss: 715.0457143783569
batch: 141
train_loss: 719.8574061393738
batch: 142
train_loss: 724.7016897201538
batch: 143
train_loss: 729.5169825553894
batch: 144
train_loss: 734.3232679367065
batch: 145
train_loss: 739.1328234672546
batch: 146
train_loss: 743.9702649116516
batch: 147
train_loss: 748.7764973640442
batch: 148
train_loss: 753.5585489273071
batch: 149
train_loss: 758.3303828239441
batch: 150
train_loss: 763.1088738441467
batch: 151
train_loss: 767.8912992477417
batch: 152
train_loss: 772.7088394165039
batch: 153
train_loss: 777.5445690155029
batch: 154
train_loss: 782.3279709815979
batch: 155
train_loss: 787.1459670066833
batch: 156
train_loss: 791.9358167648315
batch: 157
train_loss: 796.6919255256653
batch: 158
train_loss: 801.4396781921387
batch: 159
train_loss: 806.1834964752197
batch: 160
train_loss: 810.9297995567322
batch: 161
train_loss: 815.6678490638733
batch: 162
train_loss: 820.4144678115845
batch: 163
train_loss: 825.1560864448547
batch: 164
train_loss: 829.8772993087769
batch: 165
train_loss: 834.6067638397217
batch: 166
train_loss: 839.3043994903564
batch: 167
train_loss: 844.0074601173401
batch: 168
train_loss: 848.7348051071167
batch: 169
train_loss: 853.4713373184204
batch: 170
train_loss: 858.1868810653687
batch: 171
train_loss: 862.9083070755005
batch: 172
train_loss: 867.6235699653625
batch: 173
train_loss: 872.3118906021118
batch: 174
train_loss: 876.9953451156616
batch: 175
train_loss: 881.6864428520203
batch: 176
train_loss: 886.3806252479553
batch: 177
train_loss: 891.089822769165
batch: 178
train_loss: 895.7629098892212
batch: 179
train_loss: 900.4416365623474
batch: 180
train_loss: 905.111044883728
batch: 181
train_loss: 909.7853360176086
batch: 182
train_loss: 914.4553375244141
batch: 183
train_loss: 919.1237087249756
batch: 184
train_loss: 923.8194227218628
batch: 185
train_loss: 928.4660778045654
batch: 186
train_loss: 933.1123471260071
batch: 187
train_loss: 937.7839097976685
batch: 188
train_loss: 942.4380774497986
batch: 189
train_loss: 947.0950617790222
batch: 190
train_loss: 951.7486290931702
batch: 191
train_loss: 956.4197020530701
batch: 192
train_loss: 961.0546431541443
batch: 193
train_loss: 965.6996512413025
batch: 194
train_loss: 970.332866191864
batch: 195
train_loss: 974.950336933136
batch: 196
train_loss: 979.5828971862793
batch: 197
train_loss: 984.2123246192932
batch: 198
train_loss: 988.8260183334351
batch: 199
train_loss: 993.483320236206
| epoch   0 step      200 |    200 batches | lr 0.00025 | ms/batch 153.23 | loss  4.97 | bpc   7.16647
batch: 200
train_loss: 4.60943603515625
batch: 201
train_loss: 9.259142398834229
batch: 202
train_loss: 13.891940593719482
batch: 203
train_loss: 18.50006103515625
batch: 204
train_loss: 23.12382459640503
batch: 205
train_loss: 27.75247621536255
batch: 206
train_loss: 32.35749053955078
batch: 207
train_loss: 36.95703411102295
batch: 208
train_loss: 41.55063056945801
batch: 209
train_loss: 46.15077352523804
batch: 210
train_loss: 50.76286792755127
batch: 211
train_loss: 55.32411241531372
batch: 212
train_loss: 59.92514419555664
batch: 213
train_loss: 64.53744411468506
batch: 214
train_loss: 69.10974597930908
batch: 215
train_loss: 73.71674108505249
batch: 216
train_loss: 78.30218505859375
batch: 217
train_loss: 82.88602018356323
batch: 218
train_loss: 87.46491622924805
batch: 219
train_loss: 92.0445008277893
batch: 220
train_loss: 96.59603786468506
batch: 221
train_loss: 101.13626194000244
batch: 222
train_loss: 105.71124172210693
batch: 223
train_loss: 110.23982810974121
batch: 224
train_loss: 114.80303764343262
batch: 225
train_loss: 119.35642862319946
batch: 226
train_loss: 123.90088176727295
batch: 227
train_loss: 128.45907020568848
batch: 228
train_loss: 133.01245164871216
batch: 229
train_loss: 137.53598546981812
batch: 230
train_loss: 142.03579139709473
batch: 231
train_loss: 146.578462600708
batch: 232
train_loss: 151.08612966537476
batch: 233
train_loss: 155.6095094680786
batch: 234
train_loss: 160.1561598777771
batch: 235
train_loss: 164.657732963562
batch: 236
train_loss: 169.17009449005127
batch: 237
train_loss: 173.6835446357727
batch: 238
train_loss: 178.19265985488892
batch: 239
train_loss: 182.68621349334717
batch: 240
train_loss: 187.21153497695923
batch: 241
train_loss: 191.69171380996704
batch: 242
train_loss: 196.20770168304443
batch: 243
train_loss: 200.69479942321777
batch: 244
train_loss: 205.23306512832642
batch: 245
train_loss: 209.72501802444458
batch: 246
train_loss: 214.2114143371582
batch: 247
train_loss: 218.7386794090271
batch: 248
train_loss: 223.20154237747192
batch: 249
train_loss: 227.69084930419922
batch: 250
train_loss: 232.150616645813
batch: 251
train_loss: 236.64599132537842
batch: 252
train_loss: 241.11326551437378
batch: 253
train_loss: 245.5866355895996
batch: 254
train_loss: 250.12042951583862
batch: 255
train_loss: 254.58690071105957
batch: 256
train_loss: 259.0545768737793
batch: 257
train_loss: 263.4908323287964
batch: 258
train_loss: 267.9559588432312
batch: 259
train_loss: 272.4310827255249
batch: 260
train_loss: 276.8653268814087
batch: 261
train_loss: 281.30050706863403
batch: 262
train_loss: 285.75043392181396
batch: 263
train_loss: 290.21069288253784
batch: 264
train_loss: 294.6157064437866
batch: 265
train_loss: 299.0179362297058
batch: 266
train_loss: 303.4542922973633
batch: 267
train_loss: 307.901403427124
batch: 268
train_loss: 312.3317246437073
batch: 269
train_loss: 316.7378888130188
batch: 270
train_loss: 321.1785750389099
batch: 271
train_loss: 325.613130569458
batch: 272
train_loss: 330.0195469856262
batch: 273
train_loss: 334.4446277618408
batch: 274
train_loss: 338.8392071723938
batch: 275
train_loss: 343.2759346961975
batch: 276
train_loss: 347.6484217643738
batch: 277
train_loss: 352.02317810058594
batch: 278
train_loss: 356.46892786026
batch: 279
train_loss: 360.85738611221313
batch: 280
train_loss: 365.2347116470337
batch: 281
train_loss: 369.57426595687866
batch: 282
train_loss: 373.91664695739746
batch: 283
train_loss: 378.3089385032654
batch: 284
train_loss: 382.64293241500854
batch: 285
train_loss: 387.0813217163086
batch: 286
train_loss: 391.4760479927063
batch: 287
train_loss: 395.8442711830139
batch: 288
train_loss: 400.21204376220703
batch: 289
train_loss: 404.5659165382385
batch: 290
train_loss: 408.8908224105835
batch: 291
train_loss: 413.2405505180359
batch: 292
train_loss: 417.58723974227905
batch: 293
train_loss: 421.9574556350708
batch: 294
train_loss: 426.3158473968506
batch: 295
train_loss: 430.7161431312561
batch: 296
train_loss: 435.11614656448364
batch: 297
train_loss: 439.48640155792236
batch: 298
train_loss: 443.8319787979126
batch: 299
train_loss: 448.1768913269043
batch: 300
train_loss: 452.52390575408936
batch: 301
train_loss: 456.90829849243164
batch: 302
train_loss: 461.2501163482666
batch: 303
train_loss: 465.5927014350891
batch: 304
train_loss: 469.9422960281372
batch: 305
train_loss: 474.3106598854065
batch: 306
train_loss: 478.6889395713806
batch: 307
train_loss: 482.9868206977844
batch: 308
train_loss: 487.3022918701172
batch: 309
train_loss: 491.62621784210205
batch: 310
train_loss: 495.8968753814697
batch: 311
train_loss: 500.18057107925415
batch: 312
train_loss: 504.45017290115356
batch: 313
train_loss: 508.71734952926636
batch: 314
train_loss: 513.0108051300049
batch: 315
train_loss: 517.2618989944458
batch: 316
train_loss: 521.5574560165405
batch: 317
train_loss: 525.8868989944458
batch: 318
train_loss: 530.1589455604553
batch: 319
train_loss: 534.4576182365417
batch: 320
train_loss: 538.6769485473633
batch: 321
train_loss: 542.9350590705872
batch: 322
train_loss: 547.2297129631042
batch: 323
train_loss: 551.4976348876953
batch: 324
train_loss: 555.8067746162415
batch: 325
train_loss: 560.1338996887207
batch: 326
train_loss: 564.3725652694702
batch: 327
train_loss: 568.6669640541077
batch: 328
train_loss: 572.894549369812
batch: 329
train_loss: 577.1047897338867
batch: 330
train_loss: 581.3980870246887
batch: 331
train_loss: 585.6119842529297
batch: 332
train_loss: 589.8097410202026
batch: 333
train_loss: 593.9852924346924
batch: 334
train_loss: 598.1996583938599
batch: 335
train_loss: 602.414689540863
batch: 336
train_loss: 606.5990152359009
batch: 337
train_loss: 610.8067183494568
batch: 338
train_loss: 614.9760828018188
batch: 339
train_loss: 619.1079893112183
batch: 340
train_loss: 623.2622833251953
batch: 341
train_loss: 627.440101146698
batch: 342
train_loss: 631.6866464614868
batch: 343
train_loss: 635.8281736373901
batch: 344
train_loss: 640.0184578895569
batch: 345
train_loss: 644.2156229019165
batch: 346
train_loss: 648.4471979141235
batch: 347
train_loss: 652.6438269615173
batch: 348
train_loss: 656.8098545074463
batch: 349
train_loss: 661.0162482261658
batch: 350
train_loss: 665.2137231826782
batch: 351
train_loss: 669.4422640800476
batch: 352
train_loss: 673.677041053772
batch: 353
train_loss: 677.8884825706482
batch: 354
train_loss: 682.0793075561523
batch: 355
train_loss: 686.2474675178528
batch: 356
train_loss: 690.419970035553
batch: 357
train_loss: 694.5450401306152
batch: 358
train_loss: 698.7178301811218
batch: 359
train_loss: 702.8456149101257
batch: 360
train_loss: 706.9820289611816
batch: 361
train_loss: 711.1218490600586
batch: 362
train_loss: 715.2750186920166
batch: 363
train_loss: 719.4679527282715
batch: 364
train_loss: 723.6931376457214
batch: 365
train_loss: 728.0033960342407
batch: 366
train_loss: 732.2275357246399
batch: 367
train_loss: 736.4053874015808
batch: 368
train_loss: 740.5856337547302
batch: 369
train_loss: 744.7698893547058
batch: 370
train_loss: 748.977611541748
batch: 371
train_loss: 753.1340084075928
batch: 372
train_loss: 757.3132381439209
batch: 373
train_loss: 761.46941614151
batch: 374
train_loss: 765.6588358879089
batch: 375
train_loss: 769.8500356674194
batch: 376
train_loss: 773.9600257873535
batch: 377
train_loss: 778.1115131378174
batch: 378
train_loss: 782.3092060089111
batch: 379
train_loss: 786.3730225563049
batch: 380
train_loss: 790.5177006721497
batch: 381
train_loss: 794.7087941169739
batch: 382
train_loss: 798.8465509414673
batch: 383
train_loss: 803.0558228492737
batch: 384
train_loss: 807.2461748123169
batch: 385
train_loss: 811.3769164085388
batch: 386
train_loss: 815.4508996009827
batch: 387
train_loss: 819.4918203353882
batch: 388
train_loss: 823.604040145874
batch: 389
train_loss: 827.7027053833008
batch: 390
train_loss: 831.8886413574219
batch: 391
train_loss: 836.0442333221436
batch: 392
train_loss: 840.1927380561829
batch: 393
train_loss: 844.3271279335022
batch: 394
train_loss: 848.3673715591431
batch: 395
train_loss: 852.4347085952759
batch: 396
train_loss: 856.5665340423584
batch: 397
train_loss: 860.6313714981079
batch: 398
train_loss: 864.7046360969543
batch: 399
train_loss: 868.7968649864197
| epoch   0 step      400 |    400 batches | lr 0.00025 | ms/batch 152.58 | loss  4.34 | bpc   6.26704
batch: 400
train_loss: 4.143965244293213
batch: 401
train_loss: 8.182084560394287
batch: 402
train_loss: 12.323880195617676
batch: 403
train_loss: 16.39096736907959
batch: 404
train_loss: 20.472797393798828
batch: 405
train_loss: 24.54402208328247
batch: 406
train_loss: 28.694526195526123
batch: 407
train_loss: 32.868717193603516
batch: 408
train_loss: 36.983123779296875
batch: 409
train_loss: 41.05830383300781
batch: 410
train_loss: 45.168577671051025
batch: 411
train_loss: 49.270934104919434
batch: 412
train_loss: 53.31220531463623
batch: 413
train_loss: 57.33853626251221
batch: 414
train_loss: 61.40757179260254
batch: 415
train_loss: 65.40012764930725
batch: 416
train_loss: 69.3747181892395
batch: 417
train_loss: 73.42096042633057
batch: 418
train_loss: 77.43712711334229
batch: 419
train_loss: 81.45511150360107
batch: 420
train_loss: 85.47118425369263
batch: 421
train_loss: 89.53630495071411
batch: 422
train_loss: 93.60612154006958
batch: 423
train_loss: 97.68587875366211
batch: 424
train_loss: 101.71299123764038
batch: 425
train_loss: 105.66695380210876
batch: 426
train_loss: 109.65375280380249
batch: 427
train_loss: 113.66641330718994
batch: 428
train_loss: 117.6535632610321
batch: 429
train_loss: 121.6345865726471
batch: 430
train_loss: 125.6086847782135
batch: 431
train_loss: 129.58620882034302
batch: 432
train_loss: 133.56276774406433
batch: 433
train_loss: 137.5734508037567
batch: 434
train_loss: 141.57237434387207
batch: 435
train_loss: 145.61352014541626
batch: 436
train_loss: 149.55899596214294
batch: 437
train_loss: 153.5729682445526
batch: 438
train_loss: 157.55348134040833
batch: 439
train_loss: 161.64618754386902
batch: 440
train_loss: 165.6598551273346
batch: 441
train_loss: 169.65961337089539
batch: 442
train_loss: 173.64081525802612
batch: 443
train_loss: 177.6391315460205
batch: 444
train_loss: 181.61273384094238
batch: 445
train_loss: 185.62195825576782
batch: 446
train_loss: 189.59559392929077
batch: 447
train_loss: 193.58796739578247
batch: 448
train_loss: 197.50562620162964
batch: 449
train_loss: 201.4727725982666
batch: 450
train_loss: 205.41289639472961
batch: 451
train_loss: 209.43577933311462
batch: 452
train_loss: 213.414555311203
batch: 453
train_loss: 217.32762503623962
batch: 454
train_loss: 221.25751090049744
batch: 455
train_loss: 225.19733476638794
batch: 456
train_loss: 229.2266435623169
batch: 457
train_loss: 233.22501063346863
batch: 458
train_loss: 237.23179173469543
batch: 459
train_loss: 241.20486903190613
batch: 460
train_loss: 245.1213459968567
batch: 461
train_loss: 249.11318802833557
batch: 462
train_loss: 253.11657118797302
batch: 463
train_loss: 257.0520486831665
batch: 464
train_loss: 261.0700674057007
batch: 465
train_loss: 265.04822850227356
batch: 466
train_loss: 268.9882197380066
batch: 467
train_loss: 273.02897214889526
batch: 468
train_loss: 276.99461340904236
batch: 469
train_loss: 281.0339705944061
batch: 470
train_loss: 285.03837847709656
batch: 471
train_loss: 289.1655128002167
batch: 472
train_loss: 293.19632267951965
batch: 473
train_loss: 297.14898705482483
batch: 474
train_loss: 301.16132140159607
batch: 475
train_loss: 305.1382722854614
batch: 476
train_loss: 309.0603516101837
batch: 477
train_loss: 313.0220375061035
batch: 478
train_loss: 317.00897121429443
batch: 479
train_loss: 320.92155718803406
batch: 480
train_loss: 324.8534617424011
batch: 481
train_loss: 328.7702798843384
batch: 482
train_loss: 332.78667545318604
batch: 483
train_loss: 336.7669379711151
batch: 484
train_loss: 340.6642003059387
batch: 485
train_loss: 344.6908874511719
batch: 486
train_loss: 348.5501494407654
batch: 487
train_loss: 352.51690196990967
batch: 488
train_loss: 356.44040751457214
batch: 489
train_loss: 360.3479597568512
batch: 490
train_loss: 364.31974625587463
batch: 491
train_loss: 368.24330592155457
batch: 492
train_loss: 372.1850106716156
batch: 493
train_loss: 376.07072377204895
batch: 494
train_loss: 379.9916365146637
batch: 495
train_loss: 383.9414310455322
batch: 496
train_loss: 387.77983498573303
batch: 497
train_loss: 391.65104389190674
batch: 498
train_loss: 395.50092005729675
batch: 499
train_loss: 399.38405323028564
batch: 500
train_loss: 403.2324504852295
batch: 501
train_loss: 407.1283612251282
batch: 502
train_loss: 410.95093035697937
batch: 503
train_loss: 414.8075907230377
batch: 504
train_loss: 418.7070870399475
batch: 505
train_loss: 422.50642251968384
batch: 506
train_loss: 426.34118151664734
batch: 507
train_loss: 430.1732003688812
batch: 508
train_loss: 434.00164675712585
batch: 509
train_loss: 437.84483885765076
batch: 510
train_loss: 441.60118651390076
batch: 511
train_loss: 445.41465997695923
batch: 512
train_loss: 449.2205140590668
batch: 513
train_loss: 453.101126909256
batch: 514
train_loss: 456.8921067714691
batch: 515
train_loss: 460.6896712779999
batch: 516
train_loss: 464.51725602149963
batch: 517
train_loss: 468.34562587738037
batch: 518
train_loss: 472.1778075695038
batch: 519
train_loss: 475.9961974620819
batch: 520
train_loss: 479.7844624519348
batch: 521
train_loss: 483.5829756259918
batch: 522
train_loss: 487.4541480541229
batch: 523
train_loss: 491.29329442977905
batch: 524
train_loss: 495.0582642555237
batch: 525
train_loss: 498.84544920921326
batch: 526
train_loss: 502.64563250541687
batch: 527
train_loss: 506.4250285625458
batch: 528
train_loss: 510.32416701316833
batch: 529
train_loss: 514.1149582862854
batch: 530
train_loss: 517.9819648265839
batch: 531
train_loss: 521.8210470676422
batch: 532
train_loss: 525.6116874217987
batch: 533
train_loss: 529.3990042209625
batch: 534
train_loss: 533.2012968063354
batch: 535
train_loss: 536.9707531929016
batch: 536
train_loss: 540.7349812984467
batch: 537
train_loss: 544.5089263916016
batch: 538
train_loss: 548.285017490387
batch: 539
train_loss: 552.0464854240417
batch: 540
train_loss: 555.8310742378235
batch: 541
train_loss: 559.6306891441345
batch: 542
train_loss: 563.4397683143616
batch: 543
train_loss: 567.2048742771149
batch: 544
train_loss: 570.9525146484375
batch: 545
train_loss: 574.7058081626892
batch: 546
train_loss: 578.3897850513458
batch: 547
train_loss: 582.0758965015411
batch: 548
train_loss: 585.7711808681488
batch: 549
train_loss: 589.484977722168
batch: 550
train_loss: 593.284462928772
batch: 551
train_loss: 597.0911152362823
batch: 552
train_loss: 600.8227825164795
batch: 553
train_loss: 604.57373213768
batch: 554
train_loss: 608.329882144928
batch: 555
train_loss: 612.0818138122559
batch: 556
train_loss: 615.8379402160645
batch: 557
train_loss: 619.5581696033478
batch: 558
train_loss: 623.2765641212463
batch: 559
train_loss: 627.00013422966
batch: 560
train_loss: 630.6748206615448
batch: 561
train_loss: 634.423540353775
batch: 562
train_loss: 638.1632566452026
batch: 563
train_loss: 641.8581020832062
batch: 564
train_loss: 645.6043474674225
batch: 565
train_loss: 649.3262982368469
batch: 566
train_loss: 653.0937106609344
batch: 567
train_loss: 656.8026661872864
batch: 568
train_loss: 660.566534280777
batch: 569
train_loss: 664.3184380531311
batch: 570
train_loss: 668.0622789859772
batch: 571
train_loss: 671.8556070327759
batch: 572
train_loss: 675.526346206665
batch: 573
train_loss: 679.2476427555084
batch: 574
train_loss: 682.9711375236511
batch: 575
train_loss: 686.6849598884583
batch: 576
train_loss: 690.4797534942627
batch: 577
train_loss: 694.156726360321
batch: 578
train_loss: 697.9114978313446
batch: 579
train_loss: 701.6260442733765
batch: 580
train_loss: 705.3794803619385
batch: 581
train_loss: 709.0542957782745
batch: 582
train_loss: 712.7435076236725
batch: 583
train_loss: 716.4901230335236
batch: 584
train_loss: 720.2389769554138
batch: 585
train_loss: 723.9769637584686
batch: 586
train_loss: 727.6514768600464
batch: 587
train_loss: 731.251368522644
batch: 588
train_loss: 734.9708287715912
batch: 589
train_loss: 738.6736166477203
batch: 590
train_loss: 742.3462569713593
batch: 591
train_loss: 746.0895676612854
batch: 592
train_loss: 749.9118132591248
batch: 593
train_loss: 753.644118309021
batch: 594
train_loss: 757.3556606769562
batch: 595
train_loss: 761.0734198093414
batch: 596
train_loss: 764.8381702899933
batch: 597
train_loss: 768.5915675163269
batch: 598
train_loss: 772.3254761695862
batch: 599
train_loss: 776.0506310462952
| epoch   0 step      600 |    600 batches | lr 0.00025 | ms/batch 153.11 | loss  3.88 | bpc   5.59802
batch: 600
train_loss: 3.7229936122894287
batch: 601
train_loss: 7.413846969604492
batch: 602
train_loss: 11.098539113998413
batch: 603
train_loss: 14.87338137626648
batch: 604
train_loss: 18.55000329017639
batch: 605
train_loss: 22.22385048866272
batch: 606
train_loss: 26.0295991897583
batch: 607
train_loss: 29.715086460113525
batch: 608
train_loss: 33.427961587905884
batch: 609
train_loss: 37.02741813659668
batch: 610
train_loss: 40.63055443763733
batch: 611
train_loss: 44.26366829872131
batch: 612
train_loss: 47.91007351875305
batch: 613
train_loss: 51.530449867248535
batch: 614
train_loss: 55.16654706001282
batch: 615
train_loss: 58.77810192108154
batch: 616
train_loss: 62.46400332450867
batch: 617
train_loss: 66.11844801902771
batch: 618
train_loss: 69.87513756752014
batch: 619
train_loss: 73.64416742324829
batch: 620
train_loss: 77.44032216072083
batch: 621
train_loss: 81.18742394447327
batch: 622
train_loss: 84.9374451637268
batch: 623
train_loss: 88.51833057403564
batch: 624
train_loss: 92.22572326660156
batch: 625
train_loss: 95.80326175689697
batch: 626
train_loss: 99.46405029296875
batch: 627
train_loss: 103.1041669845581
batch: 628
train_loss: 106.70487189292908
batch: 629
train_loss: 110.32641887664795
batch: 630
train_loss: 113.9227511882782
batch: 631
train_loss: 117.55707430839539
batch: 632
train_loss: 121.19424104690552
batch: 633
train_loss: 124.77825689315796
batch: 634
train_loss: 128.3549554347992
batch: 635
train_loss: 131.95923328399658
batch: 636
train_loss: 135.51894450187683
batch: 637
train_loss: 139.11858534812927
batch: 638
train_loss: 142.77992510795593
batch: 639
train_loss: 146.51127195358276
batch: 640
train_loss: 150.09223341941833
batch: 641
train_loss: 153.73348760604858
batch: 642
train_loss: 157.32929348945618
batch: 643
train_loss: 161.0182399749756
batch: 644
train_loss: 164.78596448898315
batch: 645
train_loss: 168.34530186653137
batch: 646
train_loss: 171.89310002326965
batch: 647
train_loss: 175.4818344116211
batch: 648
train_loss: 179.15802097320557
batch: 649
train_loss: 182.76820015907288
batch: 650
train_loss: 186.36203956604004
batch: 651
train_loss: 190.04917550086975
batch: 652
train_loss: 193.70522737503052
batch: 653
train_loss: 197.3428282737732
batch: 654
train_loss: 201.05034947395325
batch: 655
train_loss: 204.66890263557434
batch: 656
train_loss: 208.3062882423401
batch: 657
train_loss: 211.856431722641
batch: 658
train_loss: 215.60650181770325
batch: 659
train_loss: 219.2071304321289
batch: 660
train_loss: 222.87481904029846
batch: 661
train_loss: 226.5872097015381
batch: 662
train_loss: 230.25102972984314
batch: 663
train_loss: 233.90655851364136
batch: 664
train_loss: 237.5262577533722
batch: 665
train_loss: 241.06830263137817
batch: 666
train_loss: 244.64087557792664
batch: 667
train_loss: 248.18978071212769
batch: 668
train_loss: 251.8205761909485
batch: 669
train_loss: 255.3581907749176
batch: 670
train_loss: 258.8628263473511
batch: 671
train_loss: 262.4667091369629
batch: 672
train_loss: 265.9788782596588
batch: 673
train_loss: 269.51084899902344
batch: 674
train_loss: 273.0212278366089
batch: 675
train_loss: 276.5537450313568
batch: 676
train_loss: 280.06494212150574
batch: 677
train_loss: 283.5809600353241
batch: 678
train_loss: 287.1183183193207
batch: 679
train_loss: 290.6896541118622
batch: 680
train_loss: 294.241934299469
batch: 681
train_loss: 297.7434630393982
batch: 682
train_loss: 301.295152425766
batch: 683
train_loss: 304.82222390174866
batch: 684
train_loss: 308.42009472846985
batch: 685
train_loss: 311.9200391769409
batch: 686
train_loss: 315.4918041229248
batch: 687
train_loss: 319.08719086647034
batch: 688
train_loss: 322.6284432411194
batch: 689
train_loss: 326.16772294044495
batch: 690
train_loss: 329.64840841293335
batch: 691
train_loss: 333.22974705696106
batch: 692
train_loss: 336.89047408103943
batch: 693
train_loss: 340.4065902233124
batch: 694
train_loss: 343.8760635852814
batch: 695
train_loss: 347.37047576904297
batch: 696
train_loss: 350.8233880996704
batch: 697
train_loss: 354.3700428009033
batch: 698
train_loss: 357.92093539237976
batch: 699
train_loss: 361.44474387168884
batch: 700
train_loss: 364.9093322753906
batch: 701
train_loss: 368.359801530838
batch: 702
train_loss: 371.9152762889862
batch: 703
train_loss: 375.44458746910095
batch: 704
train_loss: 378.9461245536804
batch: 705
train_loss: 382.55319333076477
batch: 706
train_loss: 386.03577613830566
batch: 707
train_loss: 389.452987909317
batch: 708
train_loss: 392.9520671367645
batch: 709
train_loss: 396.43998885154724
batch: 710
train_loss: 399.92254662513733
batch: 711
train_loss: 403.3402099609375
batch: 712
train_loss: 406.7967674732208
batch: 713
train_loss: 410.28575682640076
batch: 714
train_loss: 413.7778856754303
batch: 715
train_loss: 417.3098978996277
batch: 716
train_loss: 420.87370347976685
batch: 717
train_loss: 424.4177372455597
batch: 718
train_loss: 427.99186968803406
batch: 719
train_loss: 431.4713342189789
batch: 720
train_loss: 435.10737681388855
batch: 721
train_loss: 438.59081268310547
batch: 722
train_loss: 442.13050293922424
batch: 723
train_loss: 445.603796005249
batch: 724
train_loss: 449.16689133644104
batch: 725
train_loss: 452.6894836425781
batch: 726
train_loss: 456.2569124698639
batch: 727
train_loss: 459.8438148498535
batch: 728
train_loss: 463.46911096572876
batch: 729
train_loss: 467.0300250053406
batch: 730
train_loss: 470.5718140602112
batch: 731
train_loss: 474.05372643470764
batch: 732
train_loss: 477.5732958316803
batch: 733
train_loss: 481.0548748970032
batch: 734
train_loss: 484.62536883354187
batch: 735
train_loss: 488.1215486526489
batch: 736
train_loss: 491.66052436828613
batch: 737
train_loss: 495.1675531864166
batch: 738
train_loss: 498.7064402103424
batch: 739
train_loss: 502.3465714454651
batch: 740
train_loss: 505.85742712020874
batch: 741
train_loss: 509.4164869785309
batch: 742
train_loss: 513.0423452854156
batch: 743
train_loss: 516.5194432735443
batch: 744
train_loss: 520.0556464195251
batch: 745
train_loss: 523.6208257675171
batch: 746
train_loss: 527.1647222042084
batch: 747
train_loss: 530.6291522979736
batch: 748
train_loss: 534.1945416927338
batch: 749
train_loss: 537.87948346138
batch: 750
train_loss: 541.5233995914459
batch: 751
train_loss: 545.1387901306152
batch: 752
train_loss: 548.8241102695465
batch: 753
train_loss: 552.3805303573608
batch: 754
train_loss: 555.9606313705444
batch: 755
train_loss: 559.4843466281891
batch: 756
train_loss: 563.027063369751
batch: 757
train_loss: 566.699312210083
batch: 758
train_loss: 570.3364248275757
batch: 759
train_loss: 574.0111606121063
batch: 760
train_loss: 577.6226527690887
batch: 761
train_loss: 581.1270871162415
batch: 762
train_loss: 584.7440419197083
batch: 763
train_loss: 588.3683071136475
batch: 764
train_loss: 592.0414867401123
batch: 765
train_loss: 595.6157512664795
batch: 766
train_loss: 599.2808408737183
batch: 767
train_loss: 602.6776983737946
batch: 768
train_loss: 606.2254929542542
batch: 769
train_loss: 609.7003283500671
batch: 770
train_loss: 613.2138211727142
batch: 771
train_loss: 616.807226896286
batch: 772
train_loss: 620.3581981658936
batch: 773
train_loss: 623.8085079193115
batch: 774
train_loss: 627.2936964035034
batch: 775
train_loss: 630.8423976898193
batch: 776
train_loss: 634.4274854660034
batch: 777
train_loss: 638.0907988548279
batch: 778
train_loss: 641.6872746944427
batch: 779
train_loss: 645.218451499939
batch: 780
train_loss: 648.7182033061981
batch: 781
train_loss: 652.3942966461182
batch: 782
train_loss: 656.0357344150543
batch: 783
train_loss: 659.5463757514954
batch: 784
train_loss: 662.9849283695221
batch: 785
train_loss: 666.6104304790497
batch: 786
train_loss: 670.1311359405518
batch: 787
train_loss: 673.8154089450836
batch: 788
train_loss: 677.3906624317169
batch: 789
train_loss: 681.0087110996246
batch: 790
train_loss: 684.5562970638275
batch: 791
train_loss: 688.2000522613525
batch: 792
train_loss: 691.7362468242645
batch: 793
train_loss: 695.227335691452
batch: 794
train_loss: 698.7311565876007
batch: 795
train_loss: 702.2832453250885
batch: 796
train_loss: 705.7753992080688
batch: 797
train_loss: 709.3734843730927
batch: 798
train_loss: 712.9473261833191
batch: 799
train_loss: 716.4633901119232
| epoch   0 step      800 |    800 batches | lr 0.00025 | ms/batch 152.94 | loss  3.58 | bpc   5.16819
batch: 800
train_loss: 3.5195958614349365
batch: 801
train_loss: 7.066524982452393
batch: 802
train_loss: 10.70414924621582
batch: 803
train_loss: 14.273222923278809
batch: 804
train_loss: 17.7982816696167
batch: 805
train_loss: 21.36453676223755
batch: 806
train_loss: 24.874703884124756
batch: 807
train_loss: 28.445384740829468
batch: 808
train_loss: 32.096758127212524
batch: 809
train_loss: 35.69171190261841
batch: 810
train_loss: 39.13595700263977
batch: 811
train_loss: 42.88001227378845
batch: 812
train_loss: 46.51452898979187
batch: 813
train_loss: 50.087746143341064
batch: 814
train_loss: 53.58741593360901
batch: 815
train_loss: 57.14587998390198
batch: 816
train_loss: 60.72268867492676
batch: 817
train_loss: 64.30595970153809
batch: 818
train_loss: 67.8918673992157
batch: 819
train_loss: 71.43334317207336
batch: 820
train_loss: 74.92749667167664
batch: 821
train_loss: 78.43903636932373
batch: 822
train_loss: 82.05426168441772
batch: 823
train_loss: 85.62822794914246
batch: 824
train_loss: 89.17321372032166
batch: 825
train_loss: 92.74540209770203
batch: 826
train_loss: 96.28278470039368
batch: 827
train_loss: 99.826486825943
batch: 828
train_loss: 103.31035542488098
batch: 829
train_loss: 106.73399448394775
batch: 830
train_loss: 110.21638059616089
batch: 831
train_loss: 113.78038668632507
batch: 832
train_loss: 117.24384498596191
batch: 833
train_loss: 120.71289110183716
batch: 834
train_loss: 124.30536770820618
batch: 835
train_loss: 127.8261935710907
batch: 836
train_loss: 131.30888557434082
batch: 837
train_loss: 134.75705909729004
batch: 838
train_loss: 138.16906476020813
batch: 839
train_loss: 141.65310883522034
batch: 840
train_loss: 145.1724271774292
batch: 841
train_loss: 148.7000675201416
batch: 842
train_loss: 152.18413424491882
batch: 843
train_loss: 155.66600060462952
batch: 844
train_loss: 159.1287808418274
batch: 845
train_loss: 162.5900318622589
batch: 846
train_loss: 166.05022764205933
batch: 847
train_loss: 169.55672335624695
batch: 848
train_loss: 173.04420733451843
batch: 849
train_loss: 176.62300992012024
batch: 850
train_loss: 180.25027656555176
batch: 851
train_loss: 183.79752397537231
batch: 852
train_loss: 187.41464066505432
batch: 853
train_loss: 190.9242238998413
batch: 854
train_loss: 194.48647904396057
batch: 855
train_loss: 198.0569305419922
batch: 856
train_loss: 201.61396098136902
batch: 857
train_loss: 205.11498618125916
batch: 858
train_loss: 208.66742157936096
batch: 859
train_loss: 212.27435445785522
batch: 860
train_loss: 215.85949087142944
batch: 861
train_loss: 219.33722066879272
batch: 862
train_loss: 222.89629530906677
batch: 863
train_loss: 226.46069359779358
batch: 864
train_loss: 229.97031569480896
batch: 865
train_loss: 233.56462717056274
batch: 866
train_loss: 237.1200578212738
batch: 867
train_loss: 240.60509634017944
batch: 868
train_loss: 244.21298265457153
batch: 869
train_loss: 247.78416800498962
batch: 870
train_loss: 251.26114702224731
batch: 871
train_loss: 254.77016472816467
batch: 872
train_loss: 258.33672189712524
batch: 873
train_loss: 261.9399826526642
batch: 874
train_loss: 265.4772882461548
batch: 875
train_loss: 269.0249836444855
batch: 876
train_loss: 272.4667773246765
batch: 877
train_loss: 275.9691743850708
batch: 878
train_loss: 279.41923213005066
batch: 879
train_loss: 282.93413758277893
batch: 880
train_loss: 286.5416302680969
batch: 881
train_loss: 290.0119376182556
batch: 882
train_loss: 293.6437780857086
batch: 883
train_loss: 297.1436276435852
batch: 884
train_loss: 300.7123966217041
batch: 885
train_loss: 304.2824125289917
batch: 886
train_loss: 307.85335087776184
batch: 887
train_loss: 311.4381380081177
batch: 888
train_loss: 315.0533273220062
batch: 889
train_loss: 318.6158127784729
batch: 890
train_loss: 322.14794993400574
batch: 891
train_loss: 325.66235637664795
batch: 892
train_loss: 329.17173886299133
batch: 893
train_loss: 332.62147879600525
batch: 894
train_loss: 336.06047010421753
batch: 895
train_loss: 339.63393568992615
batch: 896
train_loss: 343.13812255859375
batch: 897
train_loss: 346.682391166687
batch: 898
train_loss: 350.1929006576538
batch: 899
train_loss: 353.74544501304626
batch: 900
train_loss: 357.15475511550903
batch: 901
train_loss: 360.6594662666321
batch: 902
train_loss: 364.07059359550476
batch: 903
train_loss: 367.46850872039795
batch: 904
train_loss: 370.8995406627655
batch: 905
train_loss: 374.42801880836487
batch: 906
train_loss: 377.85683131217957
batch: 907
train_loss: 381.2852568626404
batch: 908
train_loss: 384.7825789451599
batch: 909
train_loss: 388.20396280288696
batch: 910
train_loss: 391.6159918308258
batch: 911
train_loss: 394.9968421459198
batch: 912
train_loss: 398.4805374145508
batch: 913
train_loss: 401.8852310180664
batch: 914
train_loss: 405.36294317245483
batch: 915
train_loss: 408.72931361198425
batch: 916
train_loss: 412.2511041164398
batch: 917
train_loss: 415.79195499420166
batch: 918
train_loss: 419.26787304878235
batch: 919
train_loss: 422.72243309020996
batch: 920
train_loss: 426.16574215888977
batch: 921
train_loss: 429.6197283267975
batch: 922
train_loss: 432.949214220047
batch: 923
train_loss: 436.42256569862366
batch: 924
train_loss: 439.9138879776001
batch: 925
train_loss: 443.2477080821991
batch: 926
train_loss: 446.61133193969727
batch: 927
train_loss: 450.0895540714264
batch: 928
train_loss: 453.5531265735626
batch: 929
train_loss: 457.00250005722046
batch: 930
train_loss: 460.39976811408997
batch: 931
train_loss: 463.8316750526428
batch: 932
train_loss: 467.3431990146637
batch: 933
train_loss: 470.8522651195526
batch: 934
train_loss: 474.3732907772064
batch: 935
train_loss: 477.90976071357727
batch: 936
train_loss: 481.3136007785797
batch: 937
train_loss: 484.71931076049805
batch: 938
train_loss: 488.2184548377991
batch: 939
train_loss: 491.61557841300964
batch: 940
train_loss: 495.01713466644287
batch: 941
train_loss: 498.4672706127167
batch: 942
train_loss: 501.95144486427307
batch: 943
train_loss: 505.43245553970337
batch: 944
train_loss: 508.91876673698425
batch: 945
train_loss: 512.4340181350708
batch: 946
train_loss: 515.9996559619904
batch: 947
train_loss: 519.4489169120789
batch: 948
train_loss: 522.873458147049
batch: 949
train_loss: 526.3550386428833
batch: 950
train_loss: 529.8398966789246
batch: 951
train_loss: 533.3470394611359
batch: 952
train_loss: 536.8285853862762
batch: 953
train_loss: 540.3441369533539
batch: 954
train_loss: 543.889575958252
batch: 955
train_loss: 547.4221177101135
batch: 956
train_loss: 550.8460791110992
batch: 957
train_loss: 554.2283668518066
batch: 958
train_loss: 557.6026892662048
batch: 959
train_loss: 560.9549345970154
batch: 960
train_loss: 564.5279984474182
batch: 961
train_loss: 568.0024607181549
batch: 962
train_loss: 571.4370272159576
batch: 963
train_loss: 574.8113627433777
batch: 964
train_loss: 578.1581392288208
batch: 965
train_loss: 581.4865264892578
batch: 966
train_loss: 584.8457190990448
batch: 967
train_loss: 588.159289598465
batch: 968
train_loss: 591.4942047595978
batch: 969
train_loss: 594.8608384132385
batch: 970
train_loss: 598.4027962684631
batch: 971
train_loss: 601.8391427993774
batch: 972
train_loss: 605.2055041790009
batch: 973
train_loss: 608.6774065494537
batch: 974
train_loss: 611.9966571331024
batch: 975
train_loss: 615.3385484218597
batch: 976
train_loss: 618.6929559707642
batch: 977
train_loss: 622.0428125858307
batch: 978
train_loss: 625.400796175003
batch: 979
train_loss: 628.9409232139587
batch: 980
train_loss: 632.3863477706909
batch: 981
train_loss: 635.8348546028137
batch: 982
train_loss: 639.3391790390015
batch: 983
train_loss: 642.8235950469971
batch: 984
train_loss: 646.2304120063782
batch: 985
train_loss: 649.7335295677185
batch: 986
train_loss: 653.3206343650818
batch: 987
train_loss: 656.8349003791809
batch: 988
train_loss: 660.3085186481476
batch: 989
train_loss: 663.7349033355713
batch: 990
train_loss: 667.1020443439484
batch: 991
train_loss: 670.5995275974274
batch: 992
train_loss: 674.0835342407227
batch: 993
train_loss: 677.5335631370544
batch: 994
train_loss: 680.9559020996094
batch: 995
train_loss: 684.3398714065552
batch: 996
train_loss: 687.8543658256531
batch: 997
train_loss: 691.3308358192444
batch: 998
train_loss: 694.7938189506531
batch: 999
train_loss: 698.436740398407
| epoch   0 step     1000 |   1000 batches | lr 0.00025 | ms/batch 154.35 | loss  3.49 | bpc   5.03816
batch: 1000
train_loss: 3.5886292457580566
batch: 1001
train_loss: 7.063600063323975
batch: 1002
train_loss: 10.518655061721802
batch: 1003
train_loss: 13.887205839157104
batch: 1004
train_loss: 17.426966190338135
batch: 1005
train_loss: 21.003546237945557
batch: 1006
train_loss: 24.573508739471436
batch: 1007
train_loss: 28.077837705612183
batch: 1008
train_loss: 31.525949954986572
batch: 1009
train_loss: 34.969573974609375
batch: 1010
train_loss: 38.34228539466858
batch: 1011
train_loss: 41.798980951309204
batch: 1012
train_loss: 45.30734825134277
batch: 1013
train_loss: 48.7401921749115
batch: 1014
train_loss: 52.10557317733765
batch: 1015
train_loss: 55.59603500366211
batch: 1016
train_loss: 59.134243965148926
batch: 1017
train_loss: 62.64891600608826
batch: 1018
train_loss: 66.19189977645874
batch: 1019
train_loss: 69.7084743976593
batch: 1020
train_loss: 73.24241280555725
batch: 1021
train_loss: 76.73356366157532
batch: 1022
train_loss: 80.1513102054596
batch: 1023
train_loss: 83.61608672142029
batch: 1024
train_loss: 87.02962565422058
batch: 1025
train_loss: 90.5304536819458
batch: 1026
train_loss: 94.08519554138184
batch: 1027
train_loss: 97.51189684867859
batch: 1028
train_loss: 100.94624280929565
batch: 1029
train_loss: 104.42532157897949
batch: 1030
train_loss: 107.86924147605896
batch: 1031
train_loss: 111.2039749622345
batch: 1032
train_loss: 114.63510298728943
batch: 1033
train_loss: 118.19123339653015
batch: 1034
train_loss: 121.6640682220459
batch: 1035
train_loss: 125.20464897155762
batch: 1036
train_loss: 128.58957195281982
batch: 1037
train_loss: 132.04560542106628
batch: 1038
train_loss: 135.43672633171082
batch: 1039
train_loss: 138.8752522468567
batch: 1040
train_loss: 142.2442545890808
batch: 1041
train_loss: 145.7631754875183
batch: 1042
train_loss: 149.15806245803833
batch: 1043
train_loss: 152.56357860565186
batch: 1044
train_loss: 155.93488883972168
batch: 1045
train_loss: 159.38398146629333
batch: 1046
train_loss: 162.807715177536
batch: 1047
train_loss: 166.25678825378418
batch: 1048
train_loss: 169.69848585128784
batch: 1049
train_loss: 173.19181489944458
batch: 1050
train_loss: 176.58478450775146
batch: 1051
train_loss: 179.95902061462402
batch: 1052
train_loss: 183.36685466766357
batch: 1053
train_loss: 186.82736086845398
batch: 1054
train_loss: 190.31862878799438
batch: 1055
train_loss: 193.77867150306702
batch: 1056
train_loss: 197.19281578063965
batch: 1057
train_loss: 200.5893223285675
batch: 1058
train_loss: 204.03151726722717
batch: 1059
train_loss: 207.47470140457153
batch: 1060
train_loss: 210.75362849235535
batch: 1061
train_loss: 214.24135899543762
batch: 1062
train_loss: 217.63795638084412
batch: 1063
train_loss: 221.07070636749268
batch: 1064
train_loss: 224.60567712783813
batch: 1065
train_loss: 228.03203129768372
batch: 1066
train_loss: 231.49454069137573
batch: 1067
train_loss: 234.98849606513977
batch: 1068
train_loss: 238.50781512260437
batch: 1069
train_loss: 242.06125283241272
batch: 1070
train_loss: 245.4640130996704
batch: 1071
train_loss: 248.87175297737122
batch: 1072
train_loss: 252.26089119911194
batch: 1073
train_loss: 255.65929460525513
batch: 1074
train_loss: 259.1715896129608
batch: 1075
train_loss: 262.60904932022095
batch: 1076
train_loss: 266.06262278556824
batch: 1077
train_loss: 269.46883249282837
batch: 1078
train_loss: 272.9154849052429
batch: 1079
train_loss: 276.35258054733276
batch: 1080
train_loss: 279.6787157058716
batch: 1081
train_loss: 283.1415946483612
batch: 1082
train_loss: 286.6113135814667
batch: 1083
train_loss: 290.093626499176
batch: 1084
train_loss: 293.55839252471924
batch: 1085
train_loss: 297.04321026802063
batch: 1086
train_loss: 300.49468970298767
batch: 1087
train_loss: 304.08140993118286
batch: 1088
train_loss: 307.66406750679016
batch: 1089
train_loss: 311.11916756629944
batch: 1090
train_loss: 314.5531463623047
batch: 1091
train_loss: 318.1032030582428
batch: 1092
train_loss: 321.66610527038574
batch: 1093
train_loss: 325.2334451675415
batch: 1094
train_loss: 328.7749698162079
batch: 1095
train_loss: 332.346408367157
batch: 1096
train_loss: 335.68208718299866
batch: 1097
train_loss: 339.0996952056885
batch: 1098
train_loss: 342.45398139953613
batch: 1099
train_loss: 345.8292546272278
batch: 1100
train_loss: 349.20474886894226
batch: 1101
train_loss: 352.58356046676636
batch: 1102
train_loss: 356.06149911880493
batch: 1103
train_loss: 359.6428499221802
batch: 1104
train_loss: 363.2185490131378
batch: 1105
train_loss: 366.6505675315857
batch: 1106
train_loss: 370.14354372024536
batch: 1107
train_loss: 373.56263613700867
batch: 1108
train_loss: 377.0291233062744
batch: 1109
train_loss: 380.530259847641
batch: 1110
train_loss: 384.0745539665222
batch: 1111
train_loss: 387.53704380989075
batch: 1112
train_loss: 390.98925852775574
batch: 1113
train_loss: 394.411497592926
batch: 1114
train_loss: 397.81059885025024
batch: 1115
train_loss: 401.21096563339233
batch: 1116
train_loss: 404.6822431087494
batch: 1117
train_loss: 408.0919871330261
batch: 1118
train_loss: 411.4613084793091
batch: 1119
train_loss: 414.75154972076416
batch: 1120
train_loss: 418.1301021575928
batch: 1121
train_loss: 421.6137754917145
batch: 1122
train_loss: 425.0697784423828
batch: 1123
train_loss: 428.4618330001831
batch: 1124
train_loss: 431.9167129993439
batch: 1125
train_loss: 435.4303870201111
batch: 1126
train_loss: 438.94096875190735
batch: 1127
train_loss: 442.3328900337219
batch: 1128
train_loss: 445.76883149147034
batch: 1129
train_loss: 449.1911835670471
batch: 1130
train_loss: 452.68615794181824
batch: 1131
train_loss: 455.9907052516937
batch: 1132
train_loss: 459.3745365142822
batch: 1133
train_loss: 462.6799910068512
batch: 1134
train_loss: 466.14814138412476
batch: 1135
train_loss: 469.5370407104492
batch: 1136
train_loss: 472.94962215423584
batch: 1137
train_loss: 476.37968707084656
batch: 1138
train_loss: 479.8067388534546
batch: 1139
train_loss: 483.20490407943726
batch: 1140
train_loss: 486.53683590888977
batch: 1141
train_loss: 489.89123344421387
batch: 1142
train_loss: 493.3803837299347
batch: 1143
train_loss: 496.8581097126007
batch: 1144
train_loss: 500.19279074668884
batch: 1145
train_loss: 503.652547121048
batch: 1146
train_loss: 507.03199100494385
batch: 1147
train_loss: 510.4643294811249
batch: 1148
train_loss: 513.9296746253967
batch: 1149
train_loss: 517.3453989028931
batch: 1150
train_loss: 520.701167345047
batch: 1151
train_loss: 524.0970673561096
batch: 1152
train_loss: 527.4641718864441
batch: 1153
train_loss: 530.791609287262
batch: 1154
train_loss: 534.1535501480103
batch: 1155
train_loss: 537.4855136871338
batch: 1156
train_loss: 540.846834897995
batch: 1157
train_loss: 544.248815536499
batch: 1158
train_loss: 547.6523611545563
batch: 1159
train_loss: 551.0891842842102
batch: 1160
train_loss: 554.6353940963745
batch: 1161
train_loss: 558.1724288463593
batch: 1162
train_loss: 561.7046415805817
batch: 1163
train_loss: 565.1971726417542
batch: 1164
train_loss: 568.6544578075409
batch: 1165
train_loss: 572.1646382808685
batch: 1166
train_loss: 575.66255402565
batch: 1167
train_loss: 579.1261138916016
batch: 1168
train_loss: 582.5495963096619
batch: 1169
train_loss: 585.9983077049255
batch: 1170
train_loss: 589.4615948200226
batch: 1171
train_loss: 592.9273297786713
batch: 1172
train_loss: 596.324197769165
batch: 1173
train_loss: 599.8159222602844
batch: 1174
train_loss: 603.2761013507843
batch: 1175
train_loss: 606.7343740463257
batch: 1176
train_loss: 610.106368303299
batch: 1177
train_loss: 613.4549567699432
batch: 1178
train_loss: 616.8595204353333
batch: 1179
train_loss: 620.30091547966
batch: 1180
train_loss: 623.642591714859
batch: 1181
train_loss: 626.9558386802673
batch: 1182
train_loss: 630.374039888382
batch: 1183
train_loss: 633.7836179733276
batch: 1184
train_loss: 637.1924664974213
batch: 1185
train_loss: 640.6331195831299
batch: 1186
train_loss: 644.0199711322784
batch: 1187
train_loss: 647.4294140338898
batch: 1188
train_loss: 650.8368153572083
batch: 1189
train_loss: 654.1710157394409
batch: 1190
train_loss: 657.6110203266144
batch: 1191
train_loss: 661.0775015354156
batch: 1192
train_loss: 664.4227674007416
batch: 1193
train_loss: 667.7692649364471
batch: 1194
train_loss: 671.1888253688812
batch: 1195
train_loss: 674.5429170131683
batch: 1196
train_loss: 677.8805294036865
batch: 1197
train_loss: 681.2239782810211
batch: 1198
train_loss: 684.577220916748
batch: 1199
train_loss: 687.9558098316193
| epoch   0 step     1200 |   1200 batches | lr 0.000249 | ms/batch 153.57 | loss  3.44 | bpc   4.96255
batch: 1200
train_loss: 3.4589836597442627
batch: 1201
train_loss: 6.91122579574585
batch: 1202
train_loss: 10.364571809768677
batch: 1203
train_loss: 13.85240888595581
batch: 1204
train_loss: 17.2956645488739
batch: 1205
train_loss: 20.821589469909668
batch: 1206
train_loss: 24.282171487808228
batch: 1207
train_loss: 27.753002166748047
batch: 1208
train_loss: 31.207449197769165
batch: 1209
train_loss: 34.642722845077515
batch: 1210
train_loss: 38.12578535079956
batch: 1211
train_loss: 41.57345938682556
batch: 1212
train_loss: 44.9922034740448
batch: 1213
train_loss: 48.372730016708374
batch: 1214
train_loss: 51.843735218048096
batch: 1215
train_loss: 55.291542768478394
batch: 1216
train_loss: 58.76022171974182
batch: 1217
train_loss: 62.12817192077637
batch: 1218
train_loss: 65.51293730735779
batch: 1219
train_loss: 69.00150752067566
batch: 1220
train_loss: 72.32402968406677
batch: 1221
train_loss: 75.64758086204529
batch: 1222
train_loss: 79.05111241340637
batch: 1223
train_loss: 82.44580936431885
batch: 1224
train_loss: 85.86228251457214
batch: 1225
train_loss: 89.23073363304138
batch: 1226
train_loss: 92.66001892089844
batch: 1227
train_loss: 95.97591495513916
batch: 1228
train_loss: 99.37296628952026
batch: 1229
train_loss: 102.7497193813324
batch: 1230
train_loss: 106.20321464538574
batch: 1231
train_loss: 109.5259485244751
batch: 1232
train_loss: 112.83283114433289
batch: 1233
train_loss: 116.15157270431519
batch: 1234
train_loss: 119.49816083908081
batch: 1235
train_loss: 122.81732821464539
batch: 1236
train_loss: 126.18638014793396
batch: 1237
train_loss: 129.61275792121887
batch: 1238
train_loss: 133.00749969482422
batch: 1239
train_loss: 136.42386984825134
batch: 1240
train_loss: 139.90556716918945
batch: 1241
train_loss: 143.33023738861084
batch: 1242
train_loss: 146.80229449272156
batch: 1243
train_loss: 150.27065229415894
batch: 1244
train_loss: 153.73133444786072
batch: 1245
train_loss: 157.25263810157776
batch: 1246
train_loss: 160.70551800727844
batch: 1247
train_loss: 164.08082222938538
batch: 1248
train_loss: 167.5622844696045
batch: 1249
train_loss: 170.98037219047546
batch: 1250
train_loss: 174.33734393119812
batch: 1251
train_loss: 177.74944710731506
batch: 1252
train_loss: 181.2227029800415
batch: 1253
train_loss: 184.70795059204102
batch: 1254
train_loss: 188.15934443473816
batch: 1255
train_loss: 191.5412962436676
batch: 1256
train_loss: 195.03355836868286
batch: 1257
train_loss: 198.45273089408875
batch: 1258
train_loss: 201.93173003196716
batch: 1259
train_loss: 205.4082474708557
batch: 1260
train_loss: 208.99026942253113
batch: 1261
train_loss: 212.43557596206665
batch: 1262
train_loss: 215.986013174057
batch: 1263
train_loss: 219.4041941165924
batch: 1264
train_loss: 222.8925347328186
batch: 1265
train_loss: 226.4625678062439
batch: 1266
train_loss: 229.9723892211914
batch: 1267
train_loss: 233.4349684715271
batch: 1268
train_loss: 236.9771511554718
batch: 1269
train_loss: 240.48540210723877
batch: 1270
train_loss: 244.10528564453125
batch: 1271
train_loss: 247.50127005577087
batch: 1272
train_loss: 250.92837047576904
batch: 1273
train_loss: 254.36305809020996
batch: 1274
train_loss: 257.9406900405884
batch: 1275
train_loss: 261.3757576942444
batch: 1276
train_loss: 264.8059878349304
batch: 1277
train_loss: 268.33521580696106
batch: 1278
train_loss: 271.9690885543823
batch: 1279
train_loss: 275.4569523334503
batch: 1280
train_loss: 279.0123653411865
batch: 1281
train_loss: 282.50307059288025
batch: 1282
train_loss: 285.89786529541016
batch: 1283
train_loss: 289.3331000804901
batch: 1284
train_loss: 292.78394293785095
batch: 1285
train_loss: 296.1919803619385
batch: 1286
train_loss: 299.6038944721222
batch: 1287
train_loss: 302.9660542011261
batch: 1288
train_loss: 306.2848105430603
batch: 1289
train_loss: 309.6646263599396
batch: 1290
train_loss: 313.08577489852905
batch: 1291
train_loss: 316.568692445755
batch: 1292
train_loss: 320.1018719673157
batch: 1293
train_loss: 323.5739104747772
batch: 1294
train_loss: 326.9970726966858
batch: 1295
train_loss: 330.50462198257446
batch: 1296
train_loss: 333.8723258972168
batch: 1297
train_loss: 337.29171228408813
batch: 1298
train_loss: 340.76061630249023
batch: 1299
train_loss: 344.28742146492004
batch: 1300
train_loss: 347.8217420578003
batch: 1301
train_loss: 351.38591051101685
batch: 1302
train_loss: 354.9708149433136
batch: 1303
train_loss: 358.41470289230347
batch: 1304
train_loss: 361.9710536003113
batch: 1305
train_loss: 365.5651309490204
batch: 1306
train_loss: 369.15156531333923
batch: 1307
train_loss: 372.7052626609802
batch: 1308
train_loss: 376.13321232795715
batch: 1309
train_loss: 379.59776973724365
batch: 1310
train_loss: 382.9851293563843
batch: 1311
train_loss: 386.45924067497253
batch: 1312
train_loss: 390.01895451545715
batch: 1313
train_loss: 393.5058333873749
batch: 1314
train_loss: 396.9010167121887
batch: 1315
train_loss: 400.3362798690796
batch: 1316
train_loss: 403.8002860546112
batch: 1317
train_loss: 407.2559394836426
batch: 1318
train_loss: 410.70740365982056
batch: 1319
train_loss: 414.13639211654663
batch: 1320
train_loss: 417.6228382587433
batch: 1321
train_loss: 420.9969594478607
batch: 1322
train_loss: 424.38962984085083
batch: 1323
train_loss: 427.78614687919617
batch: 1324
train_loss: 431.25067162513733
batch: 1325
train_loss: 434.68700528144836
batch: 1326
train_loss: 438.1475305557251
batch: 1327
train_loss: 441.65138697624207
batch: 1328
train_loss: 445.0302710533142
batch: 1329
train_loss: 448.49410462379456
batch: 1330
train_loss: 451.9251232147217
batch: 1331
train_loss: 455.35217332839966
batch: 1332
train_loss: 458.8091344833374
batch: 1333
train_loss: 462.19845628738403
batch: 1334
train_loss: 465.6597673892975
batch: 1335
train_loss: 469.23914647102356
batch: 1336
train_loss: 472.6169526576996
batch: 1337
train_loss: 476.0697588920593
batch: 1338
train_loss: 479.5895574092865
batch: 1339
train_loss: 483.0018916130066
batch: 1340
train_loss: 486.50391030311584
batch: 1341
train_loss: 489.89068937301636
batch: 1342
train_loss: 493.4117283821106
batch: 1343
train_loss: 496.8082048892975
batch: 1344
train_loss: 500.14086151123047
batch: 1345
train_loss: 503.5835602283478
batch: 1346
train_loss: 506.8623237609863
batch: 1347
train_loss: 510.2934000492096
batch: 1348
train_loss: 513.7791566848755
batch: 1349
train_loss: 517.1049358844757
batch: 1350
train_loss: 520.4682364463806
batch: 1351
train_loss: 523.8387005329132
batch: 1352
train_loss: 527.1781277656555
batch: 1353
train_loss: 530.5542821884155
batch: 1354
train_loss: 533.850168466568
batch: 1355
train_loss: 537.2150685787201
batch: 1356
train_loss: 540.6085691452026
batch: 1357
train_loss: 544.0003001689911
batch: 1358
train_loss: 547.3951771259308
batch: 1359
train_loss: 550.7624125480652
batch: 1360
train_loss: 554.1646862030029
batch: 1361
train_loss: 557.6409213542938
batch: 1362
train_loss: 560.9858577251434
batch: 1363
train_loss: 564.3009736537933
batch: 1364
train_loss: 567.6526644229889
batch: 1365
train_loss: 571.0507965087891
batch: 1366
train_loss: 574.54621052742
batch: 1367
train_loss: 577.9736807346344
batch: 1368
train_loss: 581.2504353523254
batch: 1369
train_loss: 584.6929252147675
batch: 1370
train_loss: 588.015876531601
batch: 1371
train_loss: 591.3865191936493
batch: 1372
train_loss: 594.8489162921906
batch: 1373
train_loss: 598.1260595321655
batch: 1374
train_loss: 601.6261470317841
batch: 1375
train_loss: 605.0649862289429
batch: 1376
train_loss: 608.4444754123688
batch: 1377
train_loss: 611.7551686763763
batch: 1378
train_loss: 615.0572276115417
batch: 1379
train_loss: 618.5187337398529
batch: 1380
train_loss: 621.9416236877441
batch: 1381
train_loss: 625.3632385730743
batch: 1382
train_loss: 628.7787055969238
batch: 1383
train_loss: 632.1943891048431
batch: 1384
train_loss: 635.6522672176361
batch: 1385
train_loss: 639.0377471446991
batch: 1386
train_loss: 642.3813767433167
batch: 1387
train_loss: 645.7654755115509
batch: 1388
train_loss: 649.2356913089752
batch: 1389
train_loss: 652.733008146286
batch: 1390
train_loss: 656.2235133647919
batch: 1391
train_loss: 659.6405892372131
batch: 1392
train_loss: 662.9799339771271
batch: 1393
train_loss: 666.4184510707855
batch: 1394
train_loss: 669.8132615089417
batch: 1395
train_loss: 673.2187407016754
batch: 1396
train_loss: 676.6294274330139
batch: 1397
train_loss: 680.0427179336548
batch: 1398
train_loss: 683.4401643276215
batch: 1399
train_loss: 686.849187374115
| epoch   0 step     1400 |   1400 batches | lr 0.000249 | ms/batch 153.16 | loss  3.43 | bpc   4.95457
batch: 1400
train_loss: 3.397840976715088
batch: 1401
train_loss: 6.851628065109253
batch: 1402
train_loss: 10.113231658935547
batch: 1403
train_loss: 13.41126537322998
batch: 1404
train_loss: 16.796301126480103
batch: 1405
train_loss: 20.206109285354614
batch: 1406
train_loss: 23.534682989120483
batch: 1407
train_loss: 26.925535440444946
batch: 1408
train_loss: 30.293620109558105
batch: 1409
train_loss: 33.74122595787048
batch: 1410
train_loss: 37.20990180969238
batch: 1411
train_loss: 40.58498978614807
batch: 1412
train_loss: 43.97990107536316
batch: 1413
train_loss: 47.347381591796875
batch: 1414
train_loss: 50.718679904937744
batch: 1415
train_loss: 54.1178183555603
batch: 1416
train_loss: 57.544129848480225
batch: 1417
train_loss: 60.931413412094116
batch: 1418
train_loss: 64.29574823379517
batch: 1419
train_loss: 67.79888153076172
batch: 1420
train_loss: 71.22820663452148
batch: 1421
train_loss: 74.65487217903137
batch: 1422
train_loss: 78.06493401527405
batch: 1423
train_loss: 81.47605395317078
batch: 1424
train_loss: 84.96487855911255
batch: 1425
train_loss: 88.34280967712402
batch: 1426
train_loss: 91.8201961517334
batch: 1427
train_loss: 95.18528938293457
batch: 1428
train_loss: 98.50934720039368
batch: 1429
train_loss: 101.84065508842468
batch: 1430
train_loss: 105.26321649551392
batch: 1431
train_loss: 108.6627790927887
batch: 1432
train_loss: 112.02353644371033
batch: 1433
train_loss: 115.52791857719421
batch: 1434
train_loss: 118.81896877288818
batch: 1435
train_loss: 122.16559863090515
batch: 1436
train_loss: 125.44992423057556
batch: 1437
train_loss: 128.71474504470825
batch: 1438
train_loss: 132.0386221408844
batch: 1439
train_loss: 135.39495587348938
batch: 1440
train_loss: 138.79796481132507
batch: 1441
train_loss: 142.16793394088745
batch: 1442
train_loss: 145.56482124328613
batch: 1443
train_loss: 148.93798661231995
batch: 1444
train_loss: 152.36987209320068
batch: 1445
train_loss: 155.7188115119934
batch: 1446
train_loss: 159.1444697380066
batch: 1447
train_loss: 162.51379895210266
batch: 1448
train_loss: 165.9438555240631
batch: 1449
train_loss: 169.36896133422852
batch: 1450
train_loss: 172.6945765018463
batch: 1451
train_loss: 176.0617754459381
batch: 1452
train_loss: 179.49493503570557
batch: 1453
train_loss: 182.79147362709045
batch: 1454
train_loss: 186.11289858818054
batch: 1455
train_loss: 189.40090227127075
batch: 1456
train_loss: 192.71822261810303
batch: 1457
train_loss: 196.03488850593567
batch: 1458
train_loss: 199.35142588615417
batch: 1459
train_loss: 202.74766850471497
batch: 1460
train_loss: 206.04819869995117
batch: 1461
train_loss: 209.37802481651306
batch: 1462
train_loss: 212.66848587989807
batch: 1463
train_loss: 215.95365571975708
batch: 1464
train_loss: 219.24436259269714
batch: 1465
train_loss: 222.52256155014038
batch: 1466
train_loss: 225.73462677001953
batch: 1467
train_loss: 229.0151069164276
batch: 1468
train_loss: 232.40021228790283
batch: 1469
train_loss: 235.81241178512573
batch: 1470
train_loss: 239.16908621788025
batch: 1471
train_loss: 242.50747966766357
batch: 1472
train_loss: 245.92983508110046
batch: 1473
train_loss: 249.3813157081604
batch: 1474
train_loss: 252.7199923992157
batch: 1475
train_loss: 256.1447629928589
batch: 1476
train_loss: 259.57174134254456
batch: 1477
train_loss: 262.9379017353058
batch: 1478
train_loss: 266.3550066947937
batch: 1479
train_loss: 269.6927704811096
batch: 1480
train_loss: 273.0112762451172
batch: 1481
train_loss: 276.38836574554443
batch: 1482
train_loss: 279.6947453022003
batch: 1483
train_loss: 283.09813046455383
batch: 1484
train_loss: 286.4029974937439
batch: 1485
train_loss: 289.7824318408966
batch: 1486
train_loss: 293.2895128726959
batch: 1487
train_loss: 296.6697635650635
batch: 1488
train_loss: 300.0697681903839
batch: 1489
train_loss: 303.3471448421478
batch: 1490
train_loss: 306.72885274887085
batch: 1491
train_loss: 310.1616406440735
batch: 1492
train_loss: 313.5471248626709
batch: 1493
train_loss: 316.8912763595581
batch: 1494
train_loss: 320.1835706233978
batch: 1495
train_loss: 323.49952363967896
batch: 1496
train_loss: 326.85005235671997
batch: 1497
train_loss: 330.22610211372375
batch: 1498
train_loss: 333.58234667778015
batch: 1499
train_loss: 336.84723711013794
batch: 1500
train_loss: 340.1957950592041
batch: 1501
train_loss: 343.4051671028137
batch: 1502
train_loss: 346.76400804519653
batch: 1503
train_loss: 350.0762667655945
batch: 1504
train_loss: 353.4261486530304
batch: 1505
train_loss: 356.77077293395996
batch: 1506
train_loss: 360.1688516139984
batch: 1507
train_loss: 363.5245623588562
batch: 1508
train_loss: 366.9634048938751
batch: 1509
train_loss: 370.31845784187317
batch: 1510
train_loss: 373.66505002975464
batch: 1511
train_loss: 377.01991057395935
batch: 1512
train_loss: 380.33083724975586
batch: 1513
train_loss: 383.67835211753845
batch: 1514
train_loss: 387.04676723480225
batch: 1515
train_loss: 390.37008476257324
batch: 1516
train_loss: 393.5901484489441
batch: 1517
train_loss: 396.8449945449829
batch: 1518
train_loss: 400.1839852333069
batch: 1519
train_loss: 403.5236232280731
batch: 1520
train_loss: 406.854514837265
batch: 1521
train_loss: 410.26158595085144
batch: 1522
train_loss: 413.63615322113037
batch: 1523
train_loss: 416.9059453010559
batch: 1524
train_loss: 420.3038058280945
batch: 1525
train_loss: 423.5956211090088
batch: 1526
train_loss: 426.8602774143219
batch: 1527
train_loss: 430.21388244628906
batch: 1528
train_loss: 433.5323827266693
batch: 1529
train_loss: 436.8939070701599
batch: 1530
train_loss: 440.354129076004
batch: 1531
train_loss: 443.7646827697754
batch: 1532
train_loss: 447.09461307525635
batch: 1533
train_loss: 450.4837200641632
batch: 1534
train_loss: 453.8679621219635
batch: 1535
train_loss: 457.2310299873352
batch: 1536
train_loss: 460.59134554862976
batch: 1537
train_loss: 464.05837893486023
batch: 1538
train_loss: 467.5679988861084
batch: 1539
train_loss: 471.02478289604187
batch: 1540
train_loss: 474.4542353153229
batch: 1541
train_loss: 477.92329120635986
batch: 1542
train_loss: 481.326904296875
batch: 1543
train_loss: 484.6846342086792
batch: 1544
train_loss: 487.99340891838074
batch: 1545
train_loss: 491.35581731796265
batch: 1546
train_loss: 494.82828974723816
batch: 1547
train_loss: 498.1914339065552
batch: 1548
train_loss: 501.5828456878662
batch: 1549
train_loss: 504.8498868942261
batch: 1550
train_loss: 508.17556643486023
batch: 1551
train_loss: 511.47664523124695
batch: 1552
train_loss: 514.7629907131195
batch: 1553
train_loss: 518.2026567459106
batch: 1554
train_loss: 521.4705543518066
batch: 1555
train_loss: 524.7964441776276
batch: 1556
train_loss: 528.1043734550476
batch: 1557
train_loss: 531.3999178409576
batch: 1558
train_loss: 534.7194077968597
batch: 1559
train_loss: 538.0236763954163
batch: 1560
train_loss: 541.2316348552704
batch: 1561
train_loss: 544.5674564838409
batch: 1562
train_loss: 547.8450694084167
batch: 1563
train_loss: 551.1579701900482
batch: 1564
train_loss: 554.4834735393524
batch: 1565
train_loss: 557.8100385665894
batch: 1566
train_loss: 561.1213111877441
batch: 1567
train_loss: 564.4415862560272
batch: 1568
train_loss: 567.7304589748383
batch: 1569
train_loss: 571.0348427295685
batch: 1570
train_loss: 574.3872444629669
batch: 1571
train_loss: 577.7530906200409
batch: 1572
train_loss: 581.0502545833588
batch: 1573
train_loss: 584.3908286094666
batch: 1574
train_loss: 587.7011983394623
batch: 1575
train_loss: 591.1143317222595
batch: 1576
train_loss: 594.406699180603
batch: 1577
train_loss: 597.7328758239746
batch: 1578
train_loss: 601.0997314453125
batch: 1579
train_loss: 604.4846389293671
batch: 1580
train_loss: 607.8023400306702
batch: 1581
train_loss: 611.1873588562012
batch: 1582
train_loss: 614.4785223007202
batch: 1583
train_loss: 617.6569592952728
batch: 1584
train_loss: 620.8928380012512
batch: 1585
train_loss: 624.4452602863312
batch: 1586
train_loss: 627.8058986663818
batch: 1587
train_loss: 631.0378103256226
batch: 1588
train_loss: 634.3212659358978
batch: 1589
train_loss: 637.7643530368805
batch: 1590
train_loss: 641.2010803222656
batch: 1591
train_loss: 644.5133583545685
batch: 1592
train_loss: 647.8132510185242
batch: 1593
train_loss: 651.1510219573975
batch: 1594
train_loss: 654.426873922348
batch: 1595
train_loss: 657.7076416015625
batch: 1596
train_loss: 661.0832724571228
batch: 1597
train_loss: 664.3857259750366
batch: 1598
train_loss: 667.6438977718353
batch: 1599
train_loss: 670.9840483665466
| epoch   0 step     1600 |   1600 batches | lr 0.000249 | ms/batch 152.54 | loss  3.35 | bpc   4.84013
batch: 1600
train_loss: 3.3127737045288086
batch: 1601
train_loss: 6.6174232959747314
batch: 1602
train_loss: 9.974048852920532
batch: 1603
train_loss: 13.243374824523926
batch: 1604
train_loss: 16.57698678970337
batch: 1605
train_loss: 19.957929849624634
batch: 1606
train_loss: 23.32551336288452
batch: 1607
train_loss: 26.796276092529297
batch: 1608
train_loss: 30.218928337097168
batch: 1609
train_loss: 33.5454466342926
batch: 1610
train_loss: 36.89412593841553
batch: 1611
train_loss: 40.268503189086914
batch: 1612
train_loss: 43.719849586486816
batch: 1613
train_loss: 47.23836851119995
batch: 1614
train_loss: 50.65897560119629
batch: 1615
train_loss: 54.02900052070618
batch: 1616
train_loss: 57.37068581581116
batch: 1617
train_loss: 60.75866746902466
batch: 1618
train_loss: 64.08132839202881
batch: 1619
train_loss: 67.53819417953491
batch: 1620
train_loss: 70.83779525756836
batch: 1621
train_loss: 74.2066490650177
batch: 1622
train_loss: 77.59471130371094
batch: 1623
train_loss: 80.96317839622498
batch: 1624
train_loss: 84.34535145759583
batch: 1625
train_loss: 87.70051789283752
batch: 1626
train_loss: 91.09761667251587
batch: 1627
train_loss: 94.33260869979858
batch: 1628
train_loss: 97.73126602172852
batch: 1629
train_loss: 101.31509566307068
batch: 1630
train_loss: 104.59728384017944
batch: 1631
train_loss: 108.07254028320312
batch: 1632
train_loss: 111.4248948097229
batch: 1633
train_loss: 114.81106066703796
batch: 1634
train_loss: 118.17980098724365
batch: 1635
train_loss: 121.46643805503845
batch: 1636
train_loss: 124.77553701400757
batch: 1637
train_loss: 128.232360124588
batch: 1638
train_loss: 131.55708646774292
batch: 1639
train_loss: 134.87333726882935
batch: 1640
train_loss: 138.17128014564514
batch: 1641
train_loss: 141.44495701789856
batch: 1642
train_loss: 144.72754430770874
batch: 1643
train_loss: 148.0824966430664
batch: 1644
train_loss: 151.43328714370728
batch: 1645
train_loss: 154.78838205337524
batch: 1646
train_loss: 158.17737364768982
batch: 1647
train_loss: 161.6101884841919
batch: 1648
train_loss: 165.02849125862122
batch: 1649
train_loss: 168.52605724334717
batch: 1650
train_loss: 172.01903796195984
batch: 1651
train_loss: 175.371248960495
batch: 1652
train_loss: 178.69159197807312
batch: 1653
train_loss: 182.02950716018677
batch: 1654
train_loss: 185.33704280853271
batch: 1655
train_loss: 188.64442825317383
batch: 1656
train_loss: 191.99168848991394
batch: 1657
train_loss: 195.33826756477356
batch: 1658
train_loss: 198.66388249397278
batch: 1659
train_loss: 201.9376096725464
batch: 1660
train_loss: 205.26306343078613
batch: 1661
train_loss: 208.5922191143036
batch: 1662
train_loss: 211.91192293167114
batch: 1663
train_loss: 215.26060938835144
batch: 1664
train_loss: 218.58802127838135
batch: 1665
train_loss: 221.8188967704773
batch: 1666
train_loss: 225.09080624580383
batch: 1667
train_loss: 228.3745241165161
batch: 1668
train_loss: 231.7201509475708
batch: 1669
train_loss: 235.0262894630432
batch: 1670
train_loss: 238.36151671409607
batch: 1671
train_loss: 241.68495965003967
batch: 1672
train_loss: 245.0215198993683
batch: 1673
train_loss: 248.39886140823364
batch: 1674
train_loss: 251.73992729187012
batch: 1675
train_loss: 255.09968161582947
batch: 1676
train_loss: 258.4133269786835
batch: 1677
train_loss: 261.7833824157715
batch: 1678
train_loss: 265.15878438949585
batch: 1679
train_loss: 268.4161412715912
batch: 1680
train_loss: 271.7649030685425
batch: 1681
train_loss: 275.1061222553253
batch: 1682
train_loss: 278.31926226615906
batch: 1683
train_loss: 281.5843982696533
batch: 1684
train_loss: 284.97586035728455
batch: 1685
train_loss: 288.3435046672821
batch: 1686
train_loss: 291.6001501083374
batch: 1687
train_loss: 294.85333704948425
batch: 1688
train_loss: 298.09710669517517
batch: 1689
train_loss: 301.4133985042572
batch: 1690
train_loss: 304.78117060661316
batch: 1691
train_loss: 308.1583020687103
batch: 1692
train_loss: 311.3822033405304
batch: 1693
train_loss: 314.67709493637085
batch: 1694
train_loss: 318.03903126716614
batch: 1695
train_loss: 321.44669246673584
batch: 1696
train_loss: 324.8579716682434
batch: 1697
train_loss: 328.25277519226074
batch: 1698
train_loss: 331.68821144104004
batch: 1699
train_loss: 334.9941318035126
batch: 1700
train_loss: 338.31780910491943
batch: 1701
train_loss: 341.630797624588
batch: 1702
train_loss: 344.96408462524414
batch: 1703
train_loss: 348.36525797843933
batch: 1704
train_loss: 351.6935431957245
batch: 1705
train_loss: 354.9351828098297
batch: 1706
train_loss: 358.2041001319885
batch: 1707
train_loss: 361.52248978614807
batch: 1708
train_loss: 364.8881924152374
batch: 1709
train_loss: 368.2223768234253
batch: 1710
train_loss: 371.5425727367401
batch: 1711
train_loss: 374.85074758529663
batch: 1712
train_loss: 378.1259160041809
batch: 1713
train_loss: 381.34012746810913
batch: 1714
train_loss: 384.6554067134857
batch: 1715
train_loss: 387.96428513526917
batch: 1716
train_loss: 391.3363537788391
batch: 1717
train_loss: 394.6821196079254
batch: 1718
train_loss: 397.92200231552124
batch: 1719
train_loss: 401.2473633289337
batch: 1720
train_loss: 404.4390769004822
batch: 1721
train_loss: 407.7911605834961
batch: 1722
train_loss: 411.10098934173584
batch: 1723
train_loss: 414.4786550998688
batch: 1724
train_loss: 417.7835292816162
batch: 1725
train_loss: 421.01455664634705
batch: 1726
train_loss: 424.2115375995636
batch: 1727
train_loss: 427.54755544662476
batch: 1728
train_loss: 430.8389105796814
batch: 1729
train_loss: 434.09265184402466
batch: 1730
train_loss: 437.3827688694
batch: 1731
train_loss: 440.5861234664917
batch: 1732
train_loss: 443.7891731262207
batch: 1733
train_loss: 447.03780126571655
batch: 1734
train_loss: 450.28824973106384
batch: 1735
train_loss: 453.5252959728241
batch: 1736
train_loss: 456.7750449180603
batch: 1737
train_loss: 460.0995168685913
batch: 1738
train_loss: 463.4503126144409
batch: 1739
train_loss: 466.7659842967987
batch: 1740
train_loss: 470.07137846946716
batch: 1741
train_loss: 473.2079575061798
batch: 1742
train_loss: 476.4703047275543
batch: 1743
train_loss: 479.7604179382324
batch: 1744
train_loss: 482.9929549694061
batch: 1745
train_loss: 486.31215739250183
batch: 1746
train_loss: 489.56585216522217
batch: 1747
train_loss: 492.79794120788574
batch: 1748
train_loss: 496.0229012966156
batch: 1749
train_loss: 499.29058480262756
batch: 1750
train_loss: 502.59310579299927
batch: 1751
train_loss: 505.8839168548584
batch: 1752
train_loss: 509.1509311199188
batch: 1753
train_loss: 512.3786234855652
batch: 1754
train_loss: 515.6144757270813
batch: 1755
train_loss: 518.8311769962311
batch: 1756
train_loss: 522.0193538665771
batch: 1757
train_loss: 525.2561502456665
batch: 1758
train_loss: 528.4526741504669
batch: 1759
train_loss: 531.6431620121002
batch: 1760
train_loss: 534.8796417713165
batch: 1761
train_loss: 538.1222968101501
batch: 1762
train_loss: 541.38325548172
batch: 1763
train_loss: 544.647438287735
batch: 1764
train_loss: 547.9039611816406
batch: 1765
train_loss: 551.169979095459
batch: 1766
train_loss: 554.3537430763245
batch: 1767
train_loss: 557.5576727390289
batch: 1768
train_loss: 560.7615196704865
batch: 1769
train_loss: 563.9530098438263
batch: 1770
train_loss: 567.1664698123932
batch: 1771
train_loss: 570.3798944950104
batch: 1772
train_loss: 573.5818424224854
batch: 1773
train_loss: 576.8292889595032
batch: 1774
train_loss: 580.1704783439636
batch: 1775
train_loss: 583.5466175079346
batch: 1776
train_loss: 586.9014408588409
batch: 1777
train_loss: 590.1986627578735
batch: 1778
train_loss: 593.4978883266449
batch: 1779
train_loss: 596.8294992446899
batch: 1780
train_loss: 600.0558733940125
batch: 1781
train_loss: 603.2567613124847
batch: 1782
train_loss: 606.6447539329529
batch: 1783
train_loss: 609.8494725227356
batch: 1784
train_loss: 613.1166586875916
batch: 1785
train_loss: 616.3870916366577
batch: 1786
train_loss: 619.5754482746124
batch: 1787
train_loss: 622.8392436504364
batch: 1788
train_loss: 626.1814646720886
batch: 1789
train_loss: 629.4592235088348
batch: 1790
train_loss: 632.7041611671448
batch: 1791
train_loss: 636.0121622085571
batch: 1792
train_loss: 639.2775292396545
batch: 1793
train_loss: 642.551123380661
batch: 1794
train_loss: 645.8325815200806
batch: 1795
train_loss: 649.2608947753906
batch: 1796
train_loss: 652.6777787208557
batch: 1797
train_loss: 656.1089129447937
batch: 1798
train_loss: 659.4393489360809
batch: 1799
train_loss: 662.8131823539734
| epoch   0 step     1800 |   1800 batches | lr 0.000249 | ms/batch 153.21 | loss  3.31 | bpc   4.78119
batch: 1800
train_loss: 3.2943570613861084
batch: 1801
train_loss: 6.623789548873901
batch: 1802
train_loss: 9.944320440292358
batch: 1803
train_loss: 13.300267457962036
batch: 1804
train_loss: 16.694705486297607
batch: 1805
train_loss: 20.04558825492859
batch: 1806
train_loss: 23.34557056427002
batch: 1807
train_loss: 26.810320615768433
batch: 1808
train_loss: 30.215288877487183
batch: 1809
train_loss: 33.54554629325867
batch: 1810
train_loss: 36.9177885055542
batch: 1811
train_loss: 40.28855776786804
batch: 1812
train_loss: 43.57803773880005
batch: 1813
train_loss: 46.795387744903564
batch: 1814
train_loss: 50.11504769325256
batch: 1815
train_loss: 53.36309862136841
batch: 1816
train_loss: 56.60785794258118
batch: 1817
train_loss: 59.94876003265381
batch: 1818
train_loss: 63.390273094177246
batch: 1819
train_loss: 66.698561668396
batch: 1820
train_loss: 70.04959630966187
batch: 1821
train_loss: 73.33944773674011
batch: 1822
train_loss: 76.68707704544067
batch: 1823
train_loss: 79.96835660934448
batch: 1824
train_loss: 83.30997443199158
batch: 1825
train_loss: 86.57552766799927
batch: 1826
train_loss: 89.94853615760803
batch: 1827
train_loss: 93.31309509277344
batch: 1828
train_loss: 96.68925094604492
batch: 1829
train_loss: 100.03984832763672
batch: 1830
train_loss: 103.39394068717957
batch: 1831
train_loss: 106.80279064178467
batch: 1832
train_loss: 110.17014908790588
batch: 1833
train_loss: 113.63407707214355
batch: 1834
train_loss: 117.00595378875732
batch: 1835
train_loss: 120.321932554245
batch: 1836
train_loss: 123.58128714561462
batch: 1837
train_loss: 126.95374894142151
batch: 1838
train_loss: 130.21833395957947
batch: 1839
train_loss: 133.54921531677246
batch: 1840
train_loss: 136.95713257789612
batch: 1841
train_loss: 140.27130651474
batch: 1842
train_loss: 143.5976104736328
batch: 1843
train_loss: 147.0029640197754
batch: 1844
train_loss: 150.42603754997253
batch: 1845
train_loss: 153.830073595047
batch: 1846
train_loss: 157.19777512550354
batch: 1847
train_loss: 160.63174557685852
batch: 1848
train_loss: 163.914617061615
batch: 1849
train_loss: 167.23454785346985
batch: 1850
train_loss: 170.61355471611023
batch: 1851
train_loss: 173.9886212348938
batch: 1852
train_loss: 177.3279209136963
batch: 1853
train_loss: 180.62293577194214
batch: 1854
train_loss: 183.84626030921936
batch: 1855
train_loss: 187.14796233177185
batch: 1856
train_loss: 190.48603677749634
batch: 1857
train_loss: 193.8416039943695
batch: 1858
train_loss: 197.16912269592285
batch: 1859
train_loss: 200.45603132247925
batch: 1860
train_loss: 203.70977568626404
batch: 1861
train_loss: 206.9620180130005
batch: 1862
train_loss: 210.2249252796173
batch: 1863
train_loss: 213.46063232421875
batch: 1864
train_loss: 216.83483576774597
batch: 1865
train_loss: 220.14828848838806
batch: 1866
train_loss: 223.48271942138672
batch: 1867
train_loss: 226.80505681037903
batch: 1868
train_loss: 230.09425258636475
batch: 1869
train_loss: 233.44966578483582
batch: 1870
train_loss: 236.79099035263062
batch: 1871
train_loss: 240.0860252380371
batch: 1872
train_loss: 243.43898916244507
batch: 1873
train_loss: 246.83664059638977
batch: 1874
train_loss: 250.19695949554443
batch: 1875
train_loss: 253.47785019874573
batch: 1876
train_loss: 256.9065008163452
batch: 1877
train_loss: 260.2270977497101
batch: 1878
train_loss: 263.66297674179077
batch: 1879
train_loss: 266.9994339942932
batch: 1880
train_loss: 270.33711791038513
batch: 1881
train_loss: 273.6066415309906
batch: 1882
train_loss: 276.9088637828827
batch: 1883
train_loss: 280.23494052886963
batch: 1884
train_loss: 283.5575873851776
batch: 1885
train_loss: 286.831077337265
batch: 1886
train_loss: 290.13438987731934
batch: 1887
train_loss: 293.508287191391
batch: 1888
train_loss: 296.940468788147
batch: 1889
train_loss: 300.1649127006531
batch: 1890
train_loss: 303.45635080337524
batch: 1891
train_loss: 306.7546560764313
batch: 1892
train_loss: 310.06857442855835
batch: 1893
train_loss: 313.4502122402191
batch: 1894
train_loss: 316.78318548202515
batch: 1895
train_loss: 320.1963720321655
batch: 1896
train_loss: 323.55302119255066
batch: 1897
train_loss: 326.93670439720154
batch: 1898
train_loss: 330.2119598388672
batch: 1899
train_loss: 333.5186026096344
batch: 1900
train_loss: 336.9225149154663
batch: 1901
train_loss: 340.3159329891205
batch: 1902
train_loss: 343.7480854988098
batch: 1903
train_loss: 347.0923926830292
batch: 1904
train_loss: 350.3935294151306
batch: 1905
train_loss: 353.82197880744934
batch: 1906
train_loss: 357.1456949710846
batch: 1907
train_loss: 360.41220569610596
batch: 1908
train_loss: 363.75839352607727
batch: 1909
train_loss: 367.1013431549072
batch: 1910
train_loss: 370.44579672813416
batch: 1911
train_loss: 373.71739768981934
batch: 1912
train_loss: 376.9749402999878
batch: 1913
train_loss: 380.2671127319336
batch: 1914
train_loss: 383.620667219162
batch: 1915
train_loss: 386.9877145290375
batch: 1916
train_loss: 390.3450264930725
batch: 1917
train_loss: 393.7225503921509
batch: 1918
train_loss: 396.9832055568695
batch: 1919
train_loss: 400.34589314460754
batch: 1920
train_loss: 403.57468485832214
batch: 1921
train_loss: 406.8495042324066
batch: 1922
train_loss: 410.1900997161865
batch: 1923
train_loss: 413.5405683517456
batch: 1924
train_loss: 416.8923764228821
batch: 1925
train_loss: 420.2160642147064
batch: 1926
train_loss: 423.68558621406555
batch: 1927
train_loss: 427.04502630233765
batch: 1928
train_loss: 430.5091474056244
batch: 1929
train_loss: 433.92565274238586
batch: 1930
train_loss: 437.4208884239197
batch: 1931
train_loss: 440.84805154800415
batch: 1932
train_loss: 444.2385528087616
batch: 1933
train_loss: 447.6838493347168
batch: 1934
train_loss: 451.0429718494415
batch: 1935
train_loss: 454.47515630722046
batch: 1936
train_loss: 457.8220236301422
batch: 1937
train_loss: 461.12211537361145
batch: 1938
train_loss: 464.58042430877686
batch: 1939
train_loss: 468.1153094768524
batch: 1940
train_loss: 471.4824159145355
batch: 1941
train_loss: 474.9058082103729
batch: 1942
train_loss: 478.2237057685852
batch: 1943
train_loss: 481.51286125183105
batch: 1944
train_loss: 484.9529764652252
batch: 1945
train_loss: 488.28809785842896
batch: 1946
train_loss: 491.5709011554718
batch: 1947
train_loss: 494.8962068557739
batch: 1948
train_loss: 498.23434925079346
batch: 1949
train_loss: 501.58713483810425
batch: 1950
train_loss: 504.9760766029358
batch: 1951
train_loss: 508.37490725517273
batch: 1952
train_loss: 511.71949648857117
batch: 1953
train_loss: 515.0558307170868
batch: 1954
train_loss: 518.3283474445343
batch: 1955
train_loss: 521.7671644687653
batch: 1956
train_loss: 525.1621990203857
batch: 1957
train_loss: 528.415486574173
batch: 1958
train_loss: 531.7462191581726
batch: 1959
train_loss: 535.1330487728119
batch: 1960
train_loss: 538.3261408805847
batch: 1961
train_loss: 541.5712101459503
batch: 1962
train_loss: 544.9177060127258
batch: 1963
train_loss: 548.2942769527435
batch: 1964
train_loss: 551.5632483959198
batch: 1965
train_loss: 554.8849585056305
batch: 1966
train_loss: 558.2660458087921
batch: 1967
train_loss: 561.5689630508423
batch: 1968
train_loss: 564.9343569278717
batch: 1969
train_loss: 568.4274263381958
batch: 1970
train_loss: 571.8790218830109
batch: 1971
train_loss: 575.2229068279266
batch: 1972
train_loss: 578.5020015239716
batch: 1973
train_loss: 581.7608370780945
batch: 1974
train_loss: 585.0724902153015
batch: 1975
train_loss: 588.3944408893585
batch: 1976
train_loss: 591.8080153465271
batch: 1977
train_loss: 595.0767648220062
batch: 1978
train_loss: 598.4320366382599
batch: 1979
train_loss: 601.7743492126465
batch: 1980
train_loss: 605.1261878013611
batch: 1981
train_loss: 608.478414773941
batch: 1982
train_loss: 611.6644887924194
batch: 1983
train_loss: 614.881067276001
batch: 1984
train_loss: 618.1051502227783
batch: 1985
train_loss: 621.3440701961517
batch: 1986
train_loss: 624.6103026866913
batch: 1987
train_loss: 627.9499890804291
batch: 1988
train_loss: 631.2109084129333
batch: 1989
train_loss: 634.436686038971
batch: 1990
train_loss: 637.7687408924103
batch: 1991
train_loss: 641.1087019443512
batch: 1992
train_loss: 644.4473173618317
batch: 1993
train_loss: 647.8130521774292
batch: 1994
train_loss: 651.1677114963531
batch: 1995
train_loss: 654.5061912536621
batch: 1996
train_loss: 657.77698802948
batch: 1997
train_loss: 661.0389699935913
batch: 1998
train_loss: 664.4091899394989
batch: 1999
train_loss: 667.7170977592468
| epoch   0 step     2000 |   2000 batches | lr 0.000248 | ms/batch 153.31 | loss  3.34 | bpc   4.81656
batch: 2000
train_loss: 3.3187291622161865
batch: 2001
train_loss: 6.686834335327148
batch: 2002
train_loss: 9.980423927307129
batch: 2003
train_loss: 13.259858846664429
batch: 2004
train_loss: 16.595168590545654
batch: 2005
train_loss: 19.958470344543457
batch: 2006
train_loss: 23.336209297180176
batch: 2007
train_loss: 26.731660842895508
batch: 2008
train_loss: 30.01420497894287
batch: 2009
train_loss: 33.302048206329346
batch: 2010
train_loss: 36.54793047904968
batch: 2011
train_loss: 39.89649176597595
batch: 2012
train_loss: 43.177122354507446
batch: 2013
train_loss: 46.42642831802368
batch: 2014
train_loss: 49.78595972061157
batch: 2015
train_loss: 53.12963509559631
batch: 2016
train_loss: 56.49425768852234
batch: 2017
train_loss: 59.88520526885986
batch: 2018
train_loss: 63.1851863861084
batch: 2019
train_loss: 66.46179294586182
batch: 2020
train_loss: 69.75790047645569
batch: 2021
train_loss: 73.08097124099731
batch: 2022
train_loss: 76.3728551864624
batch: 2023
train_loss: 79.72574996948242
batch: 2024
train_loss: 83.06444144248962
batch: 2025
train_loss: 86.42630124092102
batch: 2026
train_loss: 89.8499186038971
batch: 2027
train_loss: 93.19707870483398
batch: 2028
train_loss: 96.46344542503357
batch: 2029
train_loss: 99.85427260398865
batch: 2030
train_loss: 103.34077525138855
batch: 2031
train_loss: 106.66410207748413
batch: 2032
train_loss: 109.9168450832367
batch: 2033
train_loss: 113.11059832572937
batch: 2034
train_loss: 116.32645750045776
batch: 2035
train_loss: 119.59703612327576
batch: 2036
train_loss: 122.81969594955444
batch: 2037
train_loss: 126.08081293106079
batch: 2038
train_loss: 129.2913374900818
batch: 2039
train_loss: 132.50642681121826
batch: 2040
train_loss: 135.6889364719391
batch: 2041
train_loss: 138.89360404014587
batch: 2042
train_loss: 142.17248487472534
batch: 2043
train_loss: 145.4568383693695
batch: 2044
train_loss: 148.76184558868408
batch: 2045
train_loss: 152.02878522872925
batch: 2046
train_loss: 155.3010151386261
batch: 2047
train_loss: 158.60355472564697
batch: 2048
train_loss: 162.01599049568176
batch: 2049
train_loss: 165.2746386528015
batch: 2050
train_loss: 168.49444389343262
batch: 2051
train_loss: 171.67299604415894
batch: 2052
train_loss: 174.87490272521973
batch: 2053
train_loss: 178.1119191646576
batch: 2054
train_loss: 181.37251496315002
batch: 2055
train_loss: 184.67521476745605
batch: 2056
train_loss: 187.9541027545929
batch: 2057
train_loss: 191.1448004245758
batch: 2058
train_loss: 194.48572540283203
batch: 2059
train_loss: 197.85798048973083
batch: 2060
train_loss: 201.22279500961304
batch: 2061
train_loss: 204.64143991470337
batch: 2062
train_loss: 207.99347257614136
batch: 2063
train_loss: 211.3110270500183
batch: 2064
train_loss: 214.654714345932
batch: 2065
train_loss: 217.86938166618347
batch: 2066
train_loss: 221.0891716480255
batch: 2067
train_loss: 224.21807503700256
batch: 2068
train_loss: 227.49285697937012
batch: 2069
train_loss: 230.7981824874878
batch: 2070
train_loss: 234.02648735046387
batch: 2071
train_loss: 237.1578586101532
batch: 2072
train_loss: 240.32986783981323
batch: 2073
train_loss: 243.5790958404541
batch: 2074
train_loss: 246.6735074520111
batch: 2075
train_loss: 249.8823606967926
batch: 2076
train_loss: 253.1122226715088
batch: 2077
train_loss: 256.4341068267822
batch: 2078
train_loss: 259.769606590271
batch: 2079
train_loss: 263.0350561141968
batch: 2080
train_loss: 266.3809027671814
batch: 2081
train_loss: 269.55357575416565
batch: 2082
train_loss: 272.7279088497162
batch: 2083
train_loss: 275.9469618797302
batch: 2084
train_loss: 279.2446928024292
batch: 2085
train_loss: 282.45714116096497
batch: 2086
train_loss: 285.71335220336914
batch: 2087
train_loss: 288.91556787490845
batch: 2088
train_loss: 292.1216604709625
batch: 2089
train_loss: 295.38100719451904
batch: 2090
train_loss: 298.5875449180603
batch: 2091
train_loss: 301.8541805744171
batch: 2092
train_loss: 305.0928614139557
batch: 2093
train_loss: 308.34940457344055
batch: 2094
train_loss: 311.53567004203796
batch: 2095
train_loss: 314.7275528907776
batch: 2096
train_loss: 317.8551902770996
batch: 2097
train_loss: 321.1135764122009
batch: 2098
train_loss: 324.29258489608765
batch: 2099
train_loss: 327.541207075119
batch: 2100
train_loss: 330.8480176925659
batch: 2101
train_loss: 334.0290915966034
batch: 2102
train_loss: 337.2477035522461
batch: 2103
train_loss: 340.49732184410095
batch: 2104
train_loss: 343.73632884025574
batch: 2105
train_loss: 346.9421889781952
batch: 2106
train_loss: 350.06426906585693
batch: 2107
train_loss: 353.2897183895111
batch: 2108
train_loss: 356.3944630622864
batch: 2109
train_loss: 359.5639281272888
batch: 2110
train_loss: 362.68564772605896
batch: 2111
train_loss: 365.9320809841156
batch: 2112
train_loss: 369.1732590198517
batch: 2113
train_loss: 372.36383986473083
batch: 2114
train_loss: 375.5964090824127
batch: 2115
train_loss: 378.78146028518677
batch: 2116
train_loss: 382.0251615047455
batch: 2117
train_loss: 385.2544219493866
batch: 2118
train_loss: 388.4461793899536
batch: 2119
train_loss: 391.61607694625854
batch: 2120
train_loss: 394.9261133670807
batch: 2121
train_loss: 398.2660448551178
batch: 2122
train_loss: 401.5048427581787
batch: 2123
train_loss: 404.7563555240631
batch: 2124
train_loss: 408.0246911048889
batch: 2125
train_loss: 411.21831345558167
batch: 2126
train_loss: 414.3534691333771
batch: 2127
train_loss: 417.61147594451904
batch: 2128
train_loss: 420.8259496688843
batch: 2129
train_loss: 424.07888674736023
batch: 2130
train_loss: 427.4257061481476
batch: 2131
train_loss: 430.7338306903839
batch: 2132
train_loss: 434.0244073867798
batch: 2133
train_loss: 437.2646539211273
batch: 2134
train_loss: 440.5972754955292
batch: 2135
train_loss: 443.895263671875
batch: 2136
train_loss: 447.25889587402344
batch: 2137
train_loss: 450.5061173439026
batch: 2138
train_loss: 453.62274718284607
batch: 2139
train_loss: 456.89313077926636
batch: 2140
train_loss: 460.05072355270386
batch: 2141
train_loss: 463.3539502620697
batch: 2142
train_loss: 466.71609711647034
batch: 2143
train_loss: 469.982382774353
batch: 2144
train_loss: 473.30268836021423
batch: 2145
train_loss: 476.5879764556885
batch: 2146
train_loss: 479.7956805229187
batch: 2147
train_loss: 483.05571722984314
batch: 2148
train_loss: 486.3104193210602
batch: 2149
train_loss: 489.6018795967102
batch: 2150
train_loss: 492.8971257209778
batch: 2151
train_loss: 496.2439830303192
batch: 2152
train_loss: 499.4727132320404
batch: 2153
train_loss: 502.84725522994995
batch: 2154
train_loss: 506.1362946033478
batch: 2155
train_loss: 509.3592562675476
batch: 2156
train_loss: 512.5893657207489
batch: 2157
train_loss: 515.812272310257
batch: 2158
train_loss: 518.9840059280396
batch: 2159
train_loss: 522.2393772602081
batch: 2160
train_loss: 525.4925105571747
batch: 2161
train_loss: 528.7571477890015
batch: 2162
train_loss: 531.9965970516205
batch: 2163
train_loss: 535.3226635456085
batch: 2164
train_loss: 538.621367931366
batch: 2165
train_loss: 541.8497767448425
batch: 2166
train_loss: 545.1277344226837
batch: 2167
train_loss: 548.3829827308655
batch: 2168
train_loss: 551.6899576187134
batch: 2169
train_loss: 554.996374130249
batch: 2170
train_loss: 558.3191480636597
batch: 2171
train_loss: 561.5786077976227
batch: 2172
train_loss: 564.885507106781
batch: 2173
train_loss: 568.070068359375
batch: 2174
train_loss: 571.3613953590393
batch: 2175
train_loss: 574.6339221000671
batch: 2176
train_loss: 577.9174275398254
batch: 2177
train_loss: 581.2331027984619
batch: 2178
train_loss: 584.4245784282684
batch: 2179
train_loss: 587.606810092926
batch: 2180
train_loss: 590.92875623703
batch: 2181
train_loss: 594.1770894527435
batch: 2182
train_loss: 597.4451789855957
batch: 2183
train_loss: 600.6157431602478
batch: 2184
train_loss: 603.8762726783752
batch: 2185
train_loss: 607.1258225440979
batch: 2186
train_loss: 610.3113687038422
batch: 2187
train_loss: 613.5820984840393
batch: 2188
train_loss: 616.8451919555664
batch: 2189
train_loss: 620.0966415405273
batch: 2190
train_loss: 623.3668746948242
batch: 2191
train_loss: 626.6362075805664
batch: 2192
train_loss: 629.845005273819
batch: 2193
train_loss: 633.0816843509674
batch: 2194
train_loss: 636.4124383926392
batch: 2195
train_loss: 639.6512734889984
batch: 2196
train_loss: 642.8977646827698
batch: 2197
train_loss: 646.1249732971191
batch: 2198
train_loss: 649.3397333621979
batch: 2199
train_loss: 652.5740082263947
| epoch   0 step     2200 |   2200 batches | lr 0.000248 | ms/batch 152.17 | loss  3.26 | bpc   4.70733
batch: 2200
train_loss: 3.2980329990386963
batch: 2201
train_loss: 6.56728196144104
batch: 2202
train_loss: 9.938600063323975
batch: 2203
train_loss: 13.194585084915161
batch: 2204
train_loss: 16.37207818031311
batch: 2205
train_loss: 19.611018419265747
batch: 2206
train_loss: 22.774945735931396
batch: 2207
train_loss: 26.001805067062378
batch: 2208
train_loss: 29.287729501724243
batch: 2209
train_loss: 32.55664420127869
batch: 2210
train_loss: 35.81366491317749
batch: 2211
train_loss: 39.088683128356934
batch: 2212
train_loss: 42.25587511062622
batch: 2213
train_loss: 45.41961884498596
batch: 2214
train_loss: 48.59316110610962
batch: 2215
train_loss: 51.79050326347351
batch: 2216
train_loss: 54.94762659072876
batch: 2217
train_loss: 58.17823052406311
batch: 2218
train_loss: 61.452197313308716
batch: 2219
train_loss: 64.70929741859436
batch: 2220
train_loss: 67.95602297782898
batch: 2221
train_loss: 71.17430019378662
batch: 2222
train_loss: 74.38021779060364
batch: 2223
train_loss: 77.52605056762695
batch: 2224
train_loss: 80.75765252113342
batch: 2225
train_loss: 83.94152474403381
batch: 2226
train_loss: 87.22402667999268
batch: 2227
train_loss: 90.51172757148743
batch: 2228
train_loss: 93.72819781303406
batch: 2229
train_loss: 96.87976813316345
batch: 2230
train_loss: 100.19202041625977
batch: 2231
train_loss: 103.39906311035156
batch: 2232
train_loss: 106.5907096862793
batch: 2233
train_loss: 109.81517386436462
batch: 2234
train_loss: 113.01641297340393
batch: 2235
train_loss: 116.12649297714233
batch: 2236
train_loss: 119.3616361618042
batch: 2237
train_loss: 122.55767488479614
batch: 2238
train_loss: 125.79109001159668
batch: 2239
train_loss: 129.02449679374695
batch: 2240
train_loss: 132.20939207077026
batch: 2241
train_loss: 135.45304083824158
batch: 2242
train_loss: 138.78436994552612
batch: 2243
train_loss: 142.03421831130981
batch: 2244
train_loss: 145.3164370059967
batch: 2245
train_loss: 148.60514998435974
batch: 2246
train_loss: 151.8386549949646
batch: 2247
train_loss: 155.01499223709106
batch: 2248
train_loss: 158.34545850753784
batch: 2249
train_loss: 161.60159873962402
batch: 2250
train_loss: 164.85070753097534
batch: 2251
train_loss: 168.19503116607666
batch: 2252
train_loss: 171.47861218452454
batch: 2253
train_loss: 174.77107000350952
batch: 2254
train_loss: 178.0589349269867
batch: 2255
train_loss: 181.3622589111328
batch: 2256
train_loss: 184.61697793006897
batch: 2257
train_loss: 187.92068672180176
batch: 2258
train_loss: 191.25909328460693
batch: 2259
train_loss: 194.5342094898224
batch: 2260
train_loss: 197.8508095741272
batch: 2261
train_loss: 201.08578896522522
batch: 2262
train_loss: 204.33611941337585
batch: 2263
train_loss: 207.6311376094818
batch: 2264
train_loss: 210.88563132286072
batch: 2265
train_loss: 214.18895721435547
batch: 2266
train_loss: 217.4791440963745
batch: 2267
train_loss: 220.67921447753906
batch: 2268
train_loss: 223.99728107452393
batch: 2269
train_loss: 227.23685908317566
batch: 2270
train_loss: 230.46057224273682
batch: 2271
train_loss: 233.6672637462616
batch: 2272
train_loss: 236.88847637176514
batch: 2273
train_loss: 240.25135731697083
batch: 2274
train_loss: 243.67519783973694
batch: 2275
train_loss: 246.9345188140869
batch: 2276
train_loss: 250.16885709762573
batch: 2277
train_loss: 253.31809163093567
batch: 2278
train_loss: 256.6061544418335
batch: 2279
train_loss: 259.8802969455719
batch: 2280
train_loss: 263.0978238582611
batch: 2281
train_loss: 266.34715938568115
batch: 2282
train_loss: 269.5106337070465
batch: 2283
train_loss: 272.6697964668274
batch: 2284
train_loss: 275.93556785583496
batch: 2285
train_loss: 279.3153681755066
batch: 2286
train_loss: 282.57406973838806
batch: 2287
train_loss: 285.82137179374695
batch: 2288
train_loss: 288.96026587486267
batch: 2289
train_loss: 292.0925033092499
batch: 2290
train_loss: 295.2809898853302
batch: 2291
train_loss: 298.461731672287
batch: 2292
train_loss: 301.6371123790741
batch: 2293
train_loss: 304.8652410507202
batch: 2294
train_loss: 308.05117177963257
batch: 2295
train_loss: 311.2923665046692
batch: 2296
train_loss: 314.4786105155945
batch: 2297
train_loss: 317.65368151664734
batch: 2298
train_loss: 320.8538455963135
batch: 2299
train_loss: 324.0573139190674
batch: 2300
train_loss: 327.2860231399536
batch: 2301
train_loss: 330.4051103591919
batch: 2302
train_loss: 333.6011323928833
batch: 2303
train_loss: 336.76383090019226
batch: 2304
train_loss: 339.95866203308105
batch: 2305
train_loss: 343.1673352718353
batch: 2306
train_loss: 346.334997177124
batch: 2307
train_loss: 349.6249349117279
batch: 2308
train_loss: 352.8528754711151
batch: 2309
train_loss: 356.2019226551056
batch: 2310
train_loss: 359.4774558544159
batch: 2311
train_loss: 362.75710439682007
batch: 2312
train_loss: 365.9711754322052
batch: 2313
train_loss: 369.18787455558777
batch: 2314
train_loss: 372.3587782382965
batch: 2315
train_loss: 375.5286674499512
batch: 2316
train_loss: 378.73042607307434
batch: 2317
train_loss: 381.9710273742676
batch: 2318
train_loss: 385.2967164516449
batch: 2319
train_loss: 388.5806803703308
batch: 2320
train_loss: 391.73077178001404
batch: 2321
train_loss: 394.88835549354553
batch: 2322
train_loss: 398.1605610847473
batch: 2323
train_loss: 401.40475153923035
batch: 2324
train_loss: 404.6448984146118
batch: 2325
train_loss: 407.81142807006836
batch: 2326
train_loss: 411.0500464439392
batch: 2327
train_loss: 414.3270254135132
batch: 2328
train_loss: 417.57204413414
batch: 2329
train_loss: 420.8420057296753
batch: 2330
train_loss: 424.0860710144043
batch: 2331
train_loss: 427.2386405467987
batch: 2332
train_loss: 430.4819974899292
batch: 2333
train_loss: 433.8347339630127
batch: 2334
train_loss: 437.1448538303375
batch: 2335
train_loss: 440.37118887901306
batch: 2336
train_loss: 443.6714084148407
batch: 2337
train_loss: 446.90410017967224
batch: 2338
train_loss: 450.2323250770569
batch: 2339
train_loss: 453.42793130874634
batch: 2340
train_loss: 456.66535449028015
batch: 2341
train_loss: 459.925758600235
batch: 2342
train_loss: 463.1933445930481
batch: 2343
train_loss: 466.4916536808014
batch: 2344
train_loss: 469.76392459869385
batch: 2345
train_loss: 473.0214378833771
batch: 2346
train_loss: 476.38439178466797
batch: 2347
train_loss: 479.6617646217346
batch: 2348
train_loss: 482.9728538990021
batch: 2349
train_loss: 486.3172881603241
batch: 2350
train_loss: 489.6004846096039
batch: 2351
train_loss: 492.9934923648834
batch: 2352
train_loss: 496.3101999759674
batch: 2353
train_loss: 499.67300605773926
batch: 2354
train_loss: 502.812518119812
batch: 2355
train_loss: 506.1119487285614
batch: 2356
train_loss: 509.3517427444458
batch: 2357
train_loss: 512.6085784435272
batch: 2358
train_loss: 515.894650220871
batch: 2359
train_loss: 519.1336007118225
batch: 2360
train_loss: 522.3797006607056
batch: 2361
train_loss: 525.5279352664948
batch: 2362
train_loss: 528.8077867031097
batch: 2363
train_loss: 532.0803401470184
batch: 2364
train_loss: 535.3631315231323
batch: 2365
train_loss: 538.614164352417
batch: 2366
train_loss: 541.864618062973
batch: 2367
train_loss: 545.0421426296234
batch: 2368
train_loss: 548.2525496482849
batch: 2369
train_loss: 551.3518490791321
batch: 2370
train_loss: 554.625815153122
batch: 2371
train_loss: 557.8108086585999
batch: 2372
train_loss: 561.071480512619
batch: 2373
train_loss: 564.3808715343475
batch: 2374
train_loss: 567.584386587143
batch: 2375
train_loss: 570.9107837677002
batch: 2376
train_loss: 574.19668841362
batch: 2377
train_loss: 577.4688785076141
batch: 2378
train_loss: 580.7325222492218
batch: 2379
train_loss: 584.0841128826141
batch: 2380
train_loss: 587.3265330791473
batch: 2381
train_loss: 590.6435945034027
batch: 2382
train_loss: 593.9474945068359
batch: 2383
train_loss: 597.1741712093353
batch: 2384
train_loss: 600.4548132419586
batch: 2385
train_loss: 603.8123669624329
batch: 2386
train_loss: 607.0668315887451
batch: 2387
train_loss: 610.3268129825592
batch: 2388
train_loss: 613.6390352249146
batch: 2389
train_loss: 616.8875095844269
batch: 2390
train_loss: 620.160849571228
batch: 2391
train_loss: 623.4354321956635
batch: 2392
train_loss: 626.7541220188141
batch: 2393
train_loss: 630.0821218490601
batch: 2394
train_loss: 633.433114528656
batch: 2395
train_loss: 636.7138085365295
batch: 2396
train_loss: 639.9925453662872
batch: 2397
train_loss: 643.365650177002
batch: 2398
train_loss: 646.6862587928772
batch: 2399
train_loss: 650.0976357460022
| epoch   0 step     2400 |   2400 batches | lr 0.000248 | ms/batch 149.92 | loss  3.25 | bpc   4.68946
batch: 2400
train_loss: 3.417478084564209
batch: 2401
train_loss: 6.8235907554626465
batch: 2402
train_loss: 10.220337390899658
batch: 2403
train_loss: 13.676688194274902
batch: 2404
train_loss: 17.11687159538269
batch: 2405
train_loss: 20.632650136947632
batch: 2406
train_loss: 23.912864923477173
batch: 2407
train_loss: 27.335952281951904
batch: 2408
train_loss: 30.773952960968018
batch: 2409
train_loss: 34.2092924118042
batch: 2410
train_loss: 37.54038381576538
batch: 2411
train_loss: 40.86800694465637
batch: 2412
train_loss: 44.10671067237854
batch: 2413
train_loss: 47.33372187614441
batch: 2414
train_loss: 50.592631578445435
batch: 2415
train_loss: 53.89483642578125
batch: 2416
train_loss: 57.18066954612732
batch: 2417
train_loss: 60.383206367492676
batch: 2418
train_loss: 63.75165581703186
batch: 2419
train_loss: 67.08632779121399
batch: 2420
train_loss: 70.33904790878296
batch: 2421
train_loss: 73.64649939537048
batch: 2422
train_loss: 77.08260679244995
batch: 2423
train_loss: 80.43674039840698
batch: 2424
train_loss: 83.73980379104614
batch: 2425
train_loss: 87.05851626396179
batch: 2426
train_loss: 90.37245965003967
batch: 2427
train_loss: 93.78532290458679
batch: 2428
train_loss: 97.10810589790344
batch: 2429
train_loss: 100.43543839454651
batch: 2430
train_loss: 103.87716865539551
batch: 2431
train_loss: 107.21619462966919
batch: 2432
train_loss: 110.64510989189148
batch: 2433
train_loss: 113.96623587608337
batch: 2434
train_loss: 117.22047257423401
batch: 2435
train_loss: 120.50522518157959
batch: 2436
train_loss: 123.79987263679504
batch: 2437
train_loss: 127.06527924537659
batch: 2438
train_loss: 130.30023503303528
batch: 2439
train_loss: 133.49268436431885
batch: 2440
train_loss: 136.59843397140503
batch: 2441
train_loss: 139.80952310562134
batch: 2442
train_loss: 142.9974925518036
batch: 2443
train_loss: 146.1452088356018
batch: 2444
train_loss: 149.29444336891174
batch: 2445
train_loss: 152.60094213485718
batch: 2446
train_loss: 155.779611825943
batch: 2447
train_loss: 159.04782032966614
batch: 2448
train_loss: 162.20661616325378
batch: 2449
train_loss: 165.31495809555054
batch: 2450
train_loss: 168.49750447273254
batch: 2451
train_loss: 171.72210001945496
batch: 2452
train_loss: 174.87969279289246
batch: 2453
train_loss: 178.10921144485474
batch: 2454
train_loss: 181.36564326286316
batch: 2455
train_loss: 184.56298279762268
batch: 2456
train_loss: 187.66335368156433
batch: 2457
train_loss: 190.91263437271118
batch: 2458
train_loss: 194.12733101844788
batch: 2459
train_loss: 197.41623878479004
batch: 2460
train_loss: 200.6972918510437
batch: 2461
train_loss: 204.02394843101501
batch: 2462
train_loss: 207.3346507549286
batch: 2463
train_loss: 210.56013298034668
batch: 2464
train_loss: 213.7696180343628
batch: 2465
train_loss: 217.03205037117004
batch: 2466
train_loss: 220.32040357589722
batch: 2467
train_loss: 223.6333737373352
batch: 2468
train_loss: 226.97044205665588
batch: 2469
train_loss: 230.21858978271484
batch: 2470
train_loss: 233.4715874195099
batch: 2471
train_loss: 236.6291847229004
batch: 2472
train_loss: 239.8565788269043
batch: 2473
train_loss: 243.1299066543579
batch: 2474
train_loss: 246.46230244636536
batch: 2475
train_loss: 249.77218556404114
batch: 2476
train_loss: 253.05590677261353
batch: 2477
train_loss: 256.30734515190125
batch: 2478
train_loss: 259.5236141681671
batch: 2479
train_loss: 262.85371351242065
batch: 2480
train_loss: 266.16498279571533
batch: 2481
train_loss: 269.4737892150879
batch: 2482
train_loss: 272.7448434829712
batch: 2483
train_loss: 276.07009840011597
batch: 2484
train_loss: 279.4212017059326
batch: 2485
train_loss: 282.7702474594116
batch: 2486
train_loss: 286.04510617256165
batch: 2487
train_loss: 289.2580335140228
batch: 2488
train_loss: 292.38363218307495
batch: 2489
train_loss: 295.5541431903839
batch: 2490
train_loss: 298.89854550361633
batch: 2491
train_loss: 302.1687297821045
batch: 2492
train_loss: 305.4079701900482
batch: 2493
train_loss: 308.7359142303467
batch: 2494
train_loss: 311.8887324333191
batch: 2495
train_loss: 315.1426012516022
batch: 2496
train_loss: 318.34692192077637
batch: 2497
train_loss: 321.56757521629333
batch: 2498
train_loss: 324.7935290336609
batch: 2499
train_loss: 328.0533998012543
batch: 2500
train_loss: 331.24197793006897
batch: 2501
train_loss: 334.426150560379
batch: 2502
train_loss: 337.62819242477417
batch: 2503
train_loss: 340.76667737960815
batch: 2504
train_loss: 343.9684727191925
batch: 2505
train_loss: 347.23930644989014
batch: 2506
train_loss: 350.45562505722046
batch: 2507
train_loss: 353.7568197250366
batch: 2508
train_loss: 356.959988117218
batch: 2509
train_loss: 360.2873373031616
batch: 2510
train_loss: 363.58032059669495
batch: 2511
train_loss: 366.8997747898102
batch: 2512
train_loss: 370.18921399116516
batch: 2513
train_loss: 373.40934658050537
batch: 2514
train_loss: 376.65639185905457
batch: 2515
train_loss: 379.9142622947693
batch: 2516
train_loss: 383.16863560676575
batch: 2517
train_loss: 386.4198577404022
batch: 2518
train_loss: 389.80205631256104
batch: 2519
train_loss: 393.04209303855896
batch: 2520
train_loss: 396.2950065135956
batch: 2521
train_loss: 399.546103477478
batch: 2522
train_loss: 402.7776908874512
batch: 2523
train_loss: 406.0856719017029
batch: 2524
train_loss: 409.30846071243286
batch: 2525
train_loss: 412.4736661911011
batch: 2526
train_loss: 415.7120382785797
batch: 2527
train_loss: 419.00439643859863
batch: 2528
train_loss: 422.26233649253845
batch: 2529
train_loss: 425.48572516441345
batch: 2530
train_loss: 428.7799825668335
batch: 2531
train_loss: 432.11100459098816
batch: 2532
train_loss: 435.35503697395325
batch: 2533
train_loss: 438.60082244873047
batch: 2534
train_loss: 441.7670876979828
batch: 2535
train_loss: 445.06289076805115
batch: 2536
train_loss: 448.2621817588806
batch: 2537
train_loss: 451.55376291275024
batch: 2538
train_loss: 454.90561175346375
batch: 2539
train_loss: 458.1846263408661
batch: 2540
train_loss: 461.5037364959717
batch: 2541
train_loss: 464.7290766239166
batch: 2542
train_loss: 467.9082896709442
batch: 2543
train_loss: 471.17438411712646
batch: 2544
train_loss: 474.4486072063446
batch: 2545
train_loss: 477.6826424598694
batch: 2546
train_loss: 480.9082815647125
batch: 2547
train_loss: 484.14793705940247
batch: 2548
train_loss: 487.314514875412
batch: 2549
train_loss: 490.50649785995483
batch: 2550
train_loss: 493.69981622695923
batch: 2551
train_loss: 496.88829135894775
batch: 2552
train_loss: 500.0880205631256
batch: 2553
train_loss: 503.4010639190674
batch: 2554
train_loss: 506.66284441947937
batch: 2555
train_loss: 509.82008934020996
batch: 2556
train_loss: 513.1004030704498
batch: 2557
train_loss: 516.3061325550079
batch: 2558
train_loss: 519.5423109531403
batch: 2559
train_loss: 522.7820761203766
batch: 2560
train_loss: 525.9598455429077
batch: 2561
train_loss: 529.203592300415
batch: 2562
train_loss: 532.4333209991455
batch: 2563
train_loss: 535.7773509025574
batch: 2564
train_loss: 538.9698238372803
batch: 2565
train_loss: 542.2556047439575
batch: 2566
train_loss: 545.6057159900665
batch: 2567
train_loss: 548.875298500061
batch: 2568
train_loss: 552.1007294654846
batch: 2569
train_loss: 555.2974886894226
batch: 2570
train_loss: 558.5948750972748
batch: 2571
train_loss: 561.8506171703339
batch: 2572
train_loss: 564.9909865856171
batch: 2573
train_loss: 568.2035446166992
batch: 2574
train_loss: 571.4846963882446
batch: 2575
train_loss: 574.7862191200256
batch: 2576
train_loss: 578.0622050762177
batch: 2577
train_loss: 581.3764431476593
batch: 2578
train_loss: 584.5551633834839
batch: 2579
train_loss: 587.7962064743042
batch: 2580
train_loss: 591.1307158470154
batch: 2581
train_loss: 594.3669974803925
batch: 2582
train_loss: 597.6080696582794
batch: 2583
train_loss: 600.8714232444763
batch: 2584
train_loss: 604.1089057922363
batch: 2585
train_loss: 607.3451669216156
batch: 2586
train_loss: 610.5657317638397
batch: 2587
train_loss: 613.7719819545746
batch: 2588
train_loss: 617.0097551345825
batch: 2589
train_loss: 620.1517519950867
batch: 2590
train_loss: 623.473286151886
batch: 2591
train_loss: 626.6991720199585
batch: 2592
train_loss: 629.9434459209442
batch: 2593
train_loss: 633.2411508560181
batch: 2594
train_loss: 636.5177657604218
batch: 2595
train_loss: 639.7993009090424
batch: 2596
train_loss: 643.1359403133392
batch: 2597
train_loss: 646.3617744445801
batch: 2598
train_loss: 649.7279839515686
batch: 2599
train_loss: 652.8690431118011
| epoch   0 step     2600 |   2600 batches | lr 0.000247 | ms/batch 151.14 | loss  3.26 | bpc   4.70945
batch: 2600
train_loss: 3.2786269187927246
batch: 2601
train_loss: 6.539869546890259
batch: 2602
train_loss: 9.905468702316284
batch: 2603
train_loss: 13.257717609405518
batch: 2604
train_loss: 16.508575201034546
batch: 2605
train_loss: 19.768197536468506
batch: 2606
train_loss: 23.023905754089355
batch: 2607
train_loss: 26.27515459060669
batch: 2608
train_loss: 29.549933910369873
batch: 2609
train_loss: 32.730088233947754
batch: 2610
train_loss: 36.07651376724243
batch: 2611
train_loss: 39.33104634284973
batch: 2612
train_loss: 42.59180545806885
batch: 2613
train_loss: 45.8746063709259
batch: 2614
train_loss: 49.16658306121826
batch: 2615
train_loss: 52.382556676864624
batch: 2616
train_loss: 55.5396933555603
batch: 2617
train_loss: 58.77958798408508
batch: 2618
train_loss: 62.065826654434204
batch: 2619
train_loss: 65.27487993240356
batch: 2620
train_loss: 68.55775952339172
batch: 2621
train_loss: 71.84477663040161
batch: 2622
train_loss: 75.09686255455017
batch: 2623
train_loss: 78.35975337028503
batch: 2624
train_loss: 81.58532762527466
batch: 2625
train_loss: 84.84942817687988
batch: 2626
train_loss: 88.03438067436218
batch: 2627
train_loss: 91.29556012153625
batch: 2628
train_loss: 94.53433632850647
batch: 2629
train_loss: 97.65458631515503
batch: 2630
train_loss: 100.9114682674408
batch: 2631
train_loss: 104.15468573570251
batch: 2632
train_loss: 107.40310049057007
batch: 2633
train_loss: 110.6029884815216
batch: 2634
train_loss: 113.83390593528748
batch: 2635
train_loss: 117.08526754379272
batch: 2636
train_loss: 120.3917944431305
batch: 2637
train_loss: 123.64768719673157
batch: 2638
train_loss: 126.85154461860657
batch: 2639
train_loss: 130.05362462997437
batch: 2640
train_loss: 133.23304915428162
batch: 2641
train_loss: 136.45401573181152
batch: 2642
train_loss: 139.66111731529236
batch: 2643
train_loss: 142.97010922431946
batch: 2644
train_loss: 146.21164655685425
batch: 2645
train_loss: 149.47650241851807
batch: 2646
train_loss: 152.62588357925415
batch: 2647
train_loss: 155.89768981933594
batch: 2648
train_loss: 159.07928848266602
batch: 2649
train_loss: 162.22442388534546
batch: 2650
train_loss: 165.50325179100037
batch: 2651
train_loss: 168.77701878547668
batch: 2652
train_loss: 172.17958164215088
batch: 2653
train_loss: 175.4538128376007
batch: 2654
train_loss: 178.6548306941986
batch: 2655
train_loss: 181.93673849105835
batch: 2656
train_loss: 185.12638592720032
batch: 2657
train_loss: 188.36920046806335
batch: 2658
train_loss: 191.52369356155396
batch: 2659
train_loss: 194.7850432395935
batch: 2660
train_loss: 197.91552805900574
batch: 2661
train_loss: 201.14992499351501
batch: 2662
train_loss: 204.35010480880737
batch: 2663
train_loss: 207.55314445495605
batch: 2664
train_loss: 210.71476650238037
batch: 2665
train_loss: 213.94828534126282
batch: 2666
train_loss: 217.2182559967041
batch: 2667
train_loss: 220.48706698417664
batch: 2668
train_loss: 223.7224678993225
batch: 2669
train_loss: 226.91773533821106
batch: 2670
train_loss: 230.0166139602661
batch: 2671
train_loss: 233.19300413131714
batch: 2672
train_loss: 236.38147687911987
batch: 2673
train_loss: 239.57332563400269
batch: 2674
train_loss: 242.78921389579773
batch: 2675
train_loss: 245.9416036605835
batch: 2676
train_loss: 249.0617537498474
batch: 2677
train_loss: 252.2929723262787
batch: 2678
train_loss: 255.43296098709106
batch: 2679
train_loss: 258.64484095573425
batch: 2680
train_loss: 261.83647084236145
batch: 2681
train_loss: 265.09245252609253
batch: 2682
train_loss: 268.2135453224182
batch: 2683
train_loss: 271.2620620727539
batch: 2684
train_loss: 274.4121804237366
batch: 2685
train_loss: 277.6392333507538
batch: 2686
train_loss: 280.890438079834
batch: 2687
train_loss: 284.0902290344238
batch: 2688
train_loss: 287.38534784317017
batch: 2689
train_loss: 290.49110889434814
batch: 2690
train_loss: 293.6730053424835
batch: 2691
train_loss: 297.0409200191498
batch: 2692
train_loss: 300.11373567581177
batch: 2693
train_loss: 303.2215404510498
batch: 2694
train_loss: 306.32035183906555
batch: 2695
train_loss: 309.46102714538574
batch: 2696
train_loss: 312.62444496154785
batch: 2697
train_loss: 315.81293511390686
batch: 2698
train_loss: 319.0067825317383
batch: 2699
train_loss: 322.1810052394867
batch: 2700
train_loss: 325.2788407802582
batch: 2701
train_loss: 328.3676657676697
batch: 2702
train_loss: 331.5776162147522
batch: 2703
train_loss: 334.6604220867157
batch: 2704
train_loss: 337.7844316959381
batch: 2705
train_loss: 340.96956181526184
batch: 2706
train_loss: 344.06759548187256
batch: 2707
train_loss: 347.2478063106537
batch: 2708
train_loss: 350.4581837654114
batch: 2709
train_loss: 353.57166361808777
batch: 2710
train_loss: 356.6820824146271
batch: 2711
train_loss: 359.9032278060913
batch: 2712
train_loss: 362.961368560791
batch: 2713
train_loss: 366.0421679019928
batch: 2714
train_loss: 369.23388838768005
batch: 2715
train_loss: 372.43098640441895
batch: 2716
train_loss: 375.55227398872375
batch: 2717
train_loss: 378.677531003952
batch: 2718
train_loss: 381.8009412288666
batch: 2719
train_loss: 384.8973650932312
batch: 2720
train_loss: 388.10567259788513
batch: 2721
train_loss: 391.2546739578247
batch: 2722
train_loss: 394.32708764076233
batch: 2723
train_loss: 397.5006654262543
batch: 2724
train_loss: 400.66055488586426
batch: 2725
train_loss: 403.84273743629456
batch: 2726
train_loss: 407.03010153770447
batch: 2727
train_loss: 410.2419753074646
batch: 2728
train_loss: 413.41295528411865
batch: 2729
train_loss: 416.5607440471649
batch: 2730
train_loss: 419.7744164466858
batch: 2731
train_loss: 422.9910066127777
batch: 2732
train_loss: 426.1798806190491
batch: 2733
train_loss: 429.37892413139343
batch: 2734
train_loss: 432.45931816101074
batch: 2735
train_loss: 435.6248972415924
batch: 2736
train_loss: 438.89074206352234
batch: 2737
train_loss: 442.11023139953613
batch: 2738
train_loss: 445.2718210220337
batch: 2739
train_loss: 448.4457881450653
batch: 2740
train_loss: 451.5983953475952
batch: 2741
train_loss: 454.85561442375183
batch: 2742
train_loss: 457.9990270137787
batch: 2743
train_loss: 461.21660900115967
batch: 2744
train_loss: 464.4922208786011
batch: 2745
train_loss: 467.6652023792267
batch: 2746
train_loss: 470.8132002353668
batch: 2747
train_loss: 474.0053472518921
batch: 2748
train_loss: 477.1734290122986
batch: 2749
train_loss: 480.3395833969116
batch: 2750
train_loss: 483.5072886943817
batch: 2751
train_loss: 486.6854372024536
batch: 2752
train_loss: 489.9553773403168
batch: 2753
train_loss: 493.09829211235046
batch: 2754
train_loss: 496.14532947540283
batch: 2755
train_loss: 499.27540922164917
batch: 2756
train_loss: 502.43366861343384
batch: 2757
train_loss: 505.6742548942566
batch: 2758
train_loss: 508.81499457359314
batch: 2759
train_loss: 512.0330228805542
batch: 2760
train_loss: 515.1075012683868
batch: 2761
train_loss: 518.1475961208344
batch: 2762
train_loss: 521.2638742923737
batch: 2763
train_loss: 524.3743824958801
batch: 2764
train_loss: 527.5540595054626
batch: 2765
train_loss: 530.6529762744904
batch: 2766
train_loss: 533.7097680568695
batch: 2767
train_loss: 536.8050158023834
batch: 2768
train_loss: 539.928010225296
batch: 2769
train_loss: 543.1188023090363
batch: 2770
train_loss: 546.3902204036713
batch: 2771
train_loss: 549.5531759262085
batch: 2772
train_loss: 552.7569622993469
batch: 2773
train_loss: 555.9048275947571
batch: 2774
train_loss: 558.985759973526
batch: 2775
train_loss: 562.1930346488953
batch: 2776
train_loss: 565.2979135513306
batch: 2777
train_loss: 568.4536113739014
batch: 2778
train_loss: 571.5826907157898
batch: 2779
train_loss: 574.6972963809967
batch: 2780
train_loss: 577.760452747345
batch: 2781
train_loss: 580.8191838264465
batch: 2782
train_loss: 583.8999667167664
batch: 2783
train_loss: 587.0310955047607
batch: 2784
train_loss: 590.1409826278687
batch: 2785
train_loss: 593.2062177658081
batch: 2786
train_loss: 596.284567117691
batch: 2787
train_loss: 599.470002412796
batch: 2788
train_loss: 602.540623664856
batch: 2789
train_loss: 605.6756556034088
batch: 2790
train_loss: 608.8506240844727
batch: 2791
train_loss: 612.0268456935883
batch: 2792
train_loss: 615.285852432251
batch: 2793
train_loss: 618.3964822292328
batch: 2794
train_loss: 621.5827906131744
batch: 2795
train_loss: 624.7832646369934
batch: 2796
train_loss: 627.8964202404022
batch: 2797
train_loss: 631.1843993663788
batch: 2798
train_loss: 634.3641285896301
batch: 2799
train_loss: 637.6675090789795
| epoch   0 step     2800 |   2800 batches | lr 0.000247 | ms/batch 150.47 | loss  3.19 | bpc   4.59980
batch: 2800
train_loss: 3.2288782596588135
batch: 2801
train_loss: 6.463275909423828
batch: 2802
train_loss: 9.604894876480103
batch: 2803
train_loss: 12.710097551345825
batch: 2804
train_loss: 15.809101343154907
batch: 2805
train_loss: 18.968063831329346
batch: 2806
train_loss: 22.05318570137024
batch: 2807
train_loss: 25.214935541152954
batch: 2808
train_loss: 28.416412830352783
batch: 2809
train_loss: 31.584012985229492
batch: 2810
train_loss: 34.710304498672485
batch: 2811
train_loss: 37.91199994087219
batch: 2812
train_loss: 41.099430561065674
batch: 2813
train_loss: 44.27272367477417
batch: 2814
train_loss: 47.46275305747986
batch: 2815
train_loss: 50.68050193786621
batch: 2816
train_loss: 53.86474132537842
batch: 2817
train_loss: 57.01700043678284
batch: 2818
train_loss: 60.167041063308716
batch: 2819
train_loss: 63.33427047729492
batch: 2820
train_loss: 66.53594875335693
batch: 2821
train_loss: 69.79799771308899
batch: 2822
train_loss: 72.94326901435852
batch: 2823
train_loss: 76.1083972454071
batch: 2824
train_loss: 79.17204928398132
batch: 2825
train_loss: 82.25442624092102
batch: 2826
train_loss: 85.4383556842804
batch: 2827
train_loss: 88.59159207344055
batch: 2828
train_loss: 91.76465177536011
batch: 2829
train_loss: 94.95799589157104
batch: 2830
train_loss: 98.13811230659485
batch: 2831
train_loss: 101.3153223991394
batch: 2832
train_loss: 104.45436954498291
batch: 2833
train_loss: 107.68558597564697
batch: 2834
train_loss: 110.86177182197571
batch: 2835
train_loss: 113.9692599773407
batch: 2836
train_loss: 117.2429850101471
batch: 2837
train_loss: 120.41708946228027
batch: 2838
train_loss: 123.58158707618713
batch: 2839
train_loss: 126.6789801120758
batch: 2840
train_loss: 129.88396978378296
batch: 2841
train_loss: 133.09180998802185
batch: 2842
train_loss: 136.34085154533386
batch: 2843
train_loss: 139.61443042755127
batch: 2844
train_loss: 142.73987436294556
batch: 2845
train_loss: 145.924307346344
batch: 2846
train_loss: 149.137686252594
batch: 2847
train_loss: 152.234069108963
batch: 2848
train_loss: 155.42641067504883
batch: 2849
train_loss: 158.54529190063477
batch: 2850
train_loss: 161.68348240852356
batch: 2851
train_loss: 164.8377549648285
batch: 2852
train_loss: 168.01192951202393
batch: 2853
train_loss: 171.17497944831848
batch: 2854
train_loss: 174.27485871315002
batch: 2855
train_loss: 177.45132899284363
batch: 2856
train_loss: 180.6007308959961
batch: 2857
train_loss: 183.7581422328949
batch: 2858
train_loss: 186.9751160144806
batch: 2859
train_loss: 190.0290811061859
batch: 2860
train_loss: 193.1728584766388
batch: 2861
train_loss: 196.2852485179901
batch: 2862
train_loss: 199.5316128730774
batch: 2863
train_loss: 202.71408987045288
batch: 2864
train_loss: 205.81121039390564
batch: 2865
train_loss: 208.94129943847656
batch: 2866
train_loss: 212.1587028503418
batch: 2867
train_loss: 215.39117980003357
batch: 2868
train_loss: 218.65282773971558
batch: 2869
train_loss: 221.8344531059265
batch: 2870
train_loss: 225.13894248008728
batch: 2871
train_loss: 228.36162066459656
batch: 2872
train_loss: 231.56178855895996
batch: 2873
train_loss: 234.89755749702454
batch: 2874
train_loss: 238.11494779586792
batch: 2875
train_loss: 241.31664156913757
batch: 2876
train_loss: 244.62967085838318
batch: 2877
train_loss: 247.80736637115479
batch: 2878
train_loss: 250.99297833442688
batch: 2879
train_loss: 254.17092037200928
batch: 2880
train_loss: 257.31806087493896
batch: 2881
train_loss: 260.3980972766876
batch: 2882
train_loss: 263.622239112854
batch: 2883
train_loss: 266.68762016296387
batch: 2884
train_loss: 269.8118951320648
batch: 2885
train_loss: 272.98663783073425
batch: 2886
train_loss: 276.1304290294647
batch: 2887
train_loss: 279.234601020813
batch: 2888
train_loss: 282.42044019699097
batch: 2889
train_loss: 285.639084815979
batch: 2890
train_loss: 288.84044337272644
batch: 2891
train_loss: 292.13030338287354
batch: 2892
train_loss: 295.4026336669922
batch: 2893
train_loss: 298.67775440216064
batch: 2894
train_loss: 301.76915431022644
batch: 2895
train_loss: 304.9999177455902
batch: 2896
train_loss: 308.27514839172363
batch: 2897
train_loss: 311.52593421936035
batch: 2898
train_loss: 314.6400554180145
batch: 2899
train_loss: 317.9057881832123
batch: 2900
train_loss: 320.96427750587463
batch: 2901
train_loss: 324.00150966644287
batch: 2902
train_loss: 327.11671900749207
batch: 2903
train_loss: 330.2516930103302
batch: 2904
train_loss: 333.34487223625183
batch: 2905
train_loss: 336.529278755188
batch: 2906
train_loss: 339.6339542865753
batch: 2907
train_loss: 342.8829348087311
batch: 2908
train_loss: 346.00171971321106
batch: 2909
train_loss: 349.12222385406494
batch: 2910
train_loss: 352.1785361766815
batch: 2911
train_loss: 355.2198143005371
batch: 2912
train_loss: 358.3274381160736
batch: 2913
train_loss: 361.46028423309326
batch: 2914
train_loss: 364.6624357700348
batch: 2915
train_loss: 367.70599579811096
batch: 2916
train_loss: 370.80779242515564
batch: 2917
train_loss: 373.8594572544098
batch: 2918
train_loss: 376.95414638519287
batch: 2919
train_loss: 380.02264285087585
batch: 2920
train_loss: 383.0427460670471
batch: 2921
train_loss: 386.1260802745819
batch: 2922
train_loss: 389.1519629955292
batch: 2923
train_loss: 392.25532126426697
batch: 2924
train_loss: 395.3096692562103
batch: 2925
train_loss: 398.40969133377075
batch: 2926
train_loss: 401.4899218082428
batch: 2927
train_loss: 404.5886073112488
batch: 2928
train_loss: 407.62492847442627
batch: 2929
train_loss: 410.78028750419617
batch: 2930
train_loss: 413.83217120170593
batch: 2931
train_loss: 416.8682019710541
batch: 2932
train_loss: 419.93668484687805
batch: 2933
train_loss: 423.027113199234
batch: 2934
train_loss: 426.1596703529358
batch: 2935
train_loss: 429.18592071533203
batch: 2936
train_loss: 432.24065709114075
batch: 2937
train_loss: 435.2940855026245
batch: 2938
train_loss: 438.41233825683594
batch: 2939
train_loss: 441.47263503074646
batch: 2940
train_loss: 444.5401804447174
batch: 2941
train_loss: 447.5719017982483
batch: 2942
train_loss: 450.60154819488525
batch: 2943
train_loss: 453.63490533828735
batch: 2944
train_loss: 456.70387530326843
batch: 2945
train_loss: 459.76420068740845
batch: 2946
train_loss: 462.82770800590515
batch: 2947
train_loss: 465.9395077228546
batch: 2948
train_loss: 469.09486198425293
batch: 2949
train_loss: 472.1735932826996
batch: 2950
train_loss: 475.28949761390686
batch: 2951
train_loss: 478.33319568634033
batch: 2952
train_loss: 481.4799304008484
batch: 2953
train_loss: 484.6188623905182
batch: 2954
train_loss: 487.7193148136139
batch: 2955
train_loss: 490.8307201862335
batch: 2956
train_loss: 494.02570819854736
batch: 2957
train_loss: 497.12671542167664
batch: 2958
train_loss: 500.1910526752472
batch: 2959
train_loss: 503.29250741004944
batch: 2960
train_loss: 506.42774629592896
batch: 2961
train_loss: 509.4611933231354
batch: 2962
train_loss: 512.5590319633484
batch: 2963
train_loss: 515.6349170207977
batch: 2964
train_loss: 518.7328004837036
batch: 2965
train_loss: 521.9212982654572
batch: 2966
train_loss: 525.0328636169434
batch: 2967
train_loss: 528.0571908950806
batch: 2968
train_loss: 531.1195833683014
batch: 2969
train_loss: 534.1633448600769
batch: 2970
train_loss: 537.3562378883362
batch: 2971
train_loss: 540.4052276611328
batch: 2972
train_loss: 543.472119808197
batch: 2973
train_loss: 546.5166516304016
batch: 2974
train_loss: 549.6243059635162
batch: 2975
train_loss: 552.7684440612793
batch: 2976
train_loss: 555.8744995594025
batch: 2977
train_loss: 558.9394791126251
batch: 2978
train_loss: 561.9824705123901
batch: 2979
train_loss: 565.0853595733643
batch: 2980
train_loss: 568.1746423244476
batch: 2981
train_loss: 571.3097369670868
batch: 2982
train_loss: 574.5220246315002
batch: 2983
train_loss: 577.6990504264832
batch: 2984
train_loss: 580.7575304508209
batch: 2985
train_loss: 583.8537800312042
batch: 2986
train_loss: 586.9752621650696
batch: 2987
train_loss: 590.1050112247467
batch: 2988
train_loss: 593.3968758583069
batch: 2989
train_loss: 596.4949822425842
batch: 2990
train_loss: 599.5925798416138
batch: 2991
train_loss: 602.6736063957214
batch: 2992
train_loss: 605.7534348964691
batch: 2993
train_loss: 608.836897611618
batch: 2994
train_loss: 611.8551669120789
batch: 2995
train_loss: 614.9276995658875
batch: 2996
train_loss: 617.9634277820587
batch: 2997
train_loss: 620.9900286197662
batch: 2998
train_loss: 624.092200756073
batch: 2999
train_loss: 627.2007286548615
| epoch   0 step     3000 |   3000 batches | lr 0.000247 | ms/batch 149.60 | loss  3.14 | bpc   4.52430
batch: 3000
train_loss: 3.160568952560425
batch: 3001
train_loss: 6.246851205825806
batch: 3002
train_loss: 9.444996356964111
batch: 3003
train_loss: 12.658588171005249
batch: 3004
train_loss: 15.806193351745605
batch: 3005
train_loss: 18.838890075683594
batch: 3006
train_loss: 21.924594163894653
batch: 3007
train_loss: 24.970351696014404
batch: 3008
train_loss: 28.02192187309265
batch: 3009
train_loss: 31.15050482749939
batch: 3010
train_loss: 34.27824902534485
batch: 3011
train_loss: 37.414170265197754
batch: 3012
train_loss: 40.62235426902771
batch: 3013
train_loss: 43.829283475875854
batch: 3014
train_loss: 46.92271661758423
batch: 3015
train_loss: 50.13530230522156
batch: 3016
train_loss: 53.26503539085388
batch: 3017
train_loss: 56.4918646812439
batch: 3018
train_loss: 59.81026887893677
batch: 3019
train_loss: 63.01644945144653
batch: 3020
train_loss: 66.22779130935669
batch: 3021
train_loss: 69.43539190292358
batch: 3022
train_loss: 72.5806667804718
batch: 3023
train_loss: 75.85016584396362
batch: 3024
train_loss: 79.0041925907135
batch: 3025
train_loss: 82.16245317459106
batch: 3026
train_loss: 85.26594376564026
batch: 3027
train_loss: 88.35086441040039
batch: 3028
train_loss: 91.55093026161194
batch: 3029
train_loss: 94.64736199378967
batch: 3030
train_loss: 97.77886772155762
batch: 3031
train_loss: 100.84023237228394
batch: 3032
train_loss: 103.99330163002014
batch: 3033
train_loss: 107.13696908950806
batch: 3034
train_loss: 110.23065733909607
batch: 3035
train_loss: 113.40544891357422
batch: 3036
train_loss: 116.52263498306274
batch: 3037
train_loss: 119.69084811210632
batch: 3038
train_loss: 122.84721040725708
batch: 3039
train_loss: 125.95894742012024
batch: 3040
train_loss: 129.0640070438385
batch: 3041
train_loss: 132.10278940200806
batch: 3042
train_loss: 135.2535445690155
batch: 3043
train_loss: 138.3086655139923
batch: 3044
train_loss: 141.39553213119507
batch: 3045
train_loss: 144.42854809761047
batch: 3046
train_loss: 147.47463941574097
batch: 3047
train_loss: 150.61245012283325
batch: 3048
train_loss: 153.72803926467896
batch: 3049
train_loss: 156.84851479530334
batch: 3050
train_loss: 160.00816750526428
batch: 3051
train_loss: 163.2118113040924
batch: 3052
train_loss: 166.38426327705383
batch: 3053
train_loss: 169.5142002105713
batch: 3054
train_loss: 172.60394549369812
batch: 3055
train_loss: 175.77863550186157
batch: 3056
train_loss: 178.9343659877777
batch: 3057
train_loss: 182.10585021972656
batch: 3058
train_loss: 185.29072666168213
batch: 3059
train_loss: 188.45369744300842
batch: 3060
train_loss: 191.54123854637146
batch: 3061
train_loss: 194.65125131607056
batch: 3062
train_loss: 197.78627467155457
batch: 3063
train_loss: 200.95336747169495
batch: 3064
train_loss: 204.02205562591553
batch: 3065
train_loss: 207.19748616218567
batch: 3066
train_loss: 210.3120515346527
batch: 3067
train_loss: 213.48907351493835
batch: 3068
train_loss: 216.62482857704163
batch: 3069
train_loss: 219.77655053138733
batch: 3070
train_loss: 222.91202855110168
batch: 3071
train_loss: 226.08115911483765
batch: 3072
train_loss: 229.22753286361694
batch: 3073
train_loss: 232.37473583221436
batch: 3074
train_loss: 235.57202076911926
batch: 3075
train_loss: 238.59637928009033
batch: 3076
train_loss: 241.63836669921875
batch: 3077
train_loss: 244.75506496429443
batch: 3078
train_loss: 247.93595623970032
batch: 3079
train_loss: 251.129465341568
batch: 3080
train_loss: 254.22096943855286
batch: 3081
train_loss: 257.38251399993896
batch: 3082
train_loss: 260.58629965782166
batch: 3083
train_loss: 263.793829202652
batch: 3084
train_loss: 266.96834230422974
batch: 3085
train_loss: 270.10842275619507
batch: 3086
train_loss: 273.2702000141144
batch: 3087
train_loss: 276.36797165870667
batch: 3088
train_loss: 279.40494656562805
batch: 3089
train_loss: 282.63979148864746
batch: 3090
train_loss: 285.7578501701355
batch: 3091
train_loss: 288.9365336894989
batch: 3092
train_loss: 292.1502604484558
batch: 3093
train_loss: 295.2853190898895
batch: 3094
train_loss: 298.37913250923157
batch: 3095
train_loss: 301.4855206012726
batch: 3096
train_loss: 304.6661648750305
batch: 3097
train_loss: 307.86777567863464
batch: 3098
train_loss: 310.9929723739624
batch: 3099
train_loss: 314.21334052085876
batch: 3100
train_loss: 317.41245794296265
batch: 3101
train_loss: 320.72448563575745
batch: 3102
train_loss: 323.7896423339844
batch: 3103
train_loss: 326.91430830955505
batch: 3104
train_loss: 330.20732140541077
batch: 3105
train_loss: 333.4102854728699
batch: 3106
train_loss: 336.70845460891724
batch: 3107
train_loss: 339.939001083374
batch: 3108
train_loss: 343.1765077114105
batch: 3109
train_loss: 346.55713844299316
batch: 3110
train_loss: 349.8207380771637
batch: 3111
train_loss: 353.2281081676483
batch: 3112
train_loss: 356.5382993221283
batch: 3113
train_loss: 359.6181745529175
batch: 3114
train_loss: 362.7333471775055
batch: 3115
train_loss: 365.86151027679443
batch: 3116
train_loss: 369.0996005535126
batch: 3117
train_loss: 372.16561555862427
batch: 3118
train_loss: 375.3402030467987
batch: 3119
train_loss: 378.4942297935486
batch: 3120
train_loss: 381.6905789375305
batch: 3121
train_loss: 384.9609754085541
batch: 3122
train_loss: 388.11506390571594
batch: 3123
train_loss: 391.28057527542114
batch: 3124
train_loss: 394.4901645183563
batch: 3125
train_loss: 397.5222384929657
batch: 3126
train_loss: 400.7368588447571
batch: 3127
train_loss: 403.92172288894653
batch: 3128
train_loss: 407.0515468120575
batch: 3129
train_loss: 410.17738008499146
batch: 3130
train_loss: 413.30904603004456
batch: 3131
train_loss: 416.44238805770874
batch: 3132
train_loss: 419.5754837989807
batch: 3133
train_loss: 422.75272512435913
batch: 3134
train_loss: 425.9401376247406
batch: 3135
train_loss: 429.10500955581665
batch: 3136
train_loss: 432.25775384902954
batch: 3137
train_loss: 435.5041139125824
batch: 3138
train_loss: 438.7692506313324
batch: 3139
train_loss: 442.0177352428436
batch: 3140
train_loss: 445.1895558834076
batch: 3141
train_loss: 448.43765687942505
batch: 3142
train_loss: 451.6299216747284
batch: 3143
train_loss: 454.81139516830444
batch: 3144
train_loss: 458.02703976631165
batch: 3145
train_loss: 461.0671880245209
batch: 3146
train_loss: 464.0728840827942
batch: 3147
train_loss: 467.1682698726654
batch: 3148
train_loss: 470.2867856025696
batch: 3149
train_loss: 473.4147448539734
batch: 3150
train_loss: 476.48598074913025
batch: 3151
train_loss: 479.68150329589844
batch: 3152
train_loss: 482.76394295692444
batch: 3153
train_loss: 485.8968095779419
batch: 3154
train_loss: 489.0500361919403
batch: 3155
train_loss: 492.21698546409607
batch: 3156
train_loss: 495.3736138343811
batch: 3157
train_loss: 498.45731592178345
batch: 3158
train_loss: 501.65396070480347
batch: 3159
train_loss: 504.78577995300293
batch: 3160
train_loss: 507.9261705875397
batch: 3161
train_loss: 511.03705525398254
batch: 3162
train_loss: 514.2013649940491
batch: 3163
train_loss: 517.3607218265533
batch: 3164
train_loss: 520.5288484096527
batch: 3165
train_loss: 523.6529154777527
batch: 3166
train_loss: 526.7870106697083
batch: 3167
train_loss: 529.8618655204773
batch: 3168
train_loss: 533.0768001079559
batch: 3169
train_loss: 536.1892969608307
batch: 3170
train_loss: 539.3409931659698
batch: 3171
train_loss: 542.4488127231598
batch: 3172
train_loss: 545.4641072750092
batch: 3173
train_loss: 548.6391911506653
batch: 3174
train_loss: 551.7448942661285
batch: 3175
train_loss: 554.9832615852356
batch: 3176
train_loss: 558.1300587654114
batch: 3177
train_loss: 561.3670880794525
batch: 3178
train_loss: 564.4744787216187
batch: 3179
train_loss: 567.5720427036285
batch: 3180
train_loss: 570.7510914802551
batch: 3181
train_loss: 573.9101252555847
batch: 3182
train_loss: 577.0781662464142
batch: 3183
train_loss: 580.2841844558716
batch: 3184
train_loss: 583.316746711731
batch: 3185
train_loss: 586.3755235671997
batch: 3186
train_loss: 589.464154958725
batch: 3187
train_loss: 592.545089006424
batch: 3188
train_loss: 595.6910047531128
batch: 3189
train_loss: 598.8527410030365
batch: 3190
train_loss: 602.0388698577881
batch: 3191
train_loss: 605.07506275177
batch: 3192
train_loss: 608.1404459476471
batch: 3193
train_loss: 611.2293391227722
batch: 3194
train_loss: 614.3428766727448
batch: 3195
train_loss: 617.4705989360809
batch: 3196
train_loss: 620.693943977356
batch: 3197
train_loss: 623.8756792545319
batch: 3198
train_loss: 626.9227039813995
batch: 3199
train_loss: 629.9557662010193
| epoch   0 step     3200 |   3200 batches | lr 0.000246 | ms/batch 150.55 | loss  3.15 | bpc   4.54417
batch: 3200
train_loss: 3.000018358230591
batch: 3201
train_loss: 6.1667609214782715
batch: 3202
train_loss: 9.332226514816284
batch: 3203
train_loss: 12.388573408126831
batch: 3204
train_loss: 15.493541479110718
batch: 3205
train_loss: 18.57127809524536
batch: 3206
train_loss: 21.6915340423584
batch: 3207
train_loss: 24.825804471969604
batch: 3208
train_loss: 27.926876544952393
batch: 3209
train_loss: 31.02826452255249
batch: 3210
train_loss: 34.16181230545044
batch: 3211
train_loss: 37.34142804145813
batch: 3212
train_loss: 40.48903489112854
batch: 3213
train_loss: 43.50782132148743
batch: 3214
train_loss: 46.631683588027954
batch: 3215
train_loss: 49.771634101867676
batch: 3216
train_loss: 52.82258915901184
batch: 3217
train_loss: 56.01343846321106
batch: 3218
train_loss: 59.12605595588684
batch: 3219
train_loss: 62.23588037490845
batch: 3220
train_loss: 65.31513595581055
batch: 3221
train_loss: 68.40507817268372
batch: 3222
train_loss: 71.6078519821167
batch: 3223
train_loss: 74.73763632774353
batch: 3224
train_loss: 77.84197449684143
batch: 3225
train_loss: 81.01514673233032
batch: 3226
train_loss: 84.2817633152008
batch: 3227
train_loss: 87.56981348991394
batch: 3228
train_loss: 90.71813488006592
batch: 3229
train_loss: 93.97103691101074
batch: 3230
train_loss: 97.13882184028625
batch: 3231
train_loss: 100.2716155052185
batch: 3232
train_loss: 103.35320210456848
batch: 3233
train_loss: 106.48652720451355
batch: 3234
train_loss: 109.74008560180664
batch: 3235
train_loss: 112.85939192771912
batch: 3236
train_loss: 115.94255208969116
batch: 3237
train_loss: 119.19881296157837
batch: 3238
train_loss: 122.29745864868164
batch: 3239
train_loss: 125.46824407577515
batch: 3240
train_loss: 128.7567093372345
batch: 3241
train_loss: 131.8106529712677
batch: 3242
train_loss: 134.95237946510315
batch: 3243
train_loss: 138.2347068786621
batch: 3244
train_loss: 141.39668488502502
batch: 3245
train_loss: 144.53504419326782
batch: 3246
train_loss: 147.65733647346497
batch: 3247
train_loss: 150.84135055541992
batch: 3248
train_loss: 154.04937529563904
batch: 3249
train_loss: 157.24181509017944
batch: 3250
train_loss: 160.42071390151978
batch: 3251
train_loss: 163.5843379497528
batch: 3252
train_loss: 166.72426962852478
batch: 3253
train_loss: 169.8106780052185
batch: 3254
train_loss: 173.06058359146118
batch: 3255
train_loss: 176.2231330871582
batch: 3256
train_loss: 179.4264099597931
batch: 3257
train_loss: 182.68112707138062
batch: 3258
train_loss: 185.94408559799194
batch: 3259
train_loss: 189.17196321487427
batch: 3260
train_loss: 192.41800904273987
batch: 3261
train_loss: 195.66803097724915
batch: 3262
train_loss: 198.86932015419006
batch: 3263
train_loss: 202.14282178878784
batch: 3264
train_loss: 205.41705632209778
batch: 3265
train_loss: 208.65675592422485
batch: 3266
train_loss: 211.93222975730896
batch: 3267
train_loss: 215.1580672264099
batch: 3268
train_loss: 218.42144560813904
batch: 3269
train_loss: 221.729234457016
batch: 3270
train_loss: 225.04040908813477
batch: 3271
train_loss: 228.32374334335327
batch: 3272
train_loss: 231.5373899936676
batch: 3273
train_loss: 234.7868721485138
batch: 3274
train_loss: 238.10875487327576
batch: 3275
train_loss: 241.39365315437317
batch: 3276
train_loss: 244.68978190422058
batch: 3277
train_loss: 248.0173032283783
batch: 3278
train_loss: 251.19093823432922
batch: 3279
train_loss: 254.41824221611023
batch: 3280
train_loss: 257.61939668655396
batch: 3281
train_loss: 260.85581278800964
batch: 3282
train_loss: 264.12764024734497
batch: 3283
train_loss: 267.3711133003235
batch: 3284
train_loss: 270.659729719162
batch: 3285
train_loss: 273.82262206077576
batch: 3286
train_loss: 276.9971115589142
batch: 3287
train_loss: 280.1343414783478
batch: 3288
train_loss: 283.4232358932495
batch: 3289
train_loss: 286.6197464466095
batch: 3290
train_loss: 289.7987825870514
batch: 3291
train_loss: 293.0230793952942
batch: 3292
train_loss: 296.2743480205536
batch: 3293
train_loss: 299.39959812164307
batch: 3294
train_loss: 302.54241251945496
batch: 3295
train_loss: 305.6375710964203
batch: 3296
train_loss: 308.74706649780273
batch: 3297
train_loss: 311.85401916503906
batch: 3298
train_loss: 314.920227766037
batch: 3299
train_loss: 318.1039457321167
batch: 3300
train_loss: 321.2810592651367
batch: 3301
train_loss: 324.486456155777
batch: 3302
train_loss: 327.60439443588257
batch: 3303
train_loss: 330.6748631000519
batch: 3304
train_loss: 333.7443768978119
batch: 3305
train_loss: 336.8682210445404
batch: 3306
train_loss: 340.12415289878845
batch: 3307
train_loss: 343.27178406715393
batch: 3308
train_loss: 346.32338523864746
batch: 3309
train_loss: 349.4367868900299
batch: 3310
train_loss: 352.52809834480286
batch: 3311
train_loss: 355.6528272628784
batch: 3312
train_loss: 358.7423605918884
batch: 3313
train_loss: 361.96098017692566
batch: 3314
train_loss: 365.0639138221741
batch: 3315
train_loss: 368.2382049560547
batch: 3316
train_loss: 371.3076226711273
batch: 3317
train_loss: 374.38346242904663
batch: 3318
train_loss: 377.5575032234192
batch: 3319
train_loss: 380.6978838443756
batch: 3320
train_loss: 383.8828446865082
batch: 3321
train_loss: 386.99032831192017
batch: 3322
train_loss: 390.100040435791
batch: 3323
train_loss: 393.22563004493713
batch: 3324
train_loss: 396.37286138534546
batch: 3325
train_loss: 399.4758355617523
batch: 3326
train_loss: 402.6150155067444
batch: 3327
train_loss: 405.77152848243713
batch: 3328
train_loss: 408.9155082702637
batch: 3329
train_loss: 411.94106674194336
batch: 3330
train_loss: 415.0281562805176
batch: 3331
train_loss: 418.145387172699
batch: 3332
train_loss: 421.20278239250183
batch: 3333
train_loss: 424.281977891922
batch: 3334
train_loss: 427.33646631240845
batch: 3335
train_loss: 430.4722740650177
batch: 3336
train_loss: 433.6017916202545
batch: 3337
train_loss: 436.6641139984131
batch: 3338
train_loss: 439.73382210731506
batch: 3339
train_loss: 442.82790303230286
batch: 3340
train_loss: 445.9537193775177
batch: 3341
train_loss: 449.0281503200531
batch: 3342
train_loss: 452.1461272239685
batch: 3343
train_loss: 455.24724650382996
batch: 3344
train_loss: 458.3285412788391
batch: 3345
train_loss: 461.4133315086365
batch: 3346
train_loss: 464.49026107788086
batch: 3347
train_loss: 467.65109395980835
batch: 3348
train_loss: 470.7641580104828
batch: 3349
train_loss: 473.8469035625458
batch: 3350
train_loss: 476.98421335220337
batch: 3351
train_loss: 480.1102569103241
batch: 3352
train_loss: 483.2852392196655
batch: 3353
train_loss: 486.4517254829407
batch: 3354
train_loss: 489.5290997028351
batch: 3355
train_loss: 492.6193251609802
batch: 3356
train_loss: 495.70141673088074
batch: 3357
train_loss: 498.7960457801819
batch: 3358
train_loss: 501.9431335926056
batch: 3359
train_loss: 505.1216514110565
batch: 3360
train_loss: 508.22779631614685
batch: 3361
train_loss: 511.39530873298645
batch: 3362
train_loss: 514.5676302909851
batch: 3363
train_loss: 517.5562646389008
batch: 3364
train_loss: 520.6581218242645
batch: 3365
train_loss: 523.6847653388977
batch: 3366
train_loss: 526.8694911003113
batch: 3367
train_loss: 530.0719094276428
batch: 3368
train_loss: 533.1599841117859
batch: 3369
train_loss: 536.3150322437286
batch: 3370
train_loss: 539.4053161144257
batch: 3371
train_loss: 542.502748966217
batch: 3372
train_loss: 545.4880263805389
batch: 3373
train_loss: 548.5908012390137
batch: 3374
train_loss: 551.6928961277008
batch: 3375
train_loss: 554.8075604438782
batch: 3376
train_loss: 557.8932139873505
batch: 3377
train_loss: 561.0656476020813
batch: 3378
train_loss: 564.2665927410126
batch: 3379
train_loss: 567.4224848747253
batch: 3380
train_loss: 570.4726898670197
batch: 3381
train_loss: 573.5865697860718
batch: 3382
train_loss: 576.8051390647888
batch: 3383
train_loss: 580.009213924408
batch: 3384
train_loss: 583.06791472435
batch: 3385
train_loss: 586.3171663284302
batch: 3386
train_loss: 589.4055986404419
batch: 3387
train_loss: 592.5011947154999
batch: 3388
train_loss: 595.621141910553
batch: 3389
train_loss: 598.7685792446136
batch: 3390
train_loss: 601.840621471405
batch: 3391
train_loss: 604.8583934307098
batch: 3392
train_loss: 607.9802289009094
batch: 3393
train_loss: 611.0650935173035
batch: 3394
train_loss: 614.2482476234436
batch: 3395
train_loss: 617.4240000247955
batch: 3396
train_loss: 620.6691505908966
batch: 3397
train_loss: 623.9334146976471
batch: 3398
train_loss: 627.0941441059113
batch: 3399
train_loss: 630.3082060813904
| epoch   0 step     3400 |   3400 batches | lr 0.000246 | ms/batch 150.61 | loss  3.15 | bpc   4.54671
batch: 3400
train_loss: 3.1494460105895996
batch: 3401
train_loss: 6.335842609405518
batch: 3402
train_loss: 9.520104885101318
batch: 3403
train_loss: 12.594850063323975
batch: 3404
train_loss: 15.753836870193481
batch: 3405
train_loss: 18.895532369613647
batch: 3406
train_loss: 22.03882098197937
batch: 3407
train_loss: 25.2288498878479
batch: 3408
train_loss: 28.383453607559204
batch: 3409
train_loss: 31.527926206588745
batch: 3410
train_loss: 34.730839014053345
batch: 3411
train_loss: 37.836177349090576
batch: 3412
train_loss: 41.02618646621704
batch: 3413
train_loss: 44.23616576194763
batch: 3414
train_loss: 47.4514479637146
batch: 3415
train_loss: 50.65542197227478
batch: 3416
train_loss: 53.835973024368286
batch: 3417
train_loss: 56.985270977020264
batch: 3418
train_loss: 60.17545437812805
batch: 3419
train_loss: 63.394896268844604
batch: 3420
train_loss: 66.59643912315369
batch: 3421
train_loss: 69.68794131278992
batch: 3422
train_loss: 72.79243588447571
batch: 3423
train_loss: 75.94266319274902
batch: 3424
train_loss: 79.12962985038757
batch: 3425
train_loss: 82.26367926597595
batch: 3426
train_loss: 85.34688758850098
batch: 3427
train_loss: 88.42449378967285
batch: 3428
train_loss: 91.58914017677307
batch: 3429
train_loss: 94.77175736427307
batch: 3430
train_loss: 97.87618088722229
batch: 3431
train_loss: 101.09365963935852
batch: 3432
train_loss: 104.20436072349548
batch: 3433
train_loss: 107.43340253829956
batch: 3434
train_loss: 110.59400367736816
batch: 3435
train_loss: 113.74053144454956
batch: 3436
train_loss: 116.8702609539032
batch: 3437
train_loss: 120.08130311965942
batch: 3438
train_loss: 123.2763946056366
batch: 3439
train_loss: 126.40371036529541
batch: 3440
train_loss: 129.63458633422852
batch: 3441
train_loss: 132.72526454925537
batch: 3442
train_loss: 135.85282111167908
batch: 3443
train_loss: 138.98105597496033
batch: 3444
train_loss: 142.1658420562744
batch: 3445
train_loss: 145.16515851020813
batch: 3446
train_loss: 148.3436634540558
batch: 3447
train_loss: 151.42774486541748
batch: 3448
train_loss: 154.46149110794067
batch: 3449
train_loss: 157.5664553642273
batch: 3450
train_loss: 160.67907047271729
batch: 3451
train_loss: 163.9128658771515
batch: 3452
train_loss: 167.04950308799744
batch: 3453
train_loss: 170.141184091568
batch: 3454
train_loss: 173.2016875743866
batch: 3455
train_loss: 176.290762424469
batch: 3456
train_loss: 179.42420601844788
batch: 3457
train_loss: 182.5698435306549
batch: 3458
train_loss: 185.65154266357422
batch: 3459
train_loss: 188.84548926353455
batch: 3460
train_loss: 191.9096541404724
batch: 3461
train_loss: 195.12540936470032
batch: 3462
train_loss: 198.25745582580566
batch: 3463
train_loss: 201.40879607200623
batch: 3464
train_loss: 204.5734703540802
batch: 3465
train_loss: 207.71055126190186
batch: 3466
train_loss: 210.74210858345032
batch: 3467
train_loss: 213.99175548553467
batch: 3468
train_loss: 217.18314504623413
batch: 3469
train_loss: 220.41145873069763
batch: 3470
train_loss: 223.58156156539917
batch: 3471
train_loss: 226.70903897285461
batch: 3472
train_loss: 229.8099603652954
batch: 3473
train_loss: 232.96496391296387
batch: 3474
train_loss: 236.07248187065125
batch: 3475
train_loss: 239.29973793029785
batch: 3476
train_loss: 242.3640956878662
batch: 3477
train_loss: 245.5515730381012
batch: 3478
train_loss: 248.66053700447083
batch: 3479
train_loss: 251.74136638641357
batch: 3480
train_loss: 254.84620332717896
batch: 3481
train_loss: 257.9538006782532
batch: 3482
train_loss: 260.9635593891144
batch: 3483
train_loss: 264.07669472694397
batch: 3484
train_loss: 267.20900654792786
batch: 3485
train_loss: 270.3765482902527
batch: 3486
train_loss: 273.4167490005493
batch: 3487
train_loss: 276.5167727470398
batch: 3488
train_loss: 279.7298984527588
batch: 3489
train_loss: 282.79918026924133
batch: 3490
train_loss: 285.80583453178406
batch: 3491
train_loss: 288.88264513015747
batch: 3492
train_loss: 292.04979515075684
batch: 3493
train_loss: 295.1229946613312
batch: 3494
train_loss: 298.1610767841339
batch: 3495
train_loss: 301.2335422039032
batch: 3496
train_loss: 304.2597370147705
batch: 3497
train_loss: 307.3516023159027
batch: 3498
train_loss: 310.50738072395325
batch: 3499
train_loss: 313.6025490760803
batch: 3500
train_loss: 316.8100519180298
batch: 3501
train_loss: 319.95036482810974
batch: 3502
train_loss: 323.1141617298126
batch: 3503
train_loss: 326.24382400512695
batch: 3504
train_loss: 329.3535158634186
batch: 3505
train_loss: 332.4926254749298
batch: 3506
train_loss: 335.6943533420563
batch: 3507
train_loss: 338.79207468032837
batch: 3508
train_loss: 341.87775349617004
batch: 3509
train_loss: 344.99177408218384
batch: 3510
train_loss: 348.1098597049713
batch: 3511
train_loss: 351.2768154144287
batch: 3512
train_loss: 354.2844579219818
batch: 3513
train_loss: 357.44337606430054
batch: 3514
train_loss: 360.67218470573425
batch: 3515
train_loss: 363.81745767593384
batch: 3516
train_loss: 366.98505878448486
batch: 3517
train_loss: 370.16034722328186
batch: 3518
train_loss: 373.4227318763733
batch: 3519
train_loss: 376.5179030895233
batch: 3520
train_loss: 379.68714451789856
batch: 3521
train_loss: 382.83489060401917
batch: 3522
train_loss: 386.04858779907227
batch: 3523
train_loss: 389.26767659187317
batch: 3524
train_loss: 392.5183358192444
batch: 3525
train_loss: 395.7307481765747
batch: 3526
train_loss: 398.9489703178406
batch: 3527
train_loss: 402.2419617176056
batch: 3528
train_loss: 405.59445428848267
batch: 3529
train_loss: 408.94733905792236
batch: 3530
train_loss: 412.2035171985626
batch: 3531
train_loss: 415.48840618133545
batch: 3532
train_loss: 418.70806193351746
batch: 3533
train_loss: 421.8926134109497
batch: 3534
train_loss: 425.03399682044983
batch: 3535
train_loss: 428.1952791213989
batch: 3536
train_loss: 431.3493854999542
batch: 3537
train_loss: 434.5525555610657
batch: 3538
train_loss: 437.84956312179565
batch: 3539
train_loss: 440.9892780780792
batch: 3540
train_loss: 444.17545890808105
batch: 3541
train_loss: 447.4230999946594
batch: 3542
train_loss: 450.66489458084106
batch: 3543
train_loss: 453.88639664649963
batch: 3544
train_loss: 457.0828278064728
batch: 3545
train_loss: 460.2120580673218
batch: 3546
train_loss: 463.45727825164795
batch: 3547
train_loss: 466.689249753952
batch: 3548
train_loss: 469.8589081764221
batch: 3549
train_loss: 472.89505529403687
batch: 3550
train_loss: 476.05279994010925
batch: 3551
train_loss: 479.14464950561523
batch: 3552
train_loss: 482.35236120224
batch: 3553
train_loss: 485.5733735561371
batch: 3554
train_loss: 488.7528917789459
batch: 3555
train_loss: 491.8913209438324
batch: 3556
train_loss: 495.11544036865234
batch: 3557
train_loss: 498.3324737548828
batch: 3558
train_loss: 501.4474573135376
batch: 3559
train_loss: 504.6147644519806
batch: 3560
train_loss: 507.7718577384949
batch: 3561
train_loss: 511.012170791626
batch: 3562
train_loss: 514.0941982269287
batch: 3563
train_loss: 517.3290503025055
batch: 3564
train_loss: 520.4240612983704
batch: 3565
train_loss: 523.5657904148102
batch: 3566
train_loss: 526.6551246643066
batch: 3567
train_loss: 529.8242712020874
batch: 3568
train_loss: 532.8852598667145
batch: 3569
train_loss: 535.9992687702179
batch: 3570
train_loss: 539.0420558452606
batch: 3571
train_loss: 542.1216578483582
batch: 3572
train_loss: 545.2963480949402
batch: 3573
train_loss: 548.4001429080963
batch: 3574
train_loss: 551.437784910202
batch: 3575
train_loss: 554.5951344966888
batch: 3576
train_loss: 557.851283788681
batch: 3577
train_loss: 561.0597434043884
batch: 3578
train_loss: 564.1507186889648
batch: 3579
train_loss: 567.286426782608
batch: 3580
train_loss: 570.3816463947296
batch: 3581
train_loss: 573.5312159061432
batch: 3582
train_loss: 576.6727120876312
batch: 3583
train_loss: 579.7815399169922
batch: 3584
train_loss: 582.9339294433594
batch: 3585
train_loss: 586.0895171165466
batch: 3586
train_loss: 589.1481070518494
batch: 3587
train_loss: 592.2756867408752
batch: 3588
train_loss: 595.5371813774109
batch: 3589
train_loss: 598.7099709510803
batch: 3590
train_loss: 601.7496829032898
batch: 3591
train_loss: 604.8092186450958
batch: 3592
train_loss: 608.0333526134491
batch: 3593
train_loss: 611.2943940162659
batch: 3594
train_loss: 614.4737982749939
batch: 3595
train_loss: 617.7465515136719
batch: 3596
train_loss: 620.9553139209747
batch: 3597
train_loss: 624.001134634018
batch: 3598
train_loss: 627.1734519004822
batch: 3599
train_loss: 630.3527150154114
| epoch   0 step     3600 |   3600 batches | lr 0.000245 | ms/batch 150.42 | loss  3.15 | bpc   4.54703
batch: 3600
train_loss: 3.2336974143981934
batch: 3601
train_loss: 6.518499135971069
batch: 3602
train_loss: 9.657798290252686
batch: 3603
train_loss: 12.873133182525635
batch: 3604
train_loss: 16.154752254486084
batch: 3605
train_loss: 19.352389574050903
batch: 3606
train_loss: 22.542683362960815
batch: 3607
train_loss: 25.93667697906494
batch: 3608
train_loss: 29.22406244277954
batch: 3609
train_loss: 32.48589015007019
batch: 3610
train_loss: 35.82890486717224
batch: 3611
train_loss: 39.11143183708191
batch: 3612
train_loss: 42.391786336898804
batch: 3613
train_loss: 45.56846308708191
batch: 3614
train_loss: 48.747042179107666
batch: 3615
train_loss: 51.83337426185608
batch: 3616
train_loss: 54.91805696487427
batch: 3617
train_loss: 58.024765491485596
batch: 3618
train_loss: 61.25548052787781
batch: 3619
train_loss: 64.43529796600342
batch: 3620
train_loss: 67.81706595420837
batch: 3621
train_loss: 71.09238719940186
batch: 3622
train_loss: 74.2438132762909
batch: 3623
train_loss: 77.41017770767212
batch: 3624
train_loss: 80.51086211204529
batch: 3625
train_loss: 83.6885986328125
batch: 3626
train_loss: 86.84980630874634
batch: 3627
train_loss: 90.08687090873718
batch: 3628
train_loss: 93.23295783996582
batch: 3629
train_loss: 96.27311158180237
batch: 3630
train_loss: 99.2721266746521
batch: 3631
train_loss: 102.40862965583801
batch: 3632
train_loss: 105.49743509292603
batch: 3633
train_loss: 108.56699442863464
batch: 3634
train_loss: 111.69044494628906
batch: 3635
train_loss: 114.74899911880493
batch: 3636
train_loss: 117.82548928260803
batch: 3637
train_loss: 120.98919606208801
batch: 3638
train_loss: 124.15188360214233
batch: 3639
train_loss: 127.21466732025146
batch: 3640
train_loss: 130.41268157958984
batch: 3641
train_loss: 133.53777527809143
batch: 3642
train_loss: 136.704252243042
batch: 3643
train_loss: 139.76401233673096
batch: 3644
train_loss: 142.91291046142578
batch: 3645
train_loss: 146.1317572593689
batch: 3646
train_loss: 149.3081977367401
batch: 3647
train_loss: 152.39368796348572
batch: 3648
train_loss: 155.41099977493286
batch: 3649
train_loss: 158.60447216033936
batch: 3650
train_loss: 161.63832449913025
batch: 3651
train_loss: 164.77213668823242
batch: 3652
train_loss: 167.79786849021912
batch: 3653
train_loss: 170.9337441921234
batch: 3654
train_loss: 174.02040123939514
batch: 3655
train_loss: 177.09897661209106
batch: 3656
train_loss: 180.12082433700562
batch: 3657
train_loss: 183.1823706626892
batch: 3658
train_loss: 186.23735928535461
batch: 3659
train_loss: 189.30450415611267
batch: 3660
train_loss: 192.42060136795044
batch: 3661
train_loss: 195.5234386920929
batch: 3662
train_loss: 198.6280002593994
batch: 3663
train_loss: 201.79882788658142
batch: 3664
train_loss: 205.00526571273804
batch: 3665
train_loss: 208.12210822105408
batch: 3666
train_loss: 211.19478940963745
batch: 3667
train_loss: 214.2026982307434
batch: 3668
train_loss: 217.24712204933167
batch: 3669
train_loss: 220.24553751945496
batch: 3670
train_loss: 223.30234742164612
batch: 3671
train_loss: 226.48639130592346
batch: 3672
train_loss: 229.6763973236084
batch: 3673
train_loss: 232.74476671218872
batch: 3674
train_loss: 235.8411831855774
batch: 3675
train_loss: 238.80026960372925
batch: 3676
train_loss: 241.77458453178406
batch: 3677
train_loss: 244.81304907798767
batch: 3678
train_loss: 248.0136775970459
batch: 3679
train_loss: 251.06762099266052
batch: 3680
train_loss: 254.11638021469116
batch: 3681
train_loss: 257.1410689353943
batch: 3682
train_loss: 260.22772216796875
batch: 3683
train_loss: 263.3271939754486
batch: 3684
train_loss: 266.4490110874176
batch: 3685
train_loss: 269.58294916152954
batch: 3686
train_loss: 272.6193480491638
batch: 3687
train_loss: 275.66046118736267
batch: 3688
train_loss: 278.71744656562805
batch: 3689
train_loss: 281.81589221954346
batch: 3690
train_loss: 284.94741916656494
batch: 3691
train_loss: 288.08152651786804
batch: 3692
train_loss: 291.26922845840454
batch: 3693
train_loss: 294.47473096847534
batch: 3694
train_loss: 297.61668634414673
batch: 3695
train_loss: 300.7880575656891
batch: 3696
train_loss: 303.88226079940796
batch: 3697
train_loss: 307.03983426094055
batch: 3698
train_loss: 310.14231538772583
batch: 3699
train_loss: 313.2602467536926
batch: 3700
train_loss: 316.33951592445374
batch: 3701
train_loss: 319.430330991745
batch: 3702
train_loss: 322.56072473526
batch: 3703
train_loss: 325.70300674438477
batch: 3704
train_loss: 328.89544677734375
batch: 3705
train_loss: 332.14100432395935
batch: 3706
train_loss: 335.2966914176941
batch: 3707
train_loss: 338.39091897010803
batch: 3708
train_loss: 341.5907623767853
batch: 3709
train_loss: 344.8115577697754
batch: 3710
train_loss: 347.9800384044647
batch: 3711
train_loss: 351.20509934425354
batch: 3712
train_loss: 354.3444426059723
batch: 3713
train_loss: 357.4521086215973
batch: 3714
train_loss: 360.62858176231384
batch: 3715
train_loss: 363.70622754096985
batch: 3716
train_loss: 366.87780570983887
batch: 3717
train_loss: 370.15850043296814
batch: 3718
train_loss: 373.27133083343506
batch: 3719
train_loss: 376.3877913951874
batch: 3720
train_loss: 379.4821469783783
batch: 3721
train_loss: 382.5284049510956
batch: 3722
train_loss: 385.75268840789795
batch: 3723
train_loss: 388.8631420135498
batch: 3724
train_loss: 391.951833486557
batch: 3725
train_loss: 395.1117949485779
batch: 3726
train_loss: 398.184761762619
batch: 3727
train_loss: 401.3879988193512
batch: 3728
train_loss: 404.5107054710388
batch: 3729
train_loss: 407.596919298172
batch: 3730
train_loss: 410.73934268951416
batch: 3731
train_loss: 413.8749134540558
batch: 3732
train_loss: 417.1688733100891
batch: 3733
train_loss: 420.36155223846436
batch: 3734
train_loss: 423.57549262046814
batch: 3735
train_loss: 426.69091415405273
batch: 3736
train_loss: 429.79932475090027
batch: 3737
train_loss: 432.94509959220886
batch: 3738
train_loss: 436.1072509288788
batch: 3739
train_loss: 439.2499601840973
batch: 3740
train_loss: 442.44610953330994
batch: 3741
train_loss: 445.50228118896484
batch: 3742
train_loss: 448.7035675048828
batch: 3743
train_loss: 451.9137690067291
batch: 3744
train_loss: 455.19203066825867
batch: 3745
train_loss: 458.42210626602173
batch: 3746
train_loss: 461.6987931728363
batch: 3747
train_loss: 464.8497099876404
batch: 3748
train_loss: 468.0230588912964
batch: 3749
train_loss: 471.2616744041443
batch: 3750
train_loss: 474.4704170227051
batch: 3751
train_loss: 477.7172951698303
batch: 3752
train_loss: 480.9274203777313
batch: 3753
train_loss: 483.99807691574097
batch: 3754
train_loss: 487.15403842926025
batch: 3755
train_loss: 490.2409679889679
batch: 3756
train_loss: 493.39362597465515
batch: 3757
train_loss: 496.5605890750885
batch: 3758
train_loss: 499.77069211006165
batch: 3759
train_loss: 502.95878505706787
batch: 3760
train_loss: 506.1815242767334
batch: 3761
train_loss: 509.3879110813141
batch: 3762
train_loss: 512.5987968444824
batch: 3763
train_loss: 515.7907149791718
batch: 3764
train_loss: 518.976912021637
batch: 3765
train_loss: 522.1081583499908
batch: 3766
train_loss: 525.3778650760651
batch: 3767
train_loss: 528.4755167961121
batch: 3768
train_loss: 531.614949464798
batch: 3769
train_loss: 534.8200199604034
batch: 3770
train_loss: 537.982887506485
batch: 3771
train_loss: 541.2031259536743
batch: 3772
train_loss: 544.3727738857269
batch: 3773
train_loss: 547.5188417434692
batch: 3774
train_loss: 550.681074142456
batch: 3775
train_loss: 553.8966271877289
batch: 3776
train_loss: 557.0333108901978
batch: 3777
train_loss: 560.0752320289612
batch: 3778
train_loss: 563.0704011917114
batch: 3779
train_loss: 566.1895806789398
batch: 3780
train_loss: 569.1906032562256
batch: 3781
train_loss: 572.2355351448059
batch: 3782
train_loss: 575.2188847064972
batch: 3783
train_loss: 578.1983706951141
batch: 3784
train_loss: 581.3270382881165
batch: 3785
train_loss: 584.4577672481537
batch: 3786
train_loss: 587.567079782486
batch: 3787
train_loss: 590.6563084125519
batch: 3788
train_loss: 593.6973478794098
batch: 3789
train_loss: 596.7630605697632
batch: 3790
train_loss: 600.020233631134
batch: 3791
train_loss: 603.2178387641907
batch: 3792
train_loss: 606.3813478946686
batch: 3793
train_loss: 609.5315539836884
batch: 3794
train_loss: 612.6719243526459
batch: 3795
train_loss: 615.7605428695679
batch: 3796
train_loss: 618.8011555671692
batch: 3797
train_loss: 621.9557256698608
batch: 3798
train_loss: 625.1820132732391
batch: 3799
train_loss: 628.3052678108215
| epoch   0 step     3800 |   3800 batches | lr 0.000244 | ms/batch 150.94 | loss  3.14 | bpc   4.53226
batch: 3800
train_loss: 2.959796190261841
batch: 3801
train_loss: 6.121939182281494
batch: 3802
train_loss: 9.190969944000244
batch: 3803
train_loss: 12.236895084381104
batch: 3804
train_loss: 15.284694910049438
batch: 3805
train_loss: 18.369789123535156
batch: 3806
train_loss: 21.578561305999756
batch: 3807
train_loss: 24.746001482009888
batch: 3808
train_loss: 27.798914194107056
batch: 3809
train_loss: 30.890358448028564
batch: 3810
train_loss: 33.91254162788391
batch: 3811
train_loss: 36.9440062046051
batch: 3812
train_loss: 40.12904477119446
batch: 3813
train_loss: 43.26371955871582
batch: 3814
train_loss: 46.41762447357178
batch: 3815
train_loss: 49.52888560295105
batch: 3816
train_loss: 52.67698264122009
batch: 3817
train_loss: 55.83354687690735
batch: 3818
train_loss: 59.01179313659668
batch: 3819
train_loss: 62.128774642944336
batch: 3820
train_loss: 65.30355644226074
batch: 3821
train_loss: 68.4147732257843
batch: 3822
train_loss: 71.67437767982483
batch: 3823
train_loss: 74.78872156143188
batch: 3824
train_loss: 77.83584356307983
batch: 3825
train_loss: 80.983877658844
batch: 3826
train_loss: 84.05051517486572
batch: 3827
train_loss: 87.16532588005066
batch: 3828
train_loss: 90.25702571868896
batch: 3829
train_loss: 93.21018505096436
batch: 3830
train_loss: 96.23753714561462
batch: 3831
train_loss: 99.3412299156189
batch: 3832
train_loss: 102.45408797264099
batch: 3833
train_loss: 105.60991930961609
batch: 3834
train_loss: 108.73056221008301
batch: 3835
train_loss: 111.81718802452087
batch: 3836
train_loss: 115.00217127799988
batch: 3837
train_loss: 118.00225448608398
batch: 3838
train_loss: 121.07595992088318
batch: 3839
train_loss: 124.1728732585907
batch: 3840
train_loss: 127.23902082443237
batch: 3841
train_loss: 130.22548246383667
batch: 3842
train_loss: 133.35820269584656
batch: 3843
train_loss: 136.45210981369019
batch: 3844
train_loss: 139.55734729766846
batch: 3845
train_loss: 142.62514305114746
batch: 3846
train_loss: 145.7628185749054
batch: 3847
train_loss: 148.83882546424866
batch: 3848
train_loss: 151.84636306762695
batch: 3849
train_loss: 154.9411964416504
batch: 3850
train_loss: 157.9451596736908
batch: 3851
train_loss: 161.00372838974
batch: 3852
train_loss: 164.17911767959595
batch: 3853
train_loss: 167.18647027015686
batch: 3854
train_loss: 170.3442268371582
batch: 3855
train_loss: 173.32795023918152
batch: 3856
train_loss: 176.3588056564331
batch: 3857
train_loss: 179.47477793693542
batch: 3858
train_loss: 182.53167867660522
batch: 3859
train_loss: 185.4743766784668
batch: 3860
train_loss: 188.55945348739624
batch: 3861
train_loss: 191.5771622657776
batch: 3862
train_loss: 194.66924810409546
batch: 3863
train_loss: 197.74096131324768
batch: 3864
train_loss: 200.95559525489807
batch: 3865
train_loss: 204.08615922927856
batch: 3866
train_loss: 207.16227865219116
batch: 3867
train_loss: 210.3662886619568
batch: 3868
train_loss: 213.4978539943695
batch: 3869
train_loss: 216.6032054424286
batch: 3870
train_loss: 219.7030668258667
batch: 3871
train_loss: 222.71400666236877
batch: 3872
train_loss: 225.8752462863922
batch: 3873
train_loss: 228.9327359199524
batch: 3874
train_loss: 231.96358442306519
batch: 3875
train_loss: 235.15562868118286
batch: 3876
train_loss: 238.27175402641296
batch: 3877
train_loss: 241.41866850852966
batch: 3878
train_loss: 244.46732878684998
batch: 3879
train_loss: 247.67527556419373
batch: 3880
train_loss: 250.7425079345703
batch: 3881
train_loss: 253.84550499916077
batch: 3882
train_loss: 256.9603433609009
batch: 3883
train_loss: 260.0393648147583
batch: 3884
train_loss: 263.13796496391296
batch: 3885
train_loss: 266.2539281845093
batch: 3886
train_loss: 269.27028703689575
batch: 3887
train_loss: 272.3430118560791
batch: 3888
train_loss: 275.3158793449402
batch: 3889
train_loss: 278.33533811569214
batch: 3890
train_loss: 281.3315668106079
batch: 3891
train_loss: 284.3903124332428
batch: 3892
train_loss: 287.41611194610596
batch: 3893
train_loss: 290.48372292518616
batch: 3894
train_loss: 293.50367736816406
batch: 3895
train_loss: 296.5500955581665
batch: 3896
train_loss: 299.58553552627563
batch: 3897
train_loss: 302.770503282547
batch: 3898
train_loss: 305.82947182655334
batch: 3899
train_loss: 308.9389307498932
batch: 3900
train_loss: 312.0176811218262
batch: 3901
train_loss: 315.1797800064087
batch: 3902
train_loss: 318.27361488342285
batch: 3903
train_loss: 321.370374917984
batch: 3904
train_loss: 324.4228127002716
batch: 3905
train_loss: 327.497679233551
batch: 3906
train_loss: 330.56365036964417
batch: 3907
train_loss: 333.68076753616333
batch: 3908
train_loss: 336.72502088546753
batch: 3909
train_loss: 339.8858926296234
batch: 3910
train_loss: 342.9959008693695
batch: 3911
train_loss: 346.0207781791687
batch: 3912
train_loss: 349.11823320388794
batch: 3913
train_loss: 352.22869634628296
batch: 3914
train_loss: 355.3335506916046
batch: 3915
train_loss: 358.3863248825073
batch: 3916
train_loss: 361.6011345386505
batch: 3917
train_loss: 364.6784896850586
batch: 3918
train_loss: 367.77043747901917
batch: 3919
train_loss: 370.86070942878723
batch: 3920
train_loss: 373.88136982917786
batch: 3921
train_loss: 376.908798456192
batch: 3922
train_loss: 379.9896876811981
batch: 3923
train_loss: 383.1118075847626
batch: 3924
train_loss: 386.125905752182
batch: 3925
train_loss: 389.1837258338928
batch: 3926
train_loss: 392.14835691452026
batch: 3927
train_loss: 395.1691257953644
batch: 3928
train_loss: 398.2312548160553
batch: 3929
train_loss: 401.25517320632935
batch: 3930
train_loss: 404.3691599369049
batch: 3931
train_loss: 407.44480061531067
batch: 3932
train_loss: 410.5720269680023
batch: 3933
train_loss: 413.62307500839233
batch: 3934
train_loss: 416.7011113166809
batch: 3935
train_loss: 419.7702395915985
batch: 3936
train_loss: 422.7899854183197
batch: 3937
train_loss: 425.89399790763855
batch: 3938
train_loss: 429.0493094921112
batch: 3939
train_loss: 432.03990387916565
batch: 3940
train_loss: 435.1030683517456
batch: 3941
train_loss: 438.2171354293823
batch: 3942
train_loss: 441.2617027759552
batch: 3943
train_loss: 444.3569839000702
batch: 3944
train_loss: 447.5033392906189
batch: 3945
train_loss: 450.6248345375061
batch: 3946
train_loss: 453.6996283531189
batch: 3947
train_loss: 456.6613404750824
batch: 3948
train_loss: 459.67486572265625
batch: 3949
train_loss: 462.67057394981384
batch: 3950
train_loss: 465.64720344543457
batch: 3951
train_loss: 468.6762845516205
batch: 3952
train_loss: 471.6912670135498
batch: 3953
train_loss: 474.7053461074829
batch: 3954
train_loss: 477.7061791419983
batch: 3955
train_loss: 480.7539596557617
batch: 3956
