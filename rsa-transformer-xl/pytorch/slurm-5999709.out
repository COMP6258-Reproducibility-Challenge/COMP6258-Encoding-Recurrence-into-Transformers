Running SLURM prolog script on pink57.cluster.local
===============================================================================
Job started on Fri May 17 02:35:45 BST 2024
Job ID          : 5999709
Job name        : run_enwik8_rsa_small.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa_small.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink57
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 7
    - n_head : 8
    - d_head : 64
    - d_embed : 256
    - d_model : 256
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 12004328
    - n_nonemb_param : 11950876
====================================================================================================
#params = 12004328
#non emb params = 11950876
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.439916610717773
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.401891708374023
batch: 2
train_loss: 15.117094039916992
batch: 3
train_loss: 19.70103168487549
batch: 4
train_loss: 24.213568210601807
batch: 5
train_loss: 28.605947017669678
batch: 6
train_loss: 32.961602210998535
batch: 7
train_loss: 37.300156116485596
batch: 8
train_loss: 41.55203342437744
batch: 9
train_loss: 45.68826961517334
batch: 10
train_loss: 49.739959716796875
batch: 11
train_loss: 53.7641396522522
batch: 12
train_loss: 57.77233362197876
batch: 13
train_loss: 61.72175908088684
batch: 14
train_loss: 65.62663865089417
batch: 15
train_loss: 69.52210092544556
batch: 16
train_loss: 73.37809920310974
batch: 17
train_loss: 77.18112969398499
batch: 18
train_loss: 80.98705697059631
batch: 19
train_loss: 84.72910332679749
batch: 20
train_loss: 88.41888427734375
batch: 21
train_loss: 92.05285096168518
batch: 22
train_loss: 95.7529776096344
batch: 23
train_loss: 99.45161581039429
batch: 24
train_loss: 103.1009132862091
batch: 25
train_loss: 106.75798964500427
batch: 26
train_loss: 110.33005928993225
batch: 27
train_loss: 113.93916821479797
batch: 28
train_loss: 117.54120302200317
batch: 29
train_loss: 121.21919226646423
batch: 30
train_loss: 124.83581805229187
batch: 31
train_loss: 128.3355357646942
batch: 32
train_loss: 131.83635115623474
batch: 33
train_loss: 135.34700322151184
batch: 34
train_loss: 138.79296445846558
batch: 35
train_loss: 142.2613561153412
batch: 36
train_loss: 145.73412442207336
batch: 37
train_loss: 149.26876735687256
batch: 38
train_loss: 152.75378608703613
batch: 39
train_loss: 156.19089484214783
batch: 40
train_loss: 159.67462873458862
batch: 41
train_loss: 163.14726734161377
batch: 42
train_loss: 166.51795840263367
batch: 43
train_loss: 169.8943066596985
batch: 44
train_loss: 173.25460648536682
batch: 45
train_loss: 176.68138337135315
batch: 46
train_loss: 180.1558289527893
batch: 47
train_loss: 183.69274830818176
batch: 48
train_loss: 187.19816756248474
batch: 49
train_loss: 190.71654844284058
batch: 50
train_loss: 194.26016187667847
batch: 51
train_loss: 197.7742486000061
batch: 52
train_loss: 201.22658252716064
batch: 53
train_loss: 204.77608561515808
batch: 54
train_loss: 208.31176447868347
batch: 55
train_loss: 211.87461829185486
batch: 56
train_loss: 215.32363653182983
batch: 57
train_loss: 218.76006245613098
batch: 58
train_loss: 222.23687887191772
batch: 59
train_loss: 225.72554683685303
batch: 60
train_loss: 229.12241578102112
batch: 61
train_loss: 232.57897686958313
batch: 62
train_loss: 236.0970721244812
batch: 63
train_loss: 239.60232639312744
batch: 64
train_loss: 243.0841338634491
batch: 65
train_loss: 246.55466222763062
batch: 66
train_loss: 250.04233169555664
batch: 67
train_loss: 253.53634119033813
batch: 68
train_loss: 257.0749423503876
batch: 69
train_loss: 260.60102558135986
batch: 70
train_loss: 264.1014177799225
batch: 71
train_loss: 267.56379103660583
batch: 72
train_loss: 271.0675468444824
batch: 73
train_loss: 274.5417547225952
batch: 74
train_loss: 278.0045895576477
batch: 75
train_loss: 281.5570502281189
batch: 76
train_loss: 285.0277588367462
batch: 77
train_loss: 288.53226923942566
batch: 78
train_loss: 292.08699321746826
batch: 79
train_loss: 295.69920229911804
batch: 80
train_loss: 299.2423450946808
batch: 81
train_loss: 302.8717107772827
batch: 82
train_loss: 306.4147152900696
batch: 83
train_loss: 309.97570872306824
batch: 84
train_loss: 313.45370841026306
batch: 85
train_loss: 316.9767291545868
batch: 86
train_loss: 320.5270662307739
batch: 87
train_loss: 324.0550935268402
batch: 88
train_loss: 327.59135913848877
batch: 89
train_loss: 331.09161257743835
batch: 90
train_loss: 334.5857763290405
batch: 91
train_loss: 338.0252683162689
batch: 92
train_loss: 341.53191351890564
batch: 93
train_loss: 345.0039029121399
batch: 94
train_loss: 348.4771304130554
batch: 95
train_loss: 351.98355889320374
batch: 96
train_loss: 355.57699155807495
batch: 97
train_loss: 359.06131625175476
batch: 98
train_loss: 362.5736765861511
batch: 99
train_loss: 366.08663988113403
batch: 100
train_loss: 369.64604210853577
batch: 101
train_loss: 373.2082996368408
batch: 102
train_loss: 376.71530199050903
batch: 103
train_loss: 380.2416651248932
batch: 104
train_loss: 383.71492052078247
batch: 105
train_loss: 387.19864678382874
batch: 106
train_loss: 390.6890199184418
batch: 107
train_loss: 394.15100479125977
batch: 108
train_loss: 397.5661361217499
batch: 109
train_loss: 400.9341218471527
batch: 110
train_loss: 404.32077050209045
batch: 111
train_loss: 407.7565653324127
batch: 112
train_loss: 411.3208658695221
batch: 113
train_loss: 414.8378040790558
batch: 114
train_loss: 418.393807888031
batch: 115
train_loss: 421.9212920665741
batch: 116
train_loss: 425.4345557689667
batch: 117
train_loss: 428.97203540802
batch: 118
train_loss: 432.50249099731445
batch: 119
train_loss: 436.018137216568
batch: 120
train_loss: 439.671222448349
batch: 121
train_loss: 443.2992482185364
batch: 122
train_loss: 446.8724567890167
batch: 123
train_loss: 450.45898246765137
batch: 124
train_loss: 453.97150468826294
batch: 125
train_loss: 457.5114047527313
batch: 126
train_loss: 461.0993905067444
batch: 127
train_loss: 464.5457673072815
batch: 128
train_loss: 468.04069924354553
batch: 129
train_loss: 471.4711663722992
batch: 130
train_loss: 474.8851351737976
batch: 131
train_loss: 478.24674558639526
batch: 132
train_loss: 481.6455616950989
batch: 133
train_loss: 485.0783543586731
batch: 134
train_loss: 488.5393168926239
batch: 135
train_loss: 492.0180027484894
batch: 136
train_loss: 495.47233414649963
batch: 137
train_loss: 498.9128005504608
batch: 138
train_loss: 502.3132574558258
batch: 139
train_loss: 505.72574973106384
batch: 140
train_loss: 509.20240235328674
batch: 141
train_loss: 512.7081577777863
batch: 142
train_loss: 516.1724445819855
batch: 143
train_loss: 519.5438120365143
batch: 144
train_loss: 522.9890322685242
batch: 145
train_loss: 526.4601109027863
batch: 146
train_loss: 529.9792540073395
batch: 147
train_loss: 533.4357063770294
batch: 148
train_loss: 536.9106917381287
batch: 149
train_loss: 540.4844679832458
batch: 150
train_loss: 544.1458899974823
batch: 151
train_loss: 547.7410111427307
batch: 152
train_loss: 551.2387275695801
batch: 153
train_loss: 554.6614389419556
batch: 154
train_loss: 558.1849672794342
batch: 155
train_loss: 561.716833114624
batch: 156
train_loss: 565.1637144088745
batch: 157
train_loss: 568.6461615562439
batch: 158
train_loss: 572.130829334259
batch: 159
train_loss: 575.5884199142456
batch: 160
train_loss: 579.061231136322
batch: 161
train_loss: 582.5727326869965
batch: 162
train_loss: 586.1261179447174
batch: 163
train_loss: 589.6448848247528
batch: 164
train_loss: 593.1222047805786
batch: 165
train_loss: 596.6469721794128
batch: 166
train_loss: 600.1009006500244
batch: 167
train_loss: 603.5306470394135
batch: 168
train_loss: 606.9574928283691
batch: 169
train_loss: 610.3289656639099
batch: 170
train_loss: 613.734808921814
batch: 171
train_loss: 617.1638996601105
batch: 172
train_loss: 620.5464611053467
batch: 173
train_loss: 623.9200007915497
batch: 174
train_loss: 627.3151438236237
batch: 175
train_loss: 630.7599740028381
batch: 176
train_loss: 634.1685450077057
batch: 177
train_loss: 637.6196942329407
batch: 178
train_loss: 641.0175559520721
batch: 179
train_loss: 644.5417375564575
batch: 180
train_loss: 647.9671845436096
batch: 181
train_loss: 651.345223903656
batch: 182
train_loss: 654.6469285488129
batch: 183
train_loss: 657.915849685669
batch: 184
train_loss: 661.258106470108
batch: 185
train_loss: 664.5799980163574
batch: 186
train_loss: 667.9478025436401
batch: 187
train_loss: 671.2643744945526
batch: 188
train_loss: 674.7280912399292
batch: 189
train_loss: 678.1327896118164
batch: 190
train_loss: 681.5603668689728
batch: 191
train_loss: 684.9464063644409
batch: 192
train_loss: 688.3788847923279
batch: 193
train_loss: 691.8324918746948
batch: 194
train_loss: 695.3897380828857
batch: 195
train_loss: 698.8398535251617
batch: 196
train_loss: 702.3278195858002
batch: 197
train_loss: 705.7742741107941
batch: 198
train_loss: 709.2222821712494
batch: 199
train_loss: 712.5831854343414
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 35987.33 | loss  3.56 | bpc   5.14020
batch: 200
train_loss: 3.39302134513855
batch: 201
train_loss: 6.852450370788574
batch: 202
train_loss: 10.355554819107056
batch: 203
train_loss: 13.877504348754883
batch: 204
train_loss: 17.443936586380005
batch: 205
train_loss: 20.98946714401245
batch: 206
train_loss: 24.452389001846313
batch: 207
train_loss: 27.87167739868164
batch: 208
train_loss: 31.252386331558228
batch: 209
train_loss: 34.66287136077881
batch: 210
train_loss: 38.045435667037964
batch: 211
train_loss: 41.458972692489624
batch: 212
train_loss: 44.90979361534119
batch: 213
train_loss: 48.3867347240448
batch: 214
train_loss: 51.84869599342346
batch: 215
train_loss: 55.302531242370605
batch: 216
train_loss: 58.81265640258789
batch: 217
train_loss: 62.23048639297485
batch: 218
train_loss: 65.62886500358582
batch: 219
train_loss: 69.04853367805481
batch: 220
train_loss: 72.61269402503967
batch: 221
train_loss: 76.08848595619202
batch: 222
train_loss: 79.6070191860199
batch: 223
train_loss: 83.06038618087769
batch: 224
train_loss: 86.57302594184875
batch: 225
train_loss: 90.16968202590942
batch: 226
train_loss: 93.710764169693
batch: 227
train_loss: 97.16832542419434
batch: 228
train_loss: 100.607253074646
batch: 229
train_loss: 104.05962872505188
batch: 230
train_loss: 107.52948474884033
batch: 231
train_loss: 111.09308624267578
batch: 232
train_loss: 114.63061809539795
batch: 233
train_loss: 118.17070269584656
batch: 234
train_loss: 121.8023247718811
batch: 235
train_loss: 125.43889236450195
batch: 236
train_loss: 128.8837022781372
batch: 237
train_loss: 132.35758900642395
batch: 238
train_loss: 135.89404129981995
batch: 239
train_loss: 139.37269830703735
batch: 240
train_loss: 142.80539965629578
batch: 241
train_loss: 146.27562141418457
batch: 242
train_loss: 149.7727987766266
batch: 243
train_loss: 153.2154347896576
batch: 244
train_loss: 156.6932234764099
batch: 245
train_loss: 160.138201713562
batch: 246
train_loss: 163.5965645313263
batch: 247
train_loss: 166.97991037368774
batch: 248
train_loss: 170.35667181015015
batch: 249
train_loss: 173.82754802703857
batch: 250
train_loss: 177.24709916114807
batch: 251
train_loss: 180.712415933609
batch: 252
train_loss: 184.11662983894348
batch: 253
train_loss: 187.61937165260315
batch: 254
train_loss: 191.10184860229492
batch: 255
train_loss: 194.6734070777893
batch: 256
train_loss: 198.2366485595703
batch: 257
train_loss: 201.682110786438
batch: 258
train_loss: 205.07183933258057
batch: 259
train_loss: 208.5259211063385
batch: 260
train_loss: 211.89315581321716
batch: 261
train_loss: 215.2848949432373
batch: 262
train_loss: 218.6346652507782
batch: 263
train_loss: 221.97880291938782
batch: 264
train_loss: 225.4079999923706
batch: 265
train_loss: 228.88430452346802
batch: 266
train_loss: 232.34208941459656
batch: 267
train_loss: 235.79716682434082
batch: 268
train_loss: 239.24466061592102
batch: 269
train_loss: 242.75460863113403
batch: 270
train_loss: 246.31675100326538
batch: 271
train_loss: 249.86706972122192
batch: 272
train_loss: 253.43623304367065
batch: 273
train_loss: 256.902734041214
batch: 274
train_loss: 260.3859910964966
batch: 275
train_loss: 263.8695116043091
batch: 276
train_loss: 267.4981508255005
batch: 277
train_loss: 270.9627196788788
batch: 278
train_loss: 274.48233461380005
batch: 279
train_loss: 278.04148507118225
batch: 280
train_loss: 281.5702164173126
batch: 281
train_loss: 285.0989980697632
batch: 282
train_loss: 288.6028137207031
batch: 283
train_loss: 292.16840982437134
batch: 284
train_loss: 295.7238290309906
batch: 285
train_loss: 299.23228645324707
batch: 286
train_loss: 302.768826007843
batch: 287
train_loss: 306.3059096336365
batch: 288
train_loss: 309.85468196868896
batch: 289
train_loss: 313.39036083221436
batch: 290
train_loss: 316.89977645874023
batch: 291
train_loss: 320.35701847076416
batch: 292
train_loss: 323.87929487228394
batch: 293
train_loss: 327.35064816474915
batch: 294
train_loss: 330.7995276451111
batch: 295
train_loss: 334.291095495224
batch: 296
train_loss: 337.87631034851074
batch: 297
train_loss: 341.5005404949188
batch: 298
train_loss: 345.0795841217041
batch: 299
train_loss: 348.56758666038513
batch: 300
train_loss: 352.0335838794708
batch: 301
train_loss: 355.4893684387207
batch: 302
train_loss: 358.9703505039215
batch: 303
train_loss: 362.4096863269806
batch: 304
train_loss: 365.8832850456238
batch: 305
train_loss: 369.40087699890137
batch: 306
train_loss: 372.85233211517334
batch: 307
train_loss: 376.31177139282227
batch: 308
train_loss: 379.8225347995758
batch: 309
train_loss: 383.35326623916626
batch: 310
train_loss: 386.8896155357361
batch: 311
train_loss: 390.3574712276459
batch: 312
train_loss: 393.7804365158081
batch: 313
train_loss: 397.2470426559448
batch: 314
train_loss: 400.7784719467163
batch: 315
train_loss: 404.33393907546997
batch: 316
train_loss: 407.8704993724823
batch: 317
train_loss: 411.3712091445923
batch: 318
train_loss: 414.84047865867615
batch: 319
train_loss: 418.3318991661072
batch: 320
train_loss: 421.80110025405884
batch: 321
train_loss: 425.2781970500946
batch: 322
train_loss: 428.77254986763
batch: 323
train_loss: 432.2770457267761
batch: 324
train_loss: 435.6976511478424
batch: 325
train_loss: 439.1585590839386
batch: 326
train_loss: 442.69752526283264
batch: 327
train_loss: 446.1851441860199
batch: 328
train_loss: 449.6843521595001
batch: 329
train_loss: 453.1252431869507
batch: 330
train_loss: 456.6321105957031
batch: 331
train_loss: 460.0888237953186
batch: 332
train_loss: 463.606808423996
batch: 333
train_loss: 467.1389207839966
batch: 334
train_loss: 470.65943002700806
batch: 335
train_loss: 474.1197030544281
batch: 336
train_loss: 477.5357642173767
batch: 337
train_loss: 480.8966658115387
batch: 338
train_loss: 484.3611545562744
batch: 339
train_loss: 487.8350405693054
batch: 340
train_loss: 491.2891376018524
batch: 341
train_loss: 494.70747661590576
batch: 342
train_loss: 498.14904022216797
batch: 343
train_loss: 501.46312618255615
batch: 344
train_loss: 504.8876967430115
batch: 345
train_loss: 508.3394386768341
batch: 346
train_loss: 511.9380211830139
batch: 347
train_loss: 515.4975714683533
batch: 348
train_loss: 519.0377006530762
batch: 349
train_loss: 522.6850957870483
batch: 350
train_loss: 526.1670219898224
batch: 351
train_loss: 529.6834418773651
batch: 352
train_loss: 533.1785702705383
batch: 353
train_loss: 536.6881606578827
batch: 354
train_loss: 540.231627702713
batch: 355
train_loss: 543.7553703784943
batch: 356
train_loss: 547.272698879242
batch: 357
train_loss: 550.7570097446442
batch: 358
train_loss: 554.2896871566772
batch: 359
train_loss: 557.8171474933624
batch: 360
train_loss: 561.3751797676086
batch: 361
train_loss: 564.7889544963837
batch: 362
train_loss: 568.1247036457062
batch: 363
train_loss: 571.4681334495544
batch: 364
train_loss: 574.9194402694702
batch: 365
train_loss: 578.3140075206757
batch: 366
train_loss: 581.6961631774902
batch: 367
train_loss: 585.1739196777344
batch: 368
train_loss: 588.6053426265717
batch: 369
train_loss: 591.9960973262787
batch: 370
train_loss: 595.4356861114502
batch: 371
train_loss: 598.8592479228973
batch: 372
train_loss: 602.3491275310516
batch: 373
train_loss: 605.913135766983
batch: 374
train_loss: 609.6238627433777
batch: 375
train_loss: 613.1243627071381
batch: 376
train_loss: 616.6909916400909
batch: 377
train_loss: 620.2126133441925
batch: 378
train_loss: 623.7226507663727
batch: 379
train_loss: 627.2454731464386
batch: 380
train_loss: 630.7150168418884
batch: 381
train_loss: 634.1739401817322
batch: 382
train_loss: 637.5941259860992
batch: 383
train_loss: 641.0429418087006
batch: 384
train_loss: 644.5183339118958
batch: 385
train_loss: 648.0720725059509
batch: 386
train_loss: 651.6211717128754
batch: 387
train_loss: 655.1666660308838
batch: 388
train_loss: 658.6842250823975
batch: 389
train_loss: 662.2367732524872
batch: 390
train_loss: 665.8795475959778
batch: 391
train_loss: 669.2757017612457
batch: 392
train_loss: 672.6796698570251
batch: 393
train_loss: 676.1116628646851
batch: 394
train_loss: 679.5361790657043
batch: 395
train_loss: 683.0151913166046
batch: 396
train_loss: 686.529627084732
batch: 397
train_loss: 689.9672012329102
batch: 398
train_loss: 693.3901610374451
batch: 399
train_loss: 696.730227470398
| epoch   1 step      400 |    400 batches | lr 0.00025 | ms/batch 33793.43 | loss  3.48 | bpc   5.02585
batch: 400
train_loss: 3.304530143737793
batch: 401
train_loss: 6.668541669845581
batch: 402
train_loss: 10.118098974227905
batch: 403
train_loss: 13.539361953735352
batch: 404
train_loss: 17.022825002670288
batch: 405
train_loss: 20.43661141395569
batch: 406
train_loss: 23.86589002609253
batch: 407
train_loss: 27.484619617462158
batch: 408
train_loss: 31.0410258769989
batch: 409
train_loss: 34.49759769439697
batch: 410
train_loss: 37.97429442405701
batch: 411
train_loss: 41.489930629730225
batch: 412
train_loss: 44.96705102920532
batch: 413
train_loss: 48.45581817626953
batch: 414
train_loss: 51.88215732574463
batch: 415
train_loss: 55.34294009208679
batch: 416
train_loss: 58.89701581001282
batch: 417
train_loss: 62.35311245918274
batch: 418
train_loss: 65.94596219062805
batch: 419
train_loss: 69.47793412208557
batch: 420
train_loss: 72.94150471687317
batch: 421
train_loss: 76.412668466568
batch: 422
train_loss: 79.88451290130615
batch: 423
train_loss: 83.37089228630066
batch: 424
train_loss: 86.81341338157654
batch: 425
train_loss: 90.32999682426453
batch: 426
train_loss: 93.78675127029419
batch: 427
train_loss: 97.25074887275696
batch: 428
train_loss: 100.7128255367279
batch: 429
train_loss: 104.20948028564453
batch: 430
train_loss: 107.58347249031067
batch: 431
train_loss: 110.89849257469177
batch: 432
train_loss: 114.18572926521301
batch: 433
train_loss: 117.5904176235199
batch: 434
train_loss: 120.9696717262268
batch: 435
train_loss: 124.29666471481323
batch: 436
train_loss: 127.6439893245697
batch: 437
train_loss: 130.994389295578
batch: 438
train_loss: 134.36098194122314
batch: 439
train_loss: 137.7797634601593
batch: 440
train_loss: 141.26020336151123
batch: 441
train_loss: 144.62474513053894
batch: 442
train_loss: 148.00883769989014
batch: 443
train_loss: 151.44022750854492
batch: 444
train_loss: 154.91489052772522
batch: 445
train_loss: 158.33211135864258
batch: 446
train_loss: 161.75381755828857
batch: 447
train_loss: 165.13650965690613
batch: 448
train_loss: 168.51454901695251
batch: 449
train_loss: 171.89801406860352
batch: 450
train_loss: 175.3538794517517
batch: 451
train_loss: 178.9387571811676
batch: 452
train_loss: 182.4883518218994
batch: 453
train_loss: 185.94550919532776
batch: 454
train_loss: 189.3586621284485
batch: 455
train_loss: 192.82437705993652
batch: 456
train_loss: 196.2668173313141
batch: 457
train_loss: 199.78595113754272
batch: 458
train_loss: 203.34290838241577
batch: 459
train_loss: 206.88453459739685
batch: 460
train_loss: 210.3945631980896
batch: 461
train_loss: 213.89051604270935
batch: 462
train_loss: 217.32068800926208
batch: 463
train_loss: 220.8382761478424
batch: 464
train_loss: 224.32772850990295
batch: 465
train_loss: 227.77699971199036
batch: 466
train_loss: 231.21630859375
batch: 467
train_loss: 234.65829873085022
batch: 468
train_loss: 238.10782265663147
batch: 469
train_loss: 241.5885875225067
batch: 470
train_loss: 245.09761667251587
batch: 471
train_loss: 248.57210540771484
batch: 472
train_loss: 252.06773734092712
batch: 473
train_loss: 255.55347347259521
batch: 474
train_loss: 259.06913685798645
batch: 475
train_loss: 262.61491084098816
batch: 476
train_loss: 266.1887369155884
batch: 477
train_loss: 269.72368121147156
batch: 478
train_loss: 273.2041394710541
batch: 479
train_loss: 276.6474287509918
batch: 480
train_loss: 280.0676395893097
batch: 481
train_loss: 283.4734683036804
batch: 482
train_loss: 286.94572377204895
batch: 483
train_loss: 290.4809868335724
batch: 484
train_loss: 293.9367036819458
batch: 485
train_loss: 297.4037857055664
batch: 486
train_loss: 300.91805958747864
batch: 487
train_loss: 304.51597690582275
batch: 488
train_loss: 307.97333335876465
batch: 489
train_loss: 311.44072008132935
batch: 490
train_loss: 314.8732969760895
batch: 491
train_loss: 318.2933602333069
batch: 492
train_loss: 321.7949686050415
batch: 493
train_loss: 325.2641170024872
batch: 494
train_loss: 328.66467690467834
batch: 495
train_loss: 332.12580919265747
batch: 496
train_loss: 335.5593419075012
batch: 497
train_loss: 338.96268033981323
batch: 498
train_loss: 342.418931722641
batch: 499
train_loss: 345.916809797287
batch: 500
train_loss: 349.4028732776642
batch: 501
train_loss: 352.92849373817444
batch: 502
train_loss: 356.39437103271484
batch: 503
train_loss: 360.05515217781067
batch: 504
train_loss: 363.5671844482422
batch: 505
train_loss: 367.1356680393219
batch: 506
train_loss: 370.7078685760498
batch: 507
train_loss: 374.28741931915283
batch: 508
train_loss: 377.86035084724426
batch: 509
train_loss: 381.4858286380768
batch: 510
train_loss: 384.96114015579224
batch: 511
train_loss: 388.4375653266907
batch: 512
train_loss: 391.84149384498596
batch: 513
train_loss: 395.2752387523651
batch: 514
train_loss: 398.69854950904846
batch: 515
train_loss: 402.0692000389099
batch: 516
train_loss: 405.4571053981781
batch: 517
train_loss: 408.87115240097046
batch: 518
train_loss: 412.4310460090637
batch: 519
train_loss: 416.07435965538025
batch: 520
train_loss: 419.6317822933197
batch: 521
train_loss: 423.12854290008545
batch: 522
train_loss: 426.6459491252899
batch: 523
train_loss: 430.29048347473145
batch: 524
train_loss: 433.83960604667664
batch: 525
train_loss: 437.414883852005
batch: 526
train_loss: 440.85174465179443
batch: 527
train_loss: 444.30262637138367
batch: 528
train_loss: 447.7981746196747
batch: 529
train_loss: 451.33705258369446
batch: 530
train_loss: 454.8860852718353
batch: 531
train_loss: 458.30753326416016
batch: 532
train_loss: 461.6598012447357
batch: 533
train_loss: 465.05154633522034
batch: 534
train_loss: 468.4517819881439
batch: 535
train_loss: 471.85616183280945
batch: 536
train_loss: 475.29485607147217
batch: 537
train_loss: 478.7833499908447
batch: 538
train_loss: 482.1989994049072
batch: 539
train_loss: 485.630765914917
batch: 540
train_loss: 489.1573164463043
batch: 541
train_loss: 492.62454199790955
batch: 542
train_loss: 496.09870195388794
batch: 543
train_loss: 499.62536454200745
batch: 544
train_loss: 503.18980145454407
batch: 545
train_loss: 506.71617007255554
batch: 546
train_loss: 510.18501234054565
batch: 547
train_loss: 513.7353458404541
batch: 548
train_loss: 517.256890296936
batch: 549
train_loss: 520.8705914020538
batch: 550
train_loss: 524.3713343143463
batch: 551
train_loss: 527.915079832077
batch: 552
train_loss: 531.5654513835907
batch: 553
train_loss: 535.2094209194183
batch: 554
train_loss: 538.8270194530487
batch: 555
train_loss: 542.3815579414368
batch: 556
train_loss: 545.8636295795441
batch: 557
train_loss: 549.3903467655182
batch: 558
train_loss: 552.8728592395782
batch: 559
train_loss: 556.3890762329102
batch: 560
train_loss: 559.9607276916504
batch: 561
train_loss: 563.4974761009216
batch: 562
train_loss: 567.0399878025055
batch: 563
train_loss: 570.602555513382
batch: 564
train_loss: 574.1298532485962
batch: 565
train_loss: 577.6127099990845
batch: 566
train_loss: 581.1792740821838
batch: 567
train_loss: 584.7118577957153
batch: 568
train_loss: 588.2709712982178
batch: 569
train_loss: 591.7679393291473
batch: 570
train_loss: 595.2957921028137
batch: 571
train_loss: 598.8246335983276
batch: 572
train_loss: 602.2697670459747
batch: 573
train_loss: 605.6823265552521
batch: 574
train_loss: 609.0261697769165
batch: 575
train_loss: 612.4659926891327
batch: 576
train_loss: 616.0883071422577
batch: 577
train_loss: 619.5660331249237
batch: 578
train_loss: 623.0906729698181
batch: 579
train_loss: 626.6510832309723
batch: 580
train_loss: 630.1528990268707
batch: 581
train_loss: 633.6769270896912
batch: 582
train_loss: 637.137454032898
batch: 583
train_loss: 640.6989417076111
batch: 584
train_loss: 644.1496253013611
batch: 585
train_loss: 647.6151986122131
batch: 586
train_loss: 651.0595033168793
batch: 587
train_loss: 654.5486693382263
batch: 588
train_loss: 657.9843633174896
batch: 589
train_loss: 661.4348697662354
batch: 590
train_loss: 664.9283947944641
batch: 591
train_loss: 668.43505692482
batch: 592
train_loss: 671.9123847484589
batch: 593
train_loss: 675.386944770813
batch: 594
train_loss: 678.8476550579071
batch: 595
train_loss: 682.3120183944702
batch: 596
train_loss: 685.856467962265
batch: 597
train_loss: 689.3588933944702
batch: 598
train_loss: 692.9015724658966
batch: 599
train_loss: 696.3944144248962
| epoch   1 step      600 |    600 batches | lr 0.00025 | ms/batch 33894.61 | loss  3.48 | bpc   5.02342
batch: 600
train_loss: 3.499772787094116
batch: 601
train_loss: 7.0135626792907715
batch: 602
train_loss: 10.485395669937134
batch: 603
train_loss: 13.99392318725586
batch: 604
train_loss: 17.529895067214966
batch: 605
train_loss: 21.031733989715576
batch: 606
train_loss: 24.565783977508545
batch: 607
train_loss: 28.133530616760254
batch: 608
train_loss: 31.735585927963257
batch: 609
train_loss: 35.388362884521484
batch: 610
train_loss: 39.01271414756775
batch: 611
train_loss: 42.548251152038574
batch: 612
train_loss: 46.143150329589844
batch: 613
train_loss: 49.60894155502319
batch: 614
train_loss: 53.144856214523315
batch: 615
train_loss: 56.66137933731079
batch: 616
train_loss: 60.11731719970703
batch: 617
train_loss: 63.614022731781006
batch: 618
train_loss: 67.13651704788208
batch: 619
train_loss: 70.63184857368469
batch: 620
train_loss: 74.17007803916931
batch: 621
train_loss: 77.67910981178284
batch: 622
train_loss: 81.21880531311035
batch: 623
train_loss: 84.74475073814392
batch: 624
train_loss: 88.19378113746643
batch: 625
train_loss: 91.67611765861511
batch: 626
train_loss: 95.10669016838074
batch: 627
train_loss: 98.54126572608948
batch: 628
train_loss: 101.98817443847656
batch: 629
train_loss: 105.43695855140686
batch: 630
train_loss: 108.87961602210999
batch: 631
train_loss: 112.37384128570557
batch: 632
train_loss: 115.95856952667236
batch: 633
train_loss: 119.48776626586914
batch: 634
train_loss: 122.94339728355408
batch: 635
train_loss: 126.58636093139648
batch: 636
train_loss: 130.14968967437744
batch: 637
train_loss: 133.79678344726562
batch: 638
train_loss: 137.4317228794098
batch: 639
train_loss: 141.05718898773193
batch: 640
train_loss: 144.67074179649353
batch: 641
train_loss: 148.20173239707947
batch: 642
train_loss: 151.74198579788208
batch: 643
train_loss: 155.28854727745056
batch: 644
train_loss: 158.90291547775269
batch: 645
train_loss: 162.4274036884308
batch: 646
train_loss: 165.91924786567688
batch: 647
train_loss: 169.38190984725952
batch: 648
train_loss: 172.8541738986969
batch: 649
train_loss: 176.30153846740723
batch: 650
train_loss: 179.73900175094604
batch: 651
train_loss: 183.20098543167114
batch: 652
train_loss: 186.67285871505737
batch: 653
train_loss: 190.18098878860474
batch: 654
train_loss: 193.68703055381775
batch: 655
train_loss: 197.16884207725525
batch: 656
train_loss: 200.67972898483276
batch: 657
train_loss: 204.21657276153564
batch: 658
train_loss: 207.72191739082336
batch: 659
train_loss: 211.31494283676147
batch: 660
train_loss: 214.87373852729797
batch: 661
train_loss: 218.43689703941345
batch: 662
train_loss: 221.95842170715332
batch: 663
train_loss: 225.45529961585999
batch: 664
train_loss: 229.10810208320618
batch: 665
train_loss: 232.76117587089539
batch: 666
train_loss: 236.37933778762817
batch: 667
train_loss: 239.95758628845215
batch: 668
train_loss: 243.42512726783752
batch: 669
train_loss: 246.9052062034607
batch: 670
train_loss: 250.3498411178589
batch: 671
train_loss: 253.9516921043396
batch: 672
train_loss: 257.53350615501404
batch: 673
train_loss: 261.08656764030457
batch: 674
train_loss: 264.54790592193604
batch: 675
train_loss: 268.0073685646057
batch: 676
train_loss: 271.49839997291565
batch: 677
train_loss: 274.90363001823425
batch: 678
train_loss: 278.3409035205841
batch: 679
train_loss: 281.8060257434845
batch: 680
train_loss: 285.25015115737915
batch: 681
train_loss: 288.8237373828888
batch: 682
train_loss: 292.3430030345917
batch: 683
train_loss: 295.85032629966736
batch: 684
train_loss: 299.3985900878906
batch: 685
train_loss: 302.93989086151123
batch: 686
train_loss: 306.39847779273987
batch: 687
train_loss: 309.7963287830353
batch: 688
train_loss: 313.2662010192871
batch: 689
train_loss: 316.76947498321533
batch: 690
train_loss: 320.24978137016296
batch: 691
train_loss: 323.8378758430481
batch: 692
train_loss: 327.31294465065
batch: 693
train_loss: 330.75886130332947
batch: 694
train_loss: 334.1386573314667
batch: 695
train_loss: 337.6136751174927
batch: 696
train_loss: 341.07340717315674
batch: 697
train_loss: 344.6812469959259
batch: 698
train_loss: 348.3074564933777
batch: 699
train_loss: 351.8950619697571
batch: 700
train_loss: 355.42748832702637
batch: 701
train_loss: 358.99206805229187
batch: 702
train_loss: 362.49581027030945
batch: 703
train_loss: 365.98864555358887
batch: 704
train_loss: 369.60706901550293
batch: 705
train_loss: 373.3177661895752
batch: 706
train_loss: 376.8810408115387
batch: 707
train_loss: 380.3259856700897
batch: 708
train_loss: 383.73320174217224
batch: 709
train_loss: 387.216059923172
batch: 710
train_loss: 390.63681197166443
batch: 711
train_loss: 394.22049045562744
batch: 712
train_loss: 397.7892289161682
batch: 713
train_loss: 401.3285553455353
batch: 714
train_loss: 404.9194734096527
batch: 715
train_loss: 408.48256397247314
batch: 716
train_loss: 412.0573356151581
batch: 717
train_loss: 415.6328101158142
batch: 718
train_loss: 419.13526463508606
batch: 719
train_loss: 422.6546473503113
batch: 720
train_loss: 426.21762895584106
batch: 721
train_loss: 429.93619203567505
batch: 722
train_loss: 433.57747316360474
batch: 723
train_loss: 437.1204721927643
batch: 724
train_loss: 440.6833064556122
batch: 725
train_loss: 444.2784616947174
batch: 726
train_loss: 447.8993077278137
batch: 727
train_loss: 451.5031683444977
batch: 728
train_loss: 455.1073055267334
batch: 729
train_loss: 458.61545515060425
batch: 730
train_loss: 462.1098198890686
batch: 731
train_loss: 465.62873911857605
batch: 732
train_loss: 469.2008993625641
batch: 733
train_loss: 472.69266629219055
batch: 734
train_loss: 476.1985628604889
batch: 735
train_loss: 479.6472668647766
batch: 736
train_loss: 483.1500105857849
batch: 737
train_loss: 486.67950677871704
batch: 738
train_loss: 490.20482087135315
batch: 739
train_loss: 493.70478796958923
batch: 740
train_loss: 497.2119560241699
batch: 741
train_loss: 500.6369285583496
batch: 742
train_loss: 504.11331248283386
batch: 743
train_loss: 507.6546323299408
batch: 744
train_loss: 511.0911703109741
batch: 745
train_loss: 514.5510952472687
batch: 746
train_loss: 517.9757795333862
batch: 747
train_loss: 521.3982458114624
batch: 748
train_loss: 524.8714537620544
batch: 749
train_loss: 528.3022820949554
batch: 750
train_loss: 531.7277200222015
batch: 751
train_loss: 535.1958651542664
batch: 752
train_loss: 538.7052943706512
batch: 753
train_loss: 542.2645456790924
batch: 754
train_loss: 545.7558207511902
batch: 755
train_loss: 549.3098306655884
batch: 756
train_loss: 552.7825813293457
batch: 757
train_loss: 556.2874443531036
batch: 758
train_loss: 559.752623796463
batch: 759
train_loss: 563.2093818187714
batch: 760
train_loss: 566.7040672302246
batch: 761
train_loss: 570.1228766441345
batch: 762
train_loss: 573.5571389198303
batch: 763
train_loss: 577.0313363075256
batch: 764
train_loss: 580.5488436222076
batch: 765
train_loss: 584.1061863899231
batch: 766
train_loss: 587.6786093711853
batch: 767
train_loss: 591.3274703025818
batch: 768
train_loss: 595.0359814167023
batch: 769
train_loss: 598.6113474369049
batch: 770
train_loss: 602.2112107276917
batch: 771
train_loss: 605.6793253421783
batch: 772
train_loss: 609.1336390972137
batch: 773
train_loss: 612.5570094585419
batch: 774
train_loss: 615.9706385135651
batch: 775
train_loss: 619.5666544437408
batch: 776
train_loss: 623.015613079071
batch: 777
train_loss: 626.5377063751221
batch: 778
train_loss: 629.9716203212738
batch: 779
train_loss: 633.4711427688599
batch: 780
train_loss: 637.0150887966156
batch: 781
train_loss: 640.568528175354
batch: 782
train_loss: 644.1302881240845
batch: 783
train_loss: 647.6907708644867
batch: 784
train_loss: 651.2718026638031
batch: 785
train_loss: 654.789873123169
batch: 786
train_loss: 658.3037929534912
batch: 787
train_loss: 661.7255818843842
batch: 788
train_loss: 665.1785619258881
batch: 789
train_loss: 668.5900039672852
batch: 790
train_loss: 671.9954564571381
batch: 791
train_loss: 675.3448195457458
batch: 792
train_loss: 678.7521963119507
batch: 793
train_loss: 682.258918762207
batch: 794
train_loss: 685.6940932273865
batch: 795
train_loss: 689.1306540966034
batch: 796
train_loss: 692.5811529159546
batch: 797
train_loss: 696.1291265487671
batch: 798
train_loss: 699.6353976726532
batch: 799
train_loss: 703.2257809638977
| epoch   1 step      800 |    800 batches | lr 0.00025 | ms/batch 27361.67 | loss  3.52 | bpc   5.07270
batch: 800
slurmstepd-pink57: error: *** JOB 5999709 ON pink57 CANCELLED AT 2024-05-17T09:52:46 ***
==============================================================================
Running epilogue script on pink57.

Submit time  : 2024-05-17T02:35:45
Start time   : 2024-05-17T02:35:45
End time     : 2024-05-17T09:52:46
Elapsed time : 07:17:01 (Timelimit=08:00:00)

Job ID: 5999709
Cluster: i5
User/Group: rc3g20/fp
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 8
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 2-10:16:08 core-walltime
Job Wall-clock time: 07:17:01
Memory Utilized: 12.28 GB
Memory Efficiency: 0.00% of 16.00 B

