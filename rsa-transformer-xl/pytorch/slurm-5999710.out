Running SLURM prolog script on pink57.cluster.local
===============================================================================
Job started on Fri May 17 02:36:13 BST 2024
Job ID          : 5999710
Job name        : run_enwik8_rsa.sh
WorkDir         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch
Command         : /mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/run_enwik8_rsa.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 8
Num of tasks    : 1
Hosts allocated : pink57
Job Output Follows ...
===============================================================================
Loading cached dataset...
====================================================================================================
    - data : ../data/enwik8/
    - dataset : enwik8
    - n_layer : 14
    - n_head : 8
    - d_head : 64
    - d_embed : 512
    - d_model : 512
    - d_inner : 2048
    - dropout : 0.1
    - dropatt : 0.0
    - init : normal
    - emb_init : normal
    - init_range : 0.1
    - emb_init_range : 0.01
    - init_std : 0.02
    - proj_init_std : 0.01
    - optim : adam
    - lr : 0.00025
    - mom : 0.0
    - scheduler : cosine
    - warmup_step : 0
    - decay_rate : 0.5
    - lr_min : 0.0
    - clip : 0.25
    - clip_nonemb : False
    - max_step : 40000
    - batch_size : 22
    - batch_chunk : 1
    - tgt_len : 512
    - eval_tgt_len : 128
    - ext_len : 0
    - mem_len : 512
    - not_tied : False
    - seed : 1111
    - cuda : True
    - adaptive : False
    - div_val : 1
    - pre_lnorm : False
    - varlen : False
    - multi_gpu : True
    - log_interval : 200
    - eval_interval : 4000
    - work_dir : LM-TFM
    - restart : False
    - restart_dir : 
    - debug : False
    - same_length : False
    - attn_type : 4
    - clamp_len : -1
    - eta_min : 0.0
    - gpu0_bsz : 4
    - max_eval_steps : -1
    - sample_softmax : -1
    - patience : 0
    - finetune_v2 : False
    - finetune_v3 : False
    - fp16 : False
    - static_loss_scale : 1
    - dynamic_loss_scale : False
    - n_rsa_head : 4
    - k_rem_indexes : [0, 0, 0, 0, 2, 2]
    - dilated_factors : [3, 6, 9, 12]
    - iridis : False
    - mu_init : 1
    - tied : True
    - n_token : 204
    - n_all_param : 47880452
    - n_nonemb_param : 47774776
====================================================================================================
#params = 47880452
#non emb params = 47774776
batch: 0
/mainfs/lyceum/rc3g20/DL_CW/transformer-xl/pytorch/mem_transformer.py:544: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  attn_mask[:,:,:,None], -float('inf')).type_as(attn_score)
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/nn/functional.py:1967: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/autograd/__init__.py:199: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525496686/work/aten/src/ATen/native/TensorAdvancedIndexing.cpp:1646.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
train_loss: 5.412010669708252
/lyceum/rc3g20/.conda/envs/transformer-xl/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
batch: 1
train_loss: 10.022048950195312
batch: 2
train_loss: 14.356818675994873
batch: 3
train_loss: 18.464337825775146
batch: 4
train_loss: 22.494664192199707
batch: 5
train_loss: 26.399555444717407
batch: 6
train_loss: 30.27245259284973
batch: 7
train_loss: 34.14869236946106
batch: 8
train_loss: 37.93566346168518
batch: 9
train_loss: 41.62051844596863
batch: 10
train_loss: 45.19595694541931
batch: 11
train_loss: 48.764015197753906
batch: 12
train_loss: 52.35084939002991
batch: 13
train_loss: 55.91760277748108
batch: 14
train_loss: 59.45865797996521
batch: 15
train_loss: 63.037750482559204
batch: 16
train_loss: 66.59486269950867
batch: 17
train_loss: 70.12008142471313
batch: 18
train_loss: 73.67946004867554
batch: 19
train_loss: 77.2108862400055
batch: 20
train_loss: 80.69627451896667
batch: 21
train_loss: 84.132737159729
batch: 22
train_loss: 87.67385053634644
batch: 23
train_loss: 91.24565005302429
batch: 24
train_loss: 94.76834893226624
batch: 25
train_loss: 98.33234786987305
batch: 26
train_loss: 101.80211234092712
batch: 27
train_loss: 105.3234121799469
batch: 28
train_loss: 108.85636949539185
batch: 29
train_loss: 112.48507261276245
batch: 30
train_loss: 116.04118394851685
batch: 31
train_loss: 119.49555945396423
batch: 32
train_loss: 122.95222091674805
batch: 33
train_loss: 126.42333316802979
batch: 34
train_loss: 129.83041954040527
batch: 35
train_loss: 133.27931308746338
batch: 36
train_loss: 136.72761988639832
batch: 37
train_loss: 140.25379729270935
batch: 38
train_loss: 143.7335033416748
batch: 39
train_loss: 147.15873551368713
batch: 40
train_loss: 150.6453025341034
batch: 41
train_loss: 154.11773657798767
batch: 42
train_loss: 157.47922039031982
batch: 43
train_loss: 160.8500533103943
batch: 44
train_loss: 164.20572566986084
batch: 45
train_loss: 167.62824082374573
batch: 46
train_loss: 171.0982208251953
batch: 47
train_loss: 174.6364450454712
batch: 48
train_loss: 178.146244764328
batch: 49
train_loss: 181.67247247695923
batch: 50
train_loss: 185.22683095932007
batch: 51
train_loss: 188.74880003929138
batch: 52
train_loss: 192.20584177970886
batch: 53
train_loss: 195.7657446861267
batch: 54
train_loss: 199.31210565567017
batch: 55
train_loss: 202.88734126091003
batch: 56
train_loss: 206.33810114860535
batch: 57
train_loss: 209.77792835235596
batch: 58
train_loss: 213.25930500030518
batch: 59
train_loss: 216.75284576416016
batch: 60
train_loss: 220.15232706069946
batch: 61
train_loss: 223.61231327056885
batch: 62
train_loss: 227.13321709632874
batch: 63
train_loss: 230.64403700828552
batch: 64
train_loss: 234.13113474845886
batch: 65
train_loss: 237.60504364967346
batch: 66
train_loss: 241.09828186035156
batch: 67
train_loss: 244.59678101539612
batch: 68
train_loss: 248.1416721343994
batch: 69
train_loss: 251.6704523563385
batch: 70
train_loss: 255.17935347557068
batch: 71
train_loss: 258.6463272571564
batch: 72
train_loss: 262.15248942375183
batch: 73
train_loss: 265.63113164901733
batch: 74
train_loss: 269.09806728363037
batch: 75
train_loss: 272.6567153930664
batch: 76
train_loss: 276.1319887638092
batch: 77
train_loss: 279.6403248310089
batch: 78
train_loss: 283.2015645503998
batch: 79
train_loss: 286.81944489479065
batch: 80
train_loss: 290.3661243915558
batch: 81
train_loss: 293.99331164360046
batch: 82
train_loss: 297.5383632183075
batch: 83
train_loss: 301.1049678325653
batch: 84
train_loss: 304.5891659259796
batch: 85
train_loss: 308.12095308303833
batch: 86
train_loss: 311.6772437095642
batch: 87
train_loss: 315.21229672431946
batch: 88
train_loss: 318.75638723373413
batch: 89
train_loss: 322.26546907424927
batch: 90
train_loss: 325.7677767276764
batch: 91
train_loss: 329.20900893211365
batch: 92
train_loss: 332.71718764305115
batch: 93
train_loss: 336.18575167655945
batch: 94
train_loss: 339.6574077606201
batch: 95
train_loss: 343.1689419746399
batch: 96
train_loss: 346.7711353302002
batch: 97
train_loss: 350.2619924545288
batch: 98
train_loss: 353.7785403728485
batch: 99
train_loss: 357.2972140312195
batch: 100
train_loss: 360.8670983314514
batch: 101
train_loss: 364.4401795864105
batch: 102
train_loss: 367.9568405151367
batch: 103
train_loss: 371.4918591976166
batch: 104
train_loss: 374.9731273651123
batch: 105
train_loss: 378.4655718803406
batch: 106
train_loss: 381.96129179000854
batch: 107
train_loss: 385.43125009536743
batch: 108
train_loss: 388.85271978378296
batch: 109
train_loss: 392.22839641571045
batch: 110
train_loss: 395.6237132549286
batch: 111
train_loss: 399.0689187049866
batch: 112
train_loss: 402.63907504081726
batch: 113
train_loss: 406.15445280075073
batch: 114
train_loss: 409.70583391189575
batch: 115
train_loss: 413.2383449077606
batch: 116
train_loss: 416.7522189617157
batch: 117
train_loss: 420.2990427017212
batch: 118
train_loss: 423.83953189849854
batch: 119
train_loss: 427.3696835041046
batch: 120
train_loss: 431.0304636955261
batch: 121
train_loss: 434.6637680530548
batch: 122
train_loss: 438.2424852848053
batch: 123
train_loss: 441.8357026576996
batch: 124
train_loss: 445.3511552810669
batch: 125
train_loss: 448.8885545730591
batch: 126
train_loss: 452.4732630252838
batch: 127
train_loss: 455.9126076698303
batch: 128
train_loss: 459.4028060436249
batch: 129
train_loss: 462.83218359947205
batch: 130
train_loss: 466.2390446662903
batch: 131
train_loss: 469.5987706184387
batch: 132
train_loss: 473.0000834465027
batch: 133
train_loss: 476.4450478553772
batch: 134
train_loss: 479.9192214012146
batch: 135
train_loss: 483.41091775894165
batch: 136
train_loss: 486.87168192863464
batch: 137
train_loss: 490.31804752349854
batch: 138
train_loss: 493.72303462028503
batch: 139
train_loss: 497.1440613269806
batch: 140
train_loss: 500.62745690345764
batch: 141
train_loss: 504.1332206726074
batch: 142
train_loss: 507.59930896759033
batch: 143
train_loss: 510.9760494232178
batch: 144
train_loss: 514.4266233444214
batch: 145
train_loss: 517.8964493274689
batch: 146
train_loss: 521.4229996204376
batch: 147
train_loss: 524.8891623020172
batch: 148
train_loss: 528.3760855197906
batch: 149
train_loss: 531.9514410495758
batch: 150
train_loss: 535.6225888729095
batch: 151
train_loss: 539.2228152751923
batch: 152
train_loss: 542.7252225875854
batch: 153
train_loss: 546.1472089290619
batch: 154
train_loss: 549.6750884056091
batch: 155
train_loss: 553.2113621234894
batch: 156
train_loss: 556.6578447818756
batch: 157
train_loss: 560.1436631679535
batch: 158
train_loss: 563.6340477466583
batch: 159
train_loss: 567.0953919887543
batch: 160
train_loss: 570.5704536437988
batch: 161
train_loss: 574.0895285606384
batch: 162
train_loss: 577.6490371227264
batch: 163
train_loss: 581.1790115833282
batch: 164
train_loss: 584.6602218151093
batch: 165
train_loss: 588.18998503685
batch: 166
train_loss: 591.6467132568359
batch: 167
train_loss: 595.0829253196716
batch: 168
train_loss: 598.5168128013611
batch: 169
train_loss: 601.8959519863129
batch: 170
train_loss: 605.3097171783447
batch: 171
train_loss: 608.7541851997375
batch: 172
train_loss: 612.1418986320496
batch: 173
train_loss: 615.5232241153717
batch: 174
train_loss: 618.9245522022247
batch: 175
train_loss: 622.3730325698853
batch: 176
train_loss: 625.7862544059753
batch: 177
train_loss: 629.2438676357269
batch: 178
train_loss: 632.6448061466217
batch: 179
train_loss: 636.1799411773682
batch: 180
train_loss: 639.6105015277863
batch: 181
train_loss: 642.9955174922943
batch: 182
train_loss: 646.3011844158173
batch: 183
train_loss: 649.5736217498779
batch: 184
train_loss: 652.9194204807281
batch: 185
train_loss: 656.246333360672
batch: 186
train_loss: 659.617748260498
batch: 187
train_loss: 662.9378576278687
batch: 188
train_loss: 666.409833908081
batch: 189
train_loss: 669.8233647346497
batch: 190
train_loss: 673.2565999031067
batch: 191
train_loss: 676.6471111774445
batch: 192
train_loss: 680.0815489292145
batch: 193
train_loss: 683.538950920105
batch: 194
train_loss: 687.1043045520782
batch: 195
train_loss: 690.5543990135193
batch: 196
train_loss: 694.0494756698608
batch: 197
train_loss: 697.5054953098297
batch: 198
train_loss: 700.9521932601929
batch: 199
train_loss: 704.3219764232635
| epoch   1 step      200 |    200 batches | lr 0.00025 | ms/batch 73281.66 | loss  3.52 | bpc   5.08061
batch: 200
train_loss: 3.3990137577056885
batch: 201
train_loss: 6.862030982971191
batch: 202
train_loss: 10.366552114486694
batch: 203
train_loss: 13.89332890510559
batch: 204
train_loss: 17.46316361427307
batch: 205
train_loss: 21.011841297149658
batch: 206
train_loss: 24.48396635055542
batch: 207
train_loss: 27.910123825073242
batch: 208
train_loss: 31.29349946975708
batch: 209
train_loss: 34.70481014251709
batch: 210
train_loss: 38.08619046211243
batch: 211
train_loss: 41.50313210487366
batch: 212
train_loss: 44.956008195877075
batch: 213
train_loss: 48.436887979507446
batch: 214
train_loss: 51.90135717391968
batch: 215
train_loss: 55.35792517662048
batch: 216
train_loss: 58.873117446899414
batch: 217
train_loss: 62.30158710479736
batch: 218
train_loss: 65.70366764068604
batch: 219
train_loss: 69.12736582756042
batch: 220
train_loss: 72.69987273216248
batch: 221
train_loss: 76.18058466911316
batch: 222
train_loss: 79.7056736946106
batch: 223
train_loss: 83.16362953186035
batch: 224
train_loss: 86.67811799049377
batch: 225
train_loss: 90.27668881416321
batch: 226
train_loss: 93.82256817817688
batch: 227
train_loss: 97.2826566696167
batch: 228
train_loss: 100.72443056106567
batch: 229
train_loss: 104.18521547317505
batch: 230
train_loss: 107.66240501403809
batch: 231
train_loss: 111.22459602355957
batch: 232
train_loss: 114.76431179046631
batch: 233
train_loss: 118.30302810668945
batch: 234
train_loss: 121.92986369132996
batch: 235
train_loss: 125.56117177009583
batch: 236
train_loss: 129.00956439971924
batch: 237
train_loss: 132.4908549785614
batch: 238
train_loss: 136.0317783355713
batch: 239
train_loss: 139.51716709136963
batch: 240
train_loss: 142.95745468139648
batch: 241
train_loss: 146.43590450286865
batch: 242
train_loss: 149.93820881843567
batch: 243
train_loss: 153.38855385780334
batch: 244
train_loss: 156.87268018722534
batch: 245
train_loss: 160.3207790851593
batch: 246
train_loss: 163.7834334373474
batch: 247
train_loss: 167.16914677619934
batch: 248
train_loss: 170.54473495483398
batch: 249
train_loss: 174.02124428749084
batch: 250
train_loss: 177.44521737098694
batch: 251
train_loss: 180.91825890541077
batch: 252
train_loss: 184.3252260684967
batch: 253
train_loss: 187.83310055732727
batch: 254
train_loss: 191.32122015953064
batch: 255
train_loss: 194.8972053527832
batch: 256
train_loss: 198.46569061279297
batch: 257
train_loss: 201.91892004013062
batch: 258
train_loss: 205.31154322624207
batch: 259
train_loss: 208.76710724830627
batch: 260
train_loss: 212.14198875427246
batch: 261
train_loss: 215.54021430015564
batch: 262
train_loss: 218.89548778533936
batch: 263
train_loss: 222.24477314949036
batch: 264
train_loss: 225.67512154579163
batch: 265
train_loss: 229.1534662246704
batch: 266
train_loss: 232.61147952079773
batch: 267
train_loss: 236.06889605522156
batch: 268
train_loss: 239.52394723892212
batch: 269
train_loss: 243.0387704372406
batch: 270
train_loss: 246.60335230827332
batch: 271
train_loss: 250.15716981887817
batch: 272
train_loss: 253.7313916683197
batch: 273
train_loss: 257.2062175273895
batch: 274
train_loss: 260.6926600933075
batch: 275
train_loss: 264.1801402568817
batch: 276
train_loss: 267.80905652046204
batch: 277
train_loss: 271.2781991958618
batch: 278
train_loss: 274.8043932914734
batch: 279
train_loss: 278.37026858329773
batch: 280
train_loss: 281.9062411785126
batch: 281
train_loss: 285.4414324760437
batch: 282
train_loss: 288.9453570842743
batch: 283
train_loss: 292.5104694366455
batch: 284
train_loss: 296.0648546218872
batch: 285
train_loss: 299.5714111328125
batch: 286
train_loss: 303.10626125335693
batch: 287
train_loss: 306.6471817493439
batch: 288
train_loss: 310.1995759010315
batch: 289
train_loss: 313.74697709083557
batch: 290
train_loss: 317.2630355358124
batch: 291
train_loss: 320.7232720851898
batch: 292
train_loss: 324.2492961883545
batch: 293
train_loss: 327.7251844406128
batch: 294
train_loss: 331.18092465400696
batch: 295
train_loss: 334.67962622642517
batch: 296
train_loss: 338.2702250480652
batch: 297
train_loss: 341.89846444129944
batch: 298
train_loss: 345.47809386253357
batch: 299
train_loss: 348.96544575691223
batch: 300
train_loss: 352.43280029296875
batch: 301
train_loss: 355.8952224254608
batch: 302
train_loss: 359.3809492588043
batch: 303
train_loss: 362.82461404800415
batch: 304
train_loss: 366.3035490512848
batch: 305
train_loss: 369.82587480545044
batch: 306
train_loss: 373.2786273956299
batch: 307
train_loss: 376.73874163627625
batch: 308
train_loss: 380.2521593570709
batch: 309
train_loss: 383.7901089191437
batch: 310
train_loss: 387.3366413116455
batch: 311
train_loss: 390.81066608428955
batch: 312
train_loss: 394.23653531074524
batch: 313
train_loss: 397.703884601593
batch: 314
train_loss: 401.2386863231659
batch: 315
slurmstepd-pink57: error: *** JOB 5999710 ON pink57 CANCELLED AT 2024-05-17T08:57:11 ***
==============================================================================
Running epilogue script on pink57.

Submit time  : 2024-05-17T02:36:12
Start time   : 2024-05-17T02:36:13
End time     : 2024-05-17T08:57:11
Elapsed time : 06:20:58 (Timelimit=08:00:00)

Job ID: 5999710
Cluster: i5
User/Group: rc3g20/fp
State: CANCELLED (exit code 0)
Nodes: 1
Cores per node: 8
CPU Utilized: 00:00:01
CPU Efficiency: 0.00% of 2-02:47:44 core-walltime
Job Wall-clock time: 06:20:58
Memory Utilized: 26.04 GB
Memory Efficiency: 0.00% of 16.00 B

